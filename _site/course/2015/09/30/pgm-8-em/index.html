
<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	
	<title>PGM: The Expectation-Maximization Algorithm </title>
    
	
	<meta name="author" content="Han Xiao">
	
	<link rel="stylesheet" href="/assets/themes/Snail/css/jquery.fancybox.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/main.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/pages/journal.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/team.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/static.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/errors.css">
	<link rel="stylesheet" href="/assets/themes/Snail/google-code-prettify/prettify.css">
	<link rel="shortcut icon" href="/assets/themes/Snail/img/favicon.ico">
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.0/jquery.min.js"></script>
	<script src="/assets/themes/Snail/google-code-prettify/prettify.js"></script>
	<script type="text/javascript">
	  $(function(){
		$("pre code").addClass("prettyprint linenums");
		prettyPrint();
	  });
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
</head>
<body>
	<noscript>
		&amp;lt;div id="no-js"&amp;gt;Please enable JavaScript in your browser to experience / fully&amp;lt;/div&amp;gt;
	</noscript>
    <div id="page-container">
		<div>
			<nav>
	<div id="nav-l">
	</div>
	<div id="nav-c">
		<ul id="nav-list" style="width: 700px;">
			<li id="home"><a href="/">Home</a></li>
			
			
			
				
				  
				
			 
				
				  
					
					<li id = "About Me"><a href="/about.html">About Me</a></li>
					
				  
				
			 
				
				  
					
					<li id = "Archive"><a href="/archive.html">Archive</a></li>
					
				  
				
			 
				
				  
				
			 
				
				  
					
					<li id = "Categories"><a href="/categories.html">Categories</a></li>
					
				  
				
			 
				
			 
				
				  
				
			 
				
				  
				
			 
				
				  
				
			 
				
				  
					
					<li id = "Tags"><a href="/tags.html">Tags</a></li>
					
				  
				
			 
			
			
		</ul>
		<form id="nav-search" method="GET" action="/search.html">
			<div id="search-right-pix">
				<div id="search-left-pix">
					<div id="search-center-pix">
						<div id="search-icon-pix"></div>
						<input name="query" type="text" placeholder="Search">
					</div>
				</div>
			</div>
		</form>
		
		
	</div>
	<div id="nav-r">
	</div>
</nav>
			<div id="page-content">
				
<div id="page-content">
	<div class="cont932">
	<div id="journal-articles-block">
		<div class="journal-article">
			<div class="journal-post-info">
				<div class="journal-cat-box">
				
				
				<div class="journal-cat-box">

<a href="/categories.html#course-ref" title="course">
	course

</a>
</div>
				
				</div>
			</div>
			<div class="journal-body">
				<h1 class="journal-title">PGM: The Expectation-Maximization Algorithm <span class="author"></span>
				</h1>
				<span class="the-article">
				<p>We are discussing EM for only <strong>BN</strong> here.</p>

<h2 id="gaussian-mixture-model">Gaussian Mixture Model</h2>

<p><img src="/assets/images/pgm/gmm.png" alt="" /></p>

<p>Task is to learn: \(\pi_k, \mu_k, \sigma_k\)</p>

<h3 id="difficulty-in-parameter-estimation">Difficulty in parameter estimation</h3>

<p>Decomposability(local estimation) in fully observed GMM:</p>

<p><img src="/assets/images/pgm/em_mixture_model_for_fully_observed_data.png" alt="" /></p>

<p>In partially observed GMM, parameters are coupled together(sum within logarithm).</p>

<p><img src="/assets/images/pgm/em_mixture_model_for_partially_observed_data.png" alt="" /></p>

<h3 id="expectation-maximization">Expectation Maximization</h3>

<p>In EM, label assignment of each data is fractional(probability).</p>

<ul>
  <li><strong>Expectation step</strong>: calculate the expected value of hidden variables given the current estimation of parameters and data(\(p(z \vert D, \theta)\), posterior inference problem)</li>
  <li><strong>Maximization step</strong>: compute the parameters by maximizing \(\mathcal{l}\) under the expected value of hidden variables(essentially, MLE for fully observed BN)</li>
</ul>

<p>Related model is K-means. Compared to K-means, label assignment is “hard”(or MAP) in K-means while it’s “soft” in EM.</p>

<h3 id="e-step-for-glim">E step for GLIM</h3>

<p><img src="/assets/images/pgm/expected_complete_log_likelihood_for_glim.png" alt="" /></p>

<h3 id="theory-why-em-works">Theory: why EM works?</h3>

<p>Why EM works? Essentially, we want to show the marginal likelihood is increasing as EM runs.</p>

<p>Definition of expected complete log likelihood:</p>

<p><img src="/assets/images/pgm/expected_complete_log_likelihood.png" alt="" /></p>

<p>Essentially the expectation of complete log likelihood over the distribution of hidden variables.</p>

<ul>
  <li><strong>Q</strong>: Why do we use the <em>expected complete log likelihood</em>?
    <ol>
      <li>For notational convenience in deriving the lower bound of \(\mathcal{l}\).</li>
      <li>Have the picture of <strong>what to infer about</strong> in the learning process</li>
    </ol>
  </li>
</ul>

<p>For E step:</p>

<p>Jensen’s equality leads to lower bound for \(\mathcal{l}\).</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray*}
\mathcal{l}(\theta; \mathbf{x})
&=& \log p(\mathbf{x} \vert \theta) \\
&=& \log \sum\limits_z p(\mathbf{x}, z \vert \theta) \\
&=& \log \sum\limits_z q(z\vert x) \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vert x)} \\
&\ge& \sum\limits_z  q(z\vert x) \log \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vert x)}
&=& <\mathcal{l}(\theta, x, z)>_q + H(q)
\end{eqnarray*} %]]></script>

<p>Running EM is coordinate-ascent where both E closes the gap between \(F(\theta, x)\) and loss function, while M step maximizes the lower bound of \(\mathcal{l}(\theta, x)\).</p>

<p><img src="/assets/images/pgm/em_in_terms_of_lower_bound.png" alt="" /></p>

<p>The lower bound is coined <em>free energy function</em>, \(F(q, \theta)\).</p>

<p>Or in the information theoretic view:</p>

<script type="math/tex; mode=display">\mathcal{l}(\theta, x) - F(q, \theta) = KL(q \vert \vert p(z \vert x, \theta))</script>

<p>By pushing up the lower bound, we wish to increase \(\mathcal{l}(\theta, x)\).</p>

<p>For M step, setting \(q(z \vert x)\) to \(p(z \vert x, \theta)\) leads to the \(F(q, \theta) = \mathcal{l}(\theta, x)\), which indicates \(F(\theta, x)\) is maximized.</p>

<p>For M step, given the optimal \(q\) in previous step(which can be treated as known) and the entropy term does not depend on \(\theta\), it reduces to MLE of fully observed model.</p>

<p><img src="/assets/images/pgm/m_step_for_lower_bound.png" alt="" /></p>

<p>Thus, we are pushing up the lower bounds until convergence.</p>

<p><strong>Q</strong>: pushing up upper bound only means the minimum value is increasing. Does it necessarily mean the actual value is increasing as well?</p>

<h2 id="hmmbaum-welch-algorithm">HMM(Baum Welch Algorithm)</h2>

<p>The labels are not observed(POS tags, NER labels, etc).</p>

<p>Complete log likelihood:</p>

<p><img src="/assets/images/pgm/hmm_complete_log_likelihood.PNG" alt="" /></p>

<p>Expected complete log likelihood:</p>

<p><img src="/assets/images/pgm/hmm_expected_complete_log_likelihood.png" alt="" /></p>

<p>Note how the above formula is constructed: for each type of parameter, we make its likelihood expectation over all the hidden variables that are involved in the parameter. In this way, we know that we want to perform inference on:</p>

<ul>
  <li>\(p(y_{t} \vert \mathbf{x})\)(actually, once we know this, the next term is easy to compute)</li>
  <li>
    <script type="math/tex; mode=display">p(y_{t-1}, y_{t} \vert \mathbf{x})</script>
  </li>
</ul>

<p>The EM algorithm(also called Baum Welch algorithm) for HMM:</p>

<ul>
  <li>E step:
<img src="/assets/images/pgm/hmm_e_step.png" alt="" /></li>
  <li>M step:
<img src="/assets/images/pgm/hmm_m_step.png" alt="" /></li>
</ul>

<h2 id="em-for-bn-in-general">EM for BN in general</h2>

<p><img src="/assets/images/pgm/em_for_BN_in_general.png" alt="" /></p>

<p>For E step, we are essentially doing inference and accumulating expected sufficient statistics</p>

<ul>
  <li><strong>Q</strong>: are we collection ESS for each node? For Baum Welch algorithm, it’s not the case.
Actually, once we collected the expected value for all hidden nodes, then we ca use them to infer whatever quantity. In the HMM case, <script type="math/tex">p(y_{t}, y_{t+1} \vert \mathbf{x})</script> can be calculated directly/easily from the single node case. So the above procedure is general enough.</li>
  <li><strong>Q</strong>: what are the ESS for the Baum Welch case?
The expected value for \(p(y_t \vert \mathbf{x})\)</li>
  <li><strong>Q</strong>: how to cope with partially observed MN?
Two inference sub problem: infer the hidden nodes and partition part.</li>
</ul>

<h2 id="other-mixture-models">Other mixture models</h2>

<h3 id="conditional-mixture-model">Conditional mixture model</h3>

<p>Application example: give real-value data points, we would like to fit several lines each of which explain non-overlapping parts of the data points.</p>

<p><img src="/assets/images/pgm/conditiona_mixture_model_example.png" alt="" /></p>

<p>where input data’s cluster membership is conditionally dependent on its value.</p>

<p><img src="/assets/images/pgm/conditiona_mixture_model_graph.png" alt="" /></p>

<p>The membership is determined by \(x\) through <em>softmax</em> function and the value \(y\) via linear regression.</p>

<p>Model:</p>

<p><img src="/assets/images/pgm/conditiona_mixture_model.png" alt="" /></p>

<p>Expected complete log likelihood:</p>

<p><img src="/assets/images/pgm/conditiona_mixture_model_expected_complete_log_likelihood.png" alt="" /></p>

<ul>
  <li>E step:
<img src="/assets/images/pgm/conditiona_mixture_model_e_step.png" alt="" /></li>
  <li>M step:
    <ul>
      <li>normal equation for standard LE with data re-weighted by \(\tau\)</li>
      <li>or using IRLS</li>
      <li><strong>Q</strong>: how?</li>
    </ul>
  </li>
</ul>

<h3 id="mixture-of-overlapping-experts">Mixture of overlapping experts</h3>

<p>In contrast to the previous model, the cluster membership is marginally independent on \(x\).</p>

<p><img src="/assets/images/pgm/mixture_of_overlapping_experts_graph.png" alt="" /></p>

<p>Note the difference in e step:</p>

<p><img src="/assets/images/pgm/mixture_of_overlapping_experts_e_step.png" alt="" /></p>

<h3 id="incomplete-datapartially-hidden-data">Incomplete data(partially hidden data)</h3>

<p>For example, in HMM, only part of the labels are given. Then what to do? Easy, treat the given one as fixed while the missing one as latent and estimate them.</p>

<p><img src="/assets/images/pgm/partially_hidden_data_log_likelihood.png" alt="" /></p>

<p>The EM we are talking about previously can be seen as a special case of EM for partially hidden data. In the previous EM, all data are missing while here part are missing.</p>

<h2 id="em-variants">EM variants</h2>

<h3 id="sparse-em">Sparse EM</h3>

<p>For the latent variable of some data points, its value can be quite certain(posterior is closes to 0 or 1). Thus, it won’t help much in estimating them at <strong>every</strong> iteration. Instead, we can estimate those “dead” points once a while. By keeping track of the change rate of the posterior value, we can have a active list of data points and only recompute those in the list.</p>

<h3 id="generalizedincomplete-em">Generalized(incomplete) EM</h3>

<p>Sometimes, closed form of solution in E/M step is not available, we improve the likelihood a bit for example, via gradient step.</p>

<h2 id="summary-on-em">Summary on EM</h2>

<p>Good:</p>

<ul>
  <li>very general(can be applied to all graphical model)</li>
  <li>no learning rate</li>
  <li>guaranteed to improve likelihood</li>
</ul>

<p>Bad:</p>

<ul>
  <li>stuck in local minimum</li>
  <li>slower than conjugate gradient</li>
  <li>expensive inference</li>
</ul>

<h2 id="disclaimer">Disclaimer</h2>

<p>Some portions of the content are directly taken from the <a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture9-EM.pdf">slides</a> of CMU Probabilistic Graphical Model, 2014 by Eric Xing</p>

				</span>
				<div class="journal-date">Published 30 September 2015</div>
				<div class="journal-tags">
				
				
	 
		<a class="tag-cont" href="/tags.html#pgm-ref">
			<div class="tag-l"></div>
			<div class="tag-c">pgm</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#latent-variable-ref">
			<div class="tag-l"></div>
			<div class="tag-c">latent-variable</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#em-ref">
			<div class="tag-l"></div>
			<div class="tag-c">em</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#gmm-ref">
			<div class="tag-l"></div>
			<div class="tag-c">gmm</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#hmm-ref">
			<div class="tag-l"></div>
			<div class="tag-c">hmm</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#baum-welch-ref">
			<div class="tag-l"></div>
			<div class="tag-c">baum-welch</div>
			<div class="tag-r"></div>
		</a>
	



				</div>
			</div>
		</div>
		<div class="clearboth"></div>
	</div>
</div>
	<div class="clearboth"></div>
	


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'xiaohan2012'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




	<div class="clearboth"></div>
</div>


			</div>
			<div class="clearboth pagebottom"></div>
		</div>
	</div>
	<footer>
	<div class="footer-940">
		<div class="footer-general-info">
			© 2014 Han Xiao.<br><br>
			Content licensed under:<br>
			<a class="cc" href="http://creativecommons.org/licenses/by-sa/3.0/">c a b</a><br>
			<a href = "/about.html">About Me</a><br>
		</div>
		<div class="footer-col-cont">
			<div class="footer-nav-col">
				<h4><a>Categories</a></h4>
				<ul>
					
					


  
     
    	<li><a href="/categories.html#paper-reading-ref">
    		paper-reading <span>5</span>
    	</a></li>
     
    	<li><a href="/categories.html#experience-ref">
    		experience <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#misc-ref">
    		misc <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#software-ref">
    		software <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#note-ref">
    		note <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#study-note-ref">
    		study-note <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#resources-ref">
    		resources <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#paper-ref">
    		paper <span>12</span>
    	</a></li>
     
    	<li><a href="/categories.html#tutorial-ref">
    		tutorial <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#course-ref">
    		course <span>8</span>
    	</a></li>
     
    	<li><a href="/categories.html#thoughts-ref">
    		thoughts <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#resource-ref">
    		resource <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#weekly-summary-ref">
    		weekly-summary <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#modeling-ref">
    		modeling <span>1</span>
    	</a></li>
    
  


				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a>Pages</a></h4>
				<ul>
					
					
					


  
    
      
    
  
    
      
      	
      	<li><a href="/about.html">About Me</a></li>
      	
      
    
  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/categories.html">Categories</a></li>
      	
      
    
  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  



				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a>Feed</a></h4>
				<ul>
					<li><a href="/atom.xml">Atom Feed</a></li>
					<li><a href="/rss.xml">RSS Feed</a></li>
				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a>Links</a></h4>
				<ul>
				 
					<li><a href = "http://xiaohan2012.github.io/">Han Xiao's Blog</a></li>
				
				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a href = "/about.html">About Me</a></h4>
				<ul>
				 
					<li><a href = "mailto:han.xiao@cs.helsinki.fi">e-mail</a></li>
				
				</ul>
			</div>
			<div class="clearboth"></div>
		</div>
		<div class="clearboth"></div>
	</div>
	<div class="clearboth"></div>
</footer>
	
</body>
</html>

