
<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	
	<title>ACL 2015 tutorial summary: Matrix/Tensor Factorization for NLP</title>
    
	
	<meta name="author" content="Han Xiao">
	
	<link rel="stylesheet" href="/assets/themes/Snail/css/jquery.fancybox.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/main.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/pages/journal.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/team.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/static.css">
	<link rel="stylesheet" href="/assets/themes/Snail/css/errors.css">
	<link rel="stylesheet" href="/assets/themes/Snail/google-code-prettify/prettify.css">
	<link rel="shortcut icon" href="/assets/themes/Snail/img/favicon.ico">
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.0/jquery.min.js"></script>
	<script src="/assets/themes/Snail/google-code-prettify/prettify.js"></script>
	<script type="text/javascript">
	  $(function(){
		$("pre code").addClass("prettyprint linenums");
		prettyPrint();
	  });
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
</head>
<body>
	<noscript>
		&amp;lt;div id="no-js"&amp;gt;Please enable JavaScript in your browser to experience / fully&amp;lt;/div&amp;gt;
	</noscript>
    <div id="page-container">
		<div>
			<nav>
	<div id="nav-l">
	</div>
	<div id="nav-c">
		<ul id="nav-list" style="width: 700px;">
			<li id="home"><a href="/">Home</a></li>
			
			
			
				
				  
				
			 
				
				  
					
					<li id = "About Me"><a href="/about.html">About Me</a></li>
					
				  
				
			 
				
				  
					
					<li id = "Archive"><a href="/archive.html">Archive</a></li>
					
				  
				
			 
				
				  
				
			 
				
				  
					
					<li id = "Categories"><a href="/categories.html">Categories</a></li>
					
				  
				
			 
				
			 
				
				  
				
			 
				
				  
				
			 
				
				  
				
			 
				
				  
					
					<li id = "Tags"><a href="/tags.html">Tags</a></li>
					
				  
				
			 
			
			
		</ul>
		<form id="nav-search" method="GET" action="/search.html">
			<div id="search-right-pix">
				<div id="search-left-pix">
					<div id="search-center-pix">
						<div id="search-icon-pix"></div>
						<input name="query" type="text" placeholder="Search">
					</div>
				</div>
			</div>
		</form>
		
		
	</div>
	<div id="nav-r">
	</div>
</nav>
			<div id="page-content">
				
<div id="page-content">
	<div class="cont932">
	<div id="journal-articles-block">
		<div class="journal-article">
			<div class="journal-post-info">
				<div class="journal-cat-box">
				
				
				<div class="journal-cat-box">

<a href="/categories.html#tutorial-ref" title="tutorial">
	tutorial

</a>
</div>
				
				</div>
			</div>
			<div class="journal-body">
				<h1 class="journal-title">ACL 2015 tutorial summary: Matrix/Tensor Factorization for NLP<span class="author"></span>
				</h1>
				<span class="the-article">
				<h1 id="matrix-and-tensor-factorization-methods-for-natural-language-processing">Matrix and Tensor Factorization Methods for Natural Language Processing</h1>

<p><a href="http://acl2015.org/tutorials-t5.html">Tutorial link</a></p>

<h1 id="matrix-factorization">Matrix Factorization</h1>

<h2 id="basics">Basics</h2>

<p>inner/outter product, Hadamard product(element-wise), element-wise p-norm(Frobenius norm = 2-norm)</p>

<h3 id="matrix-completion-via-low-rank-factorization">Matrix completion via Low-Rank Factorization</h3>

<p><strong>Matrix completion</strong>: recovery of a matrix</p>

<p>Example applications: guessing missing value in survey data, or estimate distance in sensor node network in which each node has limited range of distance sensing</p>

<p><strong>Low-Rank Factorization</strong>: \( Y \approx UV^T \), \( Y \in \mathcal{R}^{N \times M}, U \in \mathcal{R}^{N \times L}, V \in \mathcal{R}^{M \times L}\)</p>

<p>Assumption: \( rank(Y) = L \ll M,N \). In other words, \( Y \) contains <strong>redundancy</strong> and <strong>noise</strong>. We hope to reconstruct \( Y \) using less data based on redundancy.</p>

<p><strong>Why use it?</strong></p>

<h2 id="three-ways-to-factorize">Three ways to factorize</h2>

<ol>
  <li>
    <p><strong>Singular Value Decomposition</strong>
\( Y = UDV^T\), \( Y \in \mathcal{R}^{N \times M}, U \in \mathcal{R}^{N \times N}, D \in \mathcal{R} ^ {N \times M}, V \in \mathcal{R}^{M \times M}\), \( D \) is diagonal</p>

    <p>We truncate \( D \) to achieve low-rank approximation</p>
  </li>
  <li>
    <p><strong>Stochastic Gradient Descent</strong> and <strong>Alternating Least Squares</strong>:
Minimize <script type="math/tex">\left\| \mathbf{Y} - \mathbf{U} \mathbf{V^T} \right\|_F^2</script>.</p>

    <p>SGD slower and counter-intuitive to parallelize compared to Alternating Least Squares, which fixes one parameter matrix, converting the problem to Least Square, which is convex and has closed-form solution.</p>

    <p>The following regularization leads to better readability, compactness and retrieval: <script type="math/tex">\left\| \mathbf{Y} - \mathbf{U} \mathbf{V^T} \right\|_F^2 + \lambda_1 \left\| U \right\|_1 + \lambda_2 \left\| U \right\|_F^2</script></p>

    <p>L1 norm induces sparsity on the user feedback(movie analogy) while L2 norm avoid over-fitting on the item feature vectors</p>

    <p><em>word2vec</em> and <em>GloVe</em> also implicitly/explicitly used matrix factorization. See related paper section.</p>
  </li>
  <li>
    <p><strong>Non-negative Matrix Factorization</strong>
Adding constraint \( U \gt 0\) and  \( V \gt 0 \). Example: topic model, images pixels</p>

    <p>Optimization: Multiplicative update rules(2001) or constrained Alternating Least Squares(2008).</p>

    <table>
      <tbody>
        <tr>
          <td>Connection with PLSA and LDA. \( Y \) is the document-term matrix and \( L \) is the number of topics in this case. \( Y_ij \ = \sum\limit_k^L p(k</td>
          <td>d_i) p(d_ij</td>
          <td>k) \)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<h2 id="misc-for-matrix-factorization">Misc for matrix factorization</h2>

<h3 id="logisitc-loss">Logisitc loss</h3>

<p>Instead of squared loss, sign rank is prposed. See paper section.</p>

<h3 id="relation-extraction-using-matrix-factorization">Relation extraction using matrix factorization</h3>

<p>Uses also negative data(sampled from unobserved cells).</p>

<p>See <em>Riedel, 2013</em>.</p>

<h3 id="inclusion-of-prior">Inclusion of Prior</h3>

<p>Even logic is used. See <em>Rocktäschel 2015</em></p>

<h1 id="tensor-factorization">Tensor Factorization</h1>

<h2 id="basics-1">Basics</h2>

<p>Example: entity-relation-eneity tensor. 3D</p>

<p>Tensor-vector product, tensor-matrix product. outer product of three vectors: rank-1 tensor. <strong>What’s the rank for tensor?</strong></p>

<h2 id="tucker-decomposition">Tucker decomposition</h2>

<p>\( \mathcal{T} = \mathcal{G} \times \mathbf{A} \times \mathbf{B} \times \mathbf{C} \)</p>

<p><strong>Why this strange form?</strong></p>

<ul>
  <li>\( \mathcal{T} \in \mathbb{R}^{p \times r \times q } \)</li>
  <li>\( G\): core tensor. \( \mathbb{R}^{N_1 \times N_2 \times N_3 }\)</li>
  <li>
    <p>\( \mathbf{A} \in \mathbb{R}^{N_1 \times p } \mathbf{B} \in \mathbb{R}^{N_2 \times r } \mathbf{C} \in \mathbb{R}^{N_3 \times q }\): loading matrices</p>
  </li>
  <li><strong>Tucker 3</strong>: nothing is keped fixed</li>
  <li><strong>Tucker 2</strong>: 1 loading matrix is keped fixed. For example, \( \mathcal{T} = \mathcal{G} \times \mathbf{A} \times \mathbf{B} \times \mathit{I} \)</li>
  <li><strong>Tucker 1</strong>: 2 loading matrix is keped fixed, \( \mathcal{T} = \mathcal{G} \times \mathit{I} \times \mathit{I} \times \mathbf{C}  \)</li>
</ul>

<h3 id="papers">Papers</h3>

<p>Old: CANDECOMP and PARAFAC
New: RESCAL(Nickel, 2012)
Applied for semantic compositionality: Van de , 2013</p>

<p>Also related to neural network, Nickel, 2015</p>

<h2 id="collective-matrix-decomposition">Collective matrix decomposition</h2>

<p>See the references</p>

<h2 id="discriminative-factorial-models">Discriminative factorial models</h2>

<ul>
  <li><strong>Factorization machine</strong>: Rendle (2010). SVM + MF -&gt; model interaction bewteen variables. <strong>HOW?</strong></li>
  <li><strong>Reduced rank regression</strong>: the coefficient is factorized. <strong>Why doing so?</strong></li>
  <li><strong>Multi-task learning</strong>: label embedding. <strong>What? Why? How?</strong></li>
  <li><strong>Structured prediction</strong>: learn feature templates(laborsome to produce) from data using MF.
Non-convex optimization using variant of Passive-Aggressive(Crammer, 2006). Or (Lei, 2014)
Applied on SRL and parsing</li>
</ul>

<h1 id="convexification">Convexification</h1>

<p>If non-convex problems can be converted to convex, then we have theoretical guarantees and a bunch of mature tools.</p>

<p>But <strong>how to do the transformation?</strong>: add convex penalty(what? and how?) sum of trace norm</p>

<p>Application: spectral learning in NLP(combined with trace norm). polynomial learning of HMM, Grammar and NMF</p>

<h1 id="related-papers">Related papers</h1>

<h3 id="overview">Overview</h3>

<ul>
  <li>Kolda, Tamara G., and Brett W. Bader. “<a href="http://epubs.siam.org/doi/pdf/10.1137/07070111X">Tensor decompositions and applications.</a>” SIAM review 51.3 (2009): 455-500.</li>
</ul>

<h3 id="word-embedding-and-matrix-factorization">Word Embedding and Matrix factorization</h3>

<ul>
  <li>Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “<a href="http://www-nlp.stanford.edu/pubs/glove.pdf">Glove: Global vectors for word representation.</a>” Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12 (2014): 1532-1543.</li>
</ul>

<h3 id="plsalda-and-nmf">PLSA/LDA and NMF</h3>

<ul>
  <li>Gaussier, Eric, and Cyril Goutte. “<a href="http://dl.acm.org/citation.cfm?id=1076148">Relation between PLSA and NMF and implications.</a>” Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2005.</li>
  <li>Arora, Sanjeev, Rong Ge, and Ankur Moitra. “<a href="http://arxiv.org/abs/1204.1956">Learning topic models–going beyond SVD.</a>” Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE, 2012.</li>
</ul>

<h3 id="sign-rank">Sign rank</h3>

<ul>
  <li>Bouchard, Guillaume, Sameer Singh, and Théo Trouillon. “<a href="http://sameersingh.org/files/papers/logicmf-krr15.pdf">On approximate reasoning capabilities of low-rank vector spaces.</a>” AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches (2015).</li>
</ul>

<h3 id="injecting-prior--domain-knowledge">Injecting prior / domain knowledge</h3>

<ul>
  <li>Chang, <a href="http://research.microsoft.com/apps/pubs/default.aspx?id=226677">Typed Tensor Decomposition of Knowledge Bases for Relation Extraction</a>, ACL, 2014</li>
  <li>Sebastian Riedel, <a href="https://people.cs.umass.edu/~lmyao/papers/univ-schema-tacl.pdf">Relation Extraction with Matrix Factorization and Universal Schemas</a>, Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ‘13) 2013</li>
  <li>Rocktäschel, Tim, Sameer Singh, and Sebastian Riedel. “<a href="http://rockt.github.io/pdf/rocktaschel2015injecting.pdf">Injecting Logical Background Knowledge into Embeddings for Relation Extraction.</a>” Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. 2015.</li>
</ul>

<h3 id="tensor-factorization-on-relation-learning">Tensor factorization on relation learning</h3>

<p>An instance of Tucker 2</p>

<ul>
  <li>Nickel, Maximilian, Volker Tresp, and Hans-Peter Kriegel. “<a href="http://www.cip.ifi.lmu.de/~nickel/data/paper-icml2011.pdf">A three-way model for collective learning on multi-relational data.</a>” Proceedings of the 28th international conference on machine learning (ICML-11). 2011.</li>
  <li>Nickel, Maximilian, Volker Tresp, and Hans-Peter Kriegel. <a href="http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf">Factorizing YAGO: scalable machine learning for linked data</a>, WWW, 2012</li>
</ul>

<h3 id="tensor-factorization-on-semantic-compositionaliy">Tensor factorization on semantic compositionaliy</h3>

<ul>
  <li>Van de Cruys, Tim, Thierry Poibeau, and Anna Korhonen. “<a href="http://www.aclweb.org/anthology/N13-1134.pdf">A tensor-based factorization model of semantic compositionality.</a>” Conference of the North American Chapter of the Association of Computational Linguistics (HTL-NAACL). 2013.</li>
  <li>Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich,  <a href="http://arxiv.org/pdf/1503.00759v2.pdf">A Review of Relational Machine Learning for Knowledge Graphs</a>, IEEE, 2015 (<strong>relation ship to neural network</strong>)</li>
</ul>

<h3 id="combined-matrix--tensor-factorization">Combined matrix / tensor factorization</h3>

<ul>
  <li>Singh, Sameer and Rocktaschel, Tim and Riedel, Sebastian, <a href="http://rockt.github.io/pdf/singh2015towards.pdf">Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction</a>, NAACL Workshop on Vector Space Modeling for NLP (VSM), 2015</li>
</ul>

<h3 id="collective-matrix-factorization">Collective matrix factorization</h3>

<ul>
  <li>Singh, Ajit P., and Geoffrey J. Gordon. “<a href="http://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf">Relational learning via collective matrix factorization.</a>” Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2008.</li>
</ul>

<h3 id="bayesian-matrix-factorization">Bayesian matrix factorization</h3>

<ul>
  <li>Singh, Ajit P., and Geoffrey Gordon. “<a href="http://www.cs.cmu.edu/~ggordon/singh-gordon-relational.pdf">A Bayesian matrix factorization model for relational data.</a>” arXiv preprint arXiv:1203.3517 (2012).</li>
  <li>Salakhutdinov, Ruslan, and Andriy Mnih. “<a href="https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf">Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.</a>” Proceedings of the 25th international conference on Machine learning. ACM, 2008.</li>
</ul>

<h3 id="bayesian-collective-matrix-factorization">Bayesian collective matrix factorization</h3>

<ul>
  <li>Klami, Arto, Guillaume Bouchard, and Abhishek Tripathi. “<a href="http://arxiv.org/pdf/1312.5921.pdf">Group-sparse embeddings in collective matrix factorization.</a>” arXiv preprint arXiv:1312.5921 (2013).</li>
</ul>

<p>Bayesian tensor factorization?
Collective tensor factorization?</p>

<h3 id="factorization-machine">Factorization machine</h3>

<ul>
  <li>Steffen Rendle (2010): <a href="http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf">Factorization Machines</a>, in Proceedings of the 10th IEEE International Conference on Data Mining (ICDM 2010), Sydney, Australia.</li>
</ul>

<h3 id="reduced-rank-regression">Reduced rank regression</h3>

<ul>
  <li>Alan Julian Izenman, Reduced-rank regression for the multivariate linear model, 1975</li>
</ul>

<h3 id="multi-task-learning">Multi task learning</h3>

<ul>
  <li>Kishan Wimalawarne, <a href="http://papers.nips.cc/paper/5628-multitask-learning-meets-tensor-factorization-task-imputation-via-convex-optimization.pdf">Multitask learning meets tensor factorization: task imputation via convex optimization</a>, NIPS, 2014</li>
</ul>

<h3 id="structured-prediction-for-nlp">Structured prediction for NLP</h3>

<ul>
  <li>Lei, Tao, et al. “<a href="http://www.anthology.aclweb.org/P/P14/P14-1130.pdf">Low-rank tensors for scoring dependency structures.</a>” Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Vol. 1. 2014.</li>
  <li>Lei, Tao, et al. “<a href="https://people.csail.mit.edu/taolei/papers/naacl2015.pdf">High-order lowrank tensors for semantic role labeling</a>.” Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics–Human Language Technologies (NAACLHLT 2015), Denver, Colorado. 2015.</li>
  <li>Crammer, Koby, et al. “<a href="http://machinelearning.wustl.edu/mlpapers/paper_files/CrammerDKSS06.pdf">Online passive-aggressive algorithms.</a>” The Journal of Machine Learning Research 7 (2006): 551-585.</li>
</ul>

<h3 id="convexification-1">Convexification</h3>

<ul>
  <li>Bouchard, Guillaume, Dawei Yin, and Shengbo Guo. “<a href="http://www.jmlr.org/proceedings/papers/v31/bouchard13a.pdf">Convex collective matrix factorization.</a>” Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics. 2013.</li>
  <li>Ryota Tomioka, Taiji Suzuki, <a href="http://papers.nips.cc/paper/4453-statistical-performance-of-convex-tensor-decomposition.pdf">Statistical Performance of Convex Tensor Decomposition</a>, NIPS,  2011</li>
  <li>Hsu, Daniel, Sham M. Kakade, and Tong Zhang. “<a href="http://colt2009.cs.mcgill.ca/papers/011.pdf">A spectral algorithm for learning hidden Markov models.</a>” Journal of Computer and System Sciences 78.5 (2012): 1460-1480.</li>
</ul>

<p><a href="https://www.google.com/#q=convex+penalty+sum+of+trace+norms">Useful Google search</a></p>

<h1 id="what-i-have-learned">What I have learned</h1>

<ul>
  <li>What is matrix completion lower-rank factorization. Connection with rank, redundancy and the inherit nature of guessing missing values.</li>
  <li>PLDA/LDA/word2vec/GloVe uses MF either explicitly/implicitly(<strong>more to read</strong>)</li>
  <li>sign rank is another type of loss function(<strong>more to read</strong>)</li>
  <li>SVD is a special case of LRF</li>
  <li>How LRF/NMF can be solved using SGD and ALS</li>
  <li>MF can be used for relation extraction. Logic and background knowledge can be incorporated(<strong>more be read</strong>)</li>
  <li>Tensor and Tucker decomposition</li>
  <li>Tensor factorization on relation learning and semantic compositionality(<strong>more to read</strong>)</li>
  <li>New subject: (Bayesian)collective matrix decomposition and intuition on its usefulness: sharing parameters among factors(<strong>more be read</strong>)</li>
  <li>MF can be used in predictive/discrinative models: factorization machine, multi-task learning, reduced rank regression, structured prediction for NLP(<strong>more be read</strong>)</li>
  <li>New subject: convexification (<strong>more be read</strong>)</li>
</ul>

				</span>
				<div class="journal-date">Published 09 September 2015</div>
				<div class="journal-tags">
				
				
	 
		<a class="tag-cont" href="/tags.html#matrix-factorization-ref">
			<div class="tag-l"></div>
			<div class="tag-c">matrix-factorization</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#knowledge-base-completion-ref">
			<div class="tag-l"></div>
			<div class="tag-c">knowledge-base-completion</div>
			<div class="tag-r"></div>
		</a>
	 
		<a class="tag-cont" href="/tags.html#relation-learning-ref">
			<div class="tag-l"></div>
			<div class="tag-c">relation-learning</div>
			<div class="tag-r"></div>
		</a>
	



				</div>
			</div>
		</div>
		<div class="clearboth"></div>
	</div>
</div>
	<div class="clearboth"></div>
	


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'xiaohan2012'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




	<div class="clearboth"></div>
</div>


			</div>
			<div class="clearboth pagebottom"></div>
		</div>
	</div>
	<footer>
	<div class="footer-940">
		<div class="footer-general-info">
			© 2014 Han Xiao.<br><br>
			Content licensed under:<br>
			<a class="cc" href="http://creativecommons.org/licenses/by-sa/3.0/">c a b</a><br>
			<a href = "/about.html">About Me</a><br>
		</div>
		<div class="footer-col-cont">
			<div class="footer-nav-col">
				<h4><a>Categories</a></h4>
				<ul>
					
					


  
     
    	<li><a href="/categories.html#paper-reading-ref">
    		paper-reading <span>5</span>
    	</a></li>
     
    	<li><a href="/categories.html#experience-ref">
    		experience <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#misc-ref">
    		misc <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#software-ref">
    		software <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#note-ref">
    		note <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#study-note-ref">
    		study-note <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#resources-ref">
    		resources <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#paper-ref">
    		paper <span>12</span>
    	</a></li>
     
    	<li><a href="/categories.html#tutorial-ref">
    		tutorial <span>2</span>
    	</a></li>
     
    	<li><a href="/categories.html#course-ref">
    		course <span>8</span>
    	</a></li>
     
    	<li><a href="/categories.html#thoughts-ref">
    		thoughts <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#resource-ref">
    		resource <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#weekly-summary-ref">
    		weekly-summary <span>1</span>
    	</a></li>
     
    	<li><a href="/categories.html#modeling-ref">
    		modeling <span>1</span>
    	</a></li>
    
  


				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a>Pages</a></h4>
				<ul>
					
					
					


  
    
      
    
  
    
      
      	
      	<li><a href="/about.html">About Me</a></li>
      	
      
    
  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/categories.html">Categories</a></li>
      	
      
    
  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  



				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a>Feed</a></h4>
				<ul>
					<li><a href="/atom.xml">Atom Feed</a></li>
					<li><a href="/rss.xml">RSS Feed</a></li>
				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a>Links</a></h4>
				<ul>
				 
					<li><a href = "http://xiaohan2012.github.io/">Han Xiao's Blog</a></li>
				
				</ul>
			</div>
			<div class="footer-nav-col">
				<h4><a href = "/about.html">About Me</a></h4>
				<ul>
				 
					<li><a href = "mailto:han.xiao@cs.helsinki.fi">e-mail</a></li>
				
				</ul>
			</div>
			<div class="clearboth"></div>
		</div>
		<div class="clearboth"></div>
	</div>
	<div class="clearboth"></div>
</footer>
	
</body>
</html>

