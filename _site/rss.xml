<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
        <title>Han Xiao's Blog</title>
        <description>Han Xiao's Blog - Han Xiao</description>
        <link>http://simpleyyt.github.io</link>
        <link>http://simpleyyt.github.io</link>
        <lastBuildDate>2015-10-09T19:25:38+02:00</lastBuildDate>
        <pubDate>2015-10-09T19:25:38+02:00</pubDate>
        <ttl>1800</ttl>


        <item>
                <title>Overlapping Community Detection in Labeled Graphs</title>
                <description>&lt;h1 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h1&gt;

&lt;p&gt;Goal: finding dense communities that can be succinctly described by label set&lt;/p&gt;

&lt;p&gt;Input:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;label set \(L\)&lt;/li&gt;
  &lt;li&gt;graph \(G = (V, E,\mathcal{l})\), where \(V\) is vertex set, \(E\) the edge set and \(l\) mapping from vertex to label subset: \(V \rightarrow 2^{\vert L \vert}\)&lt;/li&gt;
  &lt;li&gt;\(k\), the number of communities we want to detect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;list of label set, \(S_i, \ldots S_k\), where \(S_i \in L\)&lt;/li&gt;
  &lt;li&gt;list of &lt;em&gt;disjoint&lt;/em&gt; edge set,  \(F_i, \ldots F_k\), where \(F_i \in E\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;so that that resulting list of subgraphs \(H_i = (p(S_i), F_i)\) maximizes the density:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d(H_i, \ldots, H_k) = \sum\limits_i d(H_i)&lt;/script&gt;

&lt;p&gt;where \(d(H_i) = \frac{2 \vert F_i \vert}{\vert p(S_i) \vert }\), which is the average degree of \(H_i\).&lt;/p&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;h2 id=&quot;unlabeled-graph&quot;&gt;Unlabeled graph&lt;/h2&gt;

&lt;h2 id=&quot;labeled-graph&quot;&gt;Labeled graph&lt;/h2&gt;

</description>
                <link>http://simpleyyt.github.io/paper/2015/10/09/overlapping-community-detection-in-labeled-graphs</link>
                <guid>http://simpleyyt.github.io/paper/2015/10/09/overlapping-community-detection-in-labeled-graphs</guid>
                <pubDate>2015-10-09T00:03:41+02:00</pubDate>
        </item>

        <item>
                <title>Graph summary problem</title>
                <description>&lt;h1 id=&quot;dynamic-network&quot;&gt;Dynamic network&lt;/h1&gt;

&lt;h2 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h2&gt;

&lt;p&gt;Given email communication records, we have input:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r^{(i)} = (u_s^{(i)}, u_t^{(i)}, \mathbf{L}^{(i)}, t^{(i)}), i=1 \ldots M&lt;/script&gt;

&lt;p&gt;where we have \(M\) communication records in total. For the \(r^{(i)}\)th record:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(u_s^{(i)}\): sender&lt;/li&gt;
  &lt;li&gt;\(u_t^{(i)}\): receiver&lt;/li&gt;
  &lt;li&gt;\( \mathbf{L}^{(i)} = {l_1^{(i)} \ldots l_{\vert \mathbf{L}^{(i)} \vert}^{(i)}}\): a set of labels associated with the email content&lt;/li&gt;
  &lt;li&gt;\(t^{(i)}\): time the email was sent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the output, a summary of the reocrds should be the top \(K\) &lt;em&gt;main events&lt;/em&gt; that best capture what happened  in the network. The \(i\)th event is defined by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{(i)} = (\mathbf{U}^{(i)}, \mathbf{L}_e^{(i)}, t_1^{(i)}, t_2^{(i)}), i=1 \ldots K&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(\mathbf{U}^{(i)}\): the people/users that are involved in the event,&lt;/li&gt;
  &lt;li&gt;\(\mathbf{L}_e^{(i)}\): the labels that best describe the event&lt;/li&gt;
  &lt;li&gt;\(t_1^{(i)}, t_2^{(i)}\): the time interval in which the event happened&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some design issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Besides the time interval, should be also include the &lt;strong&gt;communication strength&lt;/strong&gt; at different time points?&lt;/li&gt;
  &lt;li&gt;Should we include all the labels that appear in the event-speicific records? Probably not, especially for event involing a lot of emails. Then what is the selection criteria?&lt;/li&gt;
  &lt;li&gt;Same question for the users. Again, it’s desirable to select only the important users involved in the event. And the same question. How to select the important ones?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some desired properties of the events:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;coverage&lt;/strong&gt;: they should cover as much as the whole communication network. Note that some email records, users and labels are not included in any of the events as they are relatively trivial&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;dense&lt;/strong&gt;: emails that happened within some organization/department/group tend to be in the same event&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;time continuity&lt;/strong&gt;: emails that happened within relatively short time span tend to be in the same event&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;topical consistency&lt;/strong&gt;: emails that describe similar topic tend to be in the same event&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;clustering-approach&quot;&gt;Clustering approach&lt;/h2&gt;

&lt;h3 id=&quot;hard-clusteringk-kmeans&quot;&gt;Hard clustering(K-kmeans)&lt;/h3&gt;

&lt;p&gt;The similarity function should consider time, user closeness(in graph), topic relatedness&lt;/p&gt;

&lt;p&gt;Potential issue: some trivial/singleton/outlier record(For example, Joe sent me a linl, that’s it) should not be included into any of the event. Is there such a clustering algorithm that handles that?&lt;/p&gt;

&lt;p&gt;Some graphical illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/graph-summary/hard-clustering-illustration.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;soft-clusteringmixture-model&quot;&gt;Soft clustering(mixture model)&lt;/h3&gt;

&lt;p&gt;Instead of using hard clustering, we can switch to soft clustering, for example, using mixture model(EM algorithm).&lt;/p&gt;

&lt;p&gt;We can model it as a generative process, where we first select the event, then generate the time/topic/user conditioned on the event.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/images/graphviz/gen-380a7e15a367cb6b35a6346640b2898d.gv.png&quot; /&gt;&lt;/div&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(T\): sending time, multinomial distribution&lt;/li&gt;
  &lt;li&gt;\(L\): label, multinomial distribution&lt;/li&gt;
  &lt;li&gt;\(U\): user, multinomial distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Potential benefit of mixture model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hard clustering result can be obtained using MAP(selecting the most likely cluster assignment)&lt;/li&gt;
  &lt;li&gt;For the user/label selection critia, we can set a threshold as probability is assigned to the generating process from event to time/topic/user.&lt;/li&gt;
  &lt;li&gt;Once being Bayesian, we can add our prior as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dense-subgraph-detection-approach&quot;&gt;Dense subgraph detection approach&lt;/h2&gt;

&lt;p&gt;Find a list of \(K)\ subgrpahs that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;cover the entire graph as much as possible&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;dynamic-ego-network&quot;&gt;Dynamic ego network&lt;/h1&gt;

&lt;h1 id=&quot;static-network&quot;&gt;Static network&lt;/h1&gt;
</description>
                <link>http://simpleyyt.github.io/modeling/2015/10/08/graph-summary</link>
                <guid>http://simpleyyt.github.io/modeling/2015/10/08/graph-summary</guid>
                <pubDate>2015-10-08T14:24:14+02:00</pubDate>
        </item>

        <item>
                <title>Dynamics of Personal Social Relationships in Online Social Networks: a Study on Twitter</title>
                <description>&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2512949&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For three different groups of Twitter users, they studied:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the active life span(how long do they use)&lt;/li&gt;
  &lt;li&gt;the ego network evolution over time(only mention and reply considered)
    &lt;ul&gt;
      &lt;li&gt;# of alters contacted&lt;/li&gt;
      &lt;li&gt;# of alters actively contacted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;# of non-direct communication(hashtag, url) on daily basis&lt;/li&gt;
  &lt;li&gt;how the average time since last contact chagnes over time&lt;/li&gt;
  &lt;li&gt;statibility of ego network structure(the layer model)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;related-questions-for-email-network&quot;&gt;Related questions for email network&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;What are the different groups of email users distinguished by their usage pattern?&lt;/li&gt;
  &lt;li&gt;For each group, what are the patterns?
    &lt;ul&gt;
      &lt;li&gt;Do they add new contact at constant rate?&lt;/li&gt;
      &lt;li&gt;How actively contacted alters change over time?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Compared to Twitter, how is email ego network different?&lt;/li&gt;
  &lt;li&gt;If we get statistics from large number of online users, we can do more. For example:
    &lt;ul&gt;
      &lt;li&gt;Are we using emails less than before?&lt;/li&gt;
      &lt;li&gt;How does the type of traffic change on email network? Are we receiving more promotional emails than personal ones as online business gains popularity?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/10/07/dynamics-of-personal-social-relationship-in-online-social-network</link>
                <guid>http://simpleyyt.github.io/paper/2015/10/07/dynamics-of-personal-social-relationship-in-online-social-network</guid>
                <pubDate>2015-10-07T17:25:09+02:00</pubDate>
        </item>

        <item>
                <title>Rhythms of Information Flow through Networks by Jure Leskovec</title>
                <description>&lt;h2 id=&quot;topics&quot;&gt;Topics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Meme tracking in news media&lt;/li&gt;
  &lt;li&gt;Temporal patterns: what are the different types of temporal patterns&lt;/li&gt;
  &lt;li&gt;Predicting the influence(time series analysis): how many sites will report this news?&lt;/li&gt;
  &lt;li&gt;Inferring latent information diffusion network: how information is spread in the implicit network&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cascading-behavior-in-large-blog-graphs-patterns-and-a-modelhttpscsstanfordedupeoplejurepubsblogs-sdm07pdf-sdm-07&quot;&gt;&lt;a href=&quot;https://cs.stanford.edu/people/jure/pubs/blogs-sdm07.pdf&quot;&gt;Cascading Behavior in Large Blog Graphs Patterns and a model&lt;/a&gt;, SDM 07&lt;/h2&gt;

&lt;p&gt;Patterns of information propagation  –&amp;gt; how information(rumor, idea) spreads.&lt;/p&gt;

&lt;p&gt;Contribution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;temporal patterns: no bursty behavoir and post popularity drops with power law(&lt;strong&gt;?&lt;/strong&gt;)(instead of exponentially)&lt;/li&gt;
  &lt;li&gt;topological patterns: the star shape is most popular&lt;/li&gt;
  &lt;li&gt;generating model: some model that mimics the cascading effect&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;meme-tracking-and-the-dynamics-of-the-news-cyclehttpswwwcscornelleduhomekleinberkdd09-quotespdf&quot;&gt;&lt;a href=&quot;https://www.cs.cornell.edu/home/kleinber/kdd09-quotes.pdf&quot;&gt;Meme-tracking and the Dynamics of the News Cycle&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;What to track? Phrase, tags, links, tags, quotes, images, bar codes..&lt;/p&gt;

&lt;p&gt;In this case, quotes.&lt;/p&gt;

&lt;p&gt;How to identify variants of quotes?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create DAG by adding edges between quotes if one contains the another as a substring(up to some edit distances)&lt;/li&gt;
  &lt;li&gt;Apply heuristic-based graph partitioning to cluster them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Synchronization&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Are blogs ahead or trailing of main stream media?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why use quote? Why not use phrase?&lt;/p&gt;

&lt;h2 id=&quot;patterns-of-temporal-variation-in-online-mediahttpscsstanfordedupeoplejurepubsmemeshapes-wsdm11pdf&quot;&gt;&lt;a href=&quot;https://cs.stanford.edu/people/jure/pubs/memeshapes-wsdm11.pdf&quot;&gt;Patterns of Temporal Variation in Online Media&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Patterns of information attention. the shape of of information piece’s volume over time. How it gains popularity and loss popularity.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What classes of shapes?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Clustering time-varying volume shapes. Properties of similarity metric: 1, invariance to scale, 2, invariance to translation(offset)&lt;/p&gt;

&lt;p&gt;K-means doesn’t work. &lt;strong&gt;Why?&lt;/strong&gt; . Instead, k-spectral centroid clustering works.&lt;/p&gt;

&lt;p&gt;Different types of media give different patterns.&lt;/p&gt;

&lt;h2 id=&quot;modeling-information-diffusion-in-implicit-networkshttpcsstanfordedupeoplejurepubslim-icdm10pdf&quot;&gt;&lt;a href=&quot;http://cs.stanford.edu/people/jure/pubs/lim-icdm10.pdf&quot;&gt;Modeling Information Diffusion in Implicit Networks&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Information attention prediction(classical time series prediction problem).&lt;/p&gt;

&lt;p&gt;Basic question: given who and when the information is reported, how many sites will mention this later on.&lt;/p&gt;

&lt;p&gt;Use influence function: how many sites mention this information after this site mentions it in correlation with this site?&lt;/p&gt;

&lt;p&gt;How to estimate the influence function?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discretize each function as a vector(size 24 for example)&lt;/li&gt;
  &lt;li&gt;Use least square to solve&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compared to standard time series regression, AR and ARMA. How do they work and why this model is better?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: modeling it in explicit network?&lt;/p&gt;

&lt;h2 id=&quot;inferring-networks-of-diffusion-and-influencehttpdlacmorgcitationcfmid1835933&quot;&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1835933&quot;&gt;Inferring Networks of Diffusion and Influence&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;How does the information really spread?&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;virus propagation: only know who and when got inflected. But don’t know who inflected whom&lt;/li&gt;
  &lt;li&gt;word of mouth: only see who bought what and when but don’t know who influenced whom&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(P(G \vert C) \): how possible is \(G\) the generating graph for the inflections \(C\)? How to compute this quantity? How to find the one corresponding to maximum likelihood&lt;/p&gt;

&lt;p&gt;Intuition: if two nodes get inflected at two points of time quite close to each other, then there is probably an edge between the two nodes.&lt;/p&gt;

&lt;p&gt;Information diffusion process betwen two nodes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;flip a biased coin&lt;/li&gt;
  &lt;li&gt;sample some delay time(or incubation time)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Consider only trees(no loop). NP-hard(max-k-cover). can use greedy hill climbing, at each iteration, add the edge that gives the most improvement to the score.&lt;/p&gt;

&lt;p&gt;Experiment:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simulated data: better precision/reclal curve than baseline&lt;/li&gt;
  &lt;li&gt;On real data: topical cluster(political, entertainment, tech) as well as node position in the graph which indicates their topical preference.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-questions&quot;&gt;Further questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Sentiment change&lt;/li&gt;
  &lt;li&gt;Extensions to other domains: epidimiology, viral marketing&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;resrouceslinks&quot;&gt;Resrouces/links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://videolectures.net/eswc2011_leskovec_flow/&quot;&gt;2011 talk by Jure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://snap.stanford.edu/&quot;&gt;SNAP by Stanford&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://snap.stanford.edu/data/index.html&quot;&gt;Dataset by Snap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.mmds.org/&quot;&gt;Mining Massive Data Set book/course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-question-for-email-network&quot;&gt;Related question for email network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Meme tracking: track what I have been doing and at what involvement.
    &lt;ul&gt;
      &lt;li&gt;What is the meme for this case? How to extract the meme?&lt;/li&gt;
      &lt;li&gt;Maybe categorize the flow by event type, for example conference submission/course/research etc. How to define the event?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Temporal pattern: what are the temporal patterns for events in user’s email?
    &lt;ul&gt;
      &lt;li&gt;How to define the pattern? Response time/rate to different users? volumes of emails to certain users at different time points? Why do we study this problem?&lt;/li&gt;
      &lt;li&gt;Can we categorize those patterns? The simplest case, we just don’t reply(conference calls, promotion, etc). Or some short reply to stranger? Long-term and frequent communication with some cooperator or relatively short term communication with a course staff?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How information flows in email network? Or organizing the internal “logic” of the email timelines.
    &lt;ul&gt;
      &lt;li&gt;Why do we bother it? What do you mean by “information flow”? Can we find other information flow patterns besides those indicated by the staff network structure(this is not for ego network)?&lt;/li&gt;
      &lt;li&gt;For ego network, can we model how two different emails are correlated with each other or to what degree are they correlated. For example, one mail might be a simple extension of the work mentioned in another mail. Or some progress reporting based on some previously assigned task.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Email timeline chaining/segmentation: can we chain the emails so that those in the same story line are in the same chain? Potentially useful for email organization or retrospection.
    &lt;ul&gt;
      &lt;li&gt;Is this like a special clustering problem where two emails might be in the same chain if they share similar topic, share common recipients and their timestamp are “reasonable”(impossible that two emails whose timestamp are within 1 sec are in the same line)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What kind of information can we leverage for more efficient email using or less cognitive/mental load for the user?
    &lt;ul&gt;
      &lt;li&gt;What causes the load/inefficiency? Too many emails?&lt;/li&gt;
      &lt;li&gt;Grouping/sorting emails? Hide unnecessary/trivial emails?&lt;/li&gt;
      &lt;li&gt;Need some reading on cognitive science?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;What does it mean by temporal patterns and one way/metric to cluster different patterns&lt;/li&gt;
  &lt;li&gt;In meme tracking, a new way to clustering/partitioning quotes by adding edges(considering substring edit distance)&lt;/li&gt;
  &lt;li&gt;Time series prediction method: AR, ARMA and via the influence function&lt;/li&gt;
  &lt;li&gt;A new problem: modeling how information is spread and method to approach it&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/tutorial/2015/10/06/rhythms-of-information-flow-through-network</link>
                <guid>http://simpleyyt.github.io/tutorial/2015/10/06/rhythms-of-information-flow-through-network</guid>
                <pubDate>2015-10-06T01:52:55+02:00</pubDate>
        </item>

        <item>
                <title>Weekly Summary</title>
                <description>&lt;h1 id=&quot;done&quot;&gt;Done&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Finished watching and summary writing of:
    &lt;ol&gt;
      &lt;li&gt;Learning in Markov network, several algorithms for several configuration of the network and potential function&lt;/li&gt;
      &lt;li&gt;EM algorithm. I started to appreciate its generality on probabilistic graphical models)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Made the following progress on my thesis topic
    &lt;ol&gt;
      &lt;li&gt;Discussed with Aris and narrowed our attention to email network. And we only focus on ego network as it avoids the privacy issue&lt;/li&gt;
      &lt;li&gt;Found some literatures of the following topics:
        &lt;ul&gt;
          &lt;li&gt;Anomaly/outlier detection of graphs&lt;/li&gt;
          &lt;li&gt;Event detection in dynamic graphs&lt;/li&gt;
          &lt;li&gt;Hierarchical topic mining&lt;/li&gt;
          &lt;li&gt;Topic evolution&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Something we can do for the ego email network:
        &lt;ul&gt;
          &lt;li&gt;Topic evolution modeling for the user’s email even on finer granurity, e.g, different groups&lt;/li&gt;
          &lt;li&gt;Event detection/classification of incoming email&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;next-plan--questions&quot;&gt;Next plan / questions&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Consider other possible directions.
    &lt;ul&gt;
      &lt;li&gt;more reading&lt;/li&gt;
      &lt;li&gt;email grouping / tag suggestion / user-user relationship categorization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Let’s try event detection for now. Before doing it, try to define what an event is?(change of group? change of topic? Some deadline/holiday comes?) Try to think about it from both text/content and graph structure perspective.&lt;/li&gt;
  &lt;li&gt;Try to find interesting topics for ego network(communication strengths of different groups for example)&lt;/li&gt;
  &lt;li&gt;Try to find what aspects a user might be interested for his own email network(an retrospect of what he has been doing for the past months?)&lt;/li&gt;
  &lt;li&gt;Topic evolution is cool. Think about what are they possible solutions for topic modeling is possible.&lt;/li&gt;
  &lt;li&gt;How can email user experience be further improved? What are the desire features a user want?&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>http://simpleyyt.github.io/weekly-summary/2015/10/03/weekly-study-summary</link>
                <guid>http://simpleyyt.github.io/weekly-summary/2015/10/03/weekly-study-summary</guid>
                <pubDate>2015-10-03T00:19:38+02:00</pubDate>
        </item>

        <item>
                <title>Useful paper on email mining</title>
                <description>&lt;h2 id=&quot;network-structure&quot;&gt;Network structure&lt;/h2&gt;

&lt;h3 id=&quot;only-user&quot;&gt;Only user&lt;/h3&gt;

&lt;p&gt;Edge between A and B can be defined as A sent an email to B. Edge weight can be the number of emails A sent to B.&lt;/p&gt;

&lt;h3 id=&quot;both-user-and-email&quot;&gt;Both user and email&lt;/h3&gt;

&lt;p&gt;We can add email node as the bridge between user-user interaction. For example, two edges from user U1 to email E and from E to user U2 can be added if U1 sent E to U2.&lt;/p&gt;

&lt;h3 id=&quot;more-information&quot;&gt;More information&lt;/h3&gt;

&lt;p&gt;Various types of nodes/edges maybe added to enrich the network:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Two emails might be connected by their heading in title. For example: the “[Hwfat2]” in some title “[Hwfat2] HWFA meeting …”.&lt;/li&gt;
  &lt;li&gt;Two user might be connected if they are in the same group&lt;/li&gt;
  &lt;li&gt;Two emails might share some important keywords, for example, “Deadline for KDD” and such keywords can be a different type of node&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;other-similar-networks&quot;&gt;Other similar networks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Forum/Q&amp;amp;A network: user sends post and received replies. Just like email network&lt;/li&gt;
  &lt;li&gt;Cellphone message network&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;community-labelingboth-static-and-dynamic&quot;&gt;Community labeling(both static and dynamic)&lt;/h2&gt;

&lt;p&gt;Simplest idea on &lt;strong&gt;static&lt;/strong&gt; graph:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Detect the community&lt;/li&gt;
  &lt;li&gt;Get BoW representation for each community, so we have a matrix(row: community, column: terms)&lt;/li&gt;
  &lt;li&gt;Selecting the important terms by either:
    &lt;ul&gt;
      &lt;li&gt;reweighing the matrix using TdIdf&lt;/li&gt;
      &lt;li&gt;topic mining on the entire corpus(possibly adding some network structure constraint, e.g, Mei, Qiaozhu, et al. “&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1367512&quot;&gt;Topic modeling with network regularization.&lt;/a&gt;” Proceedings of the 17th international conference on World Wide Web. ACM, 2008.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How to extend to &lt;strong&gt;dynamic&lt;/strong&gt; graph? What do we want?&lt;/p&gt;

&lt;p&gt;Following is one related paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Large-Scale Community Detection on YouTube for Topic Discovery and Exploration&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topiceventanomaly-detection&quot;&gt;Topic/event/anomaly detection&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Emerging Topic Detection on Twitter based on Temporal and Social Terms Evaluation&lt;/em&gt;: only terms are extracted. We can extract summaries/phrases&lt;/li&gt;
  &lt;li&gt;ePeriodicity: Mining Event Periodicity from Incomplete Observations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;outlier-detection&quot;&gt;Outlier detection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.eecs.umich.edu/~dkoutra/tut/icdm14.html#outline&quot;&gt;ICDM 2014 Tutorial Node and graph similarity: Theory and Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/edbt15_jkuck.pdf&quot;&gt;Query-Based Outlier Detection in Heterogeneous Information Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/pubs/217054/gupta14_tkde.pdf&quot;&gt;Outlier Detection for Temporal Data: A Survey(Section 6)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs.ucsb.edu/~victor/pub/ucsb/mae/references/ranshous-anomaly-detection-in-networks-survey-2014.pdf&quot;&gt;Anomaly Detection in Dynamic Networks: A Survey&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/icdm14_hzhuang.pdf&quot;&gt;Mining Query-Based Subnetwork Outliers in Heterogeneous Information Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.www2015.it/documents/proceedings/companion/p793.pdf&quot;&gt;Community Change Detection in Dynamic Networks in Noisy Environment&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Local Learning for Mining Outlier Subgraphs from Network Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;event-detection&quot;&gt;Event detection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www3.cs.stonybrook.edu/~leman/wsdm13/WSDM13-Tutorial%20-%20PartII.pdf&quot;&gt;Event detection in dynamic graphs&lt;/a&gt;: contains &lt;strong&gt;a lot of references&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1403.0989&quot;&gt;Detecting change points in the large-scale structure of evolving networks&lt;/a&gt;: with some experiment on email dataset: Enron&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1529618&quot;&gt;Link-based event detection in email communication networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;role-detection&quot;&gt;Role detection&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Danilevsky, Marina, et al. “&lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/mds13_mdanilevsky.pdf&quot;&gt;Entity role discovery in hierarchical topical communities&lt;/a&gt;” Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2013.
    &lt;ul&gt;
      &lt;li&gt;Problem:
        &lt;ol&gt;
          &lt;li&gt;constructed a hierarchy of topical communities from network data. in other words, communities are labeled,&lt;/li&gt;
          &lt;li&gt;infered roles of the entities&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Related work:
        &lt;ol&gt;
          &lt;li&gt;Hierarchical community detection&lt;/li&gt;
          &lt;li&gt;Role discovery&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topic-evoluion&quot;&gt;Topic evoluion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/um/people/weiweicu/images/flow.pdf&quot;&gt;TextFlow: Towards Better Understanding of Evolving Topics in Text&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Topic merging/splitting;&lt;/li&gt;
      &lt;li&gt;visualizaiton&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1646076&quot;&gt;Detecting topic evolution in scientific literature: how can citations help?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1183653&quot;&gt;Topic evolution and social interactions: how authors effect research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;keyword-extraction-for-document--topic-mining&quot;&gt;Keyword extraction for document / topic mining&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Extracting Key Terms From Noisy and Multi-theme Documents&lt;/em&gt;: one graph per document
Can we take into account more documents in the graph so that keywords is not soled defined by the document itself but also other documents?&lt;/li&gt;
  &lt;li&gt;Automatic Construction and Ranking of Topical Keyphrases on Collections of Short Documents(not so related)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;community-detection-in-general&quot;&gt;Community detection in general&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1134280&quot;&gt;Mining hidden community in heterogeneous social networks&lt;/a&gt;: from single-network, user-independent analysis to multi-network, user-dependant, and query-based analysis&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Combining link and content for community detection&lt;/em&gt;: a discriminative approach: probabilistic approach&lt;/li&gt;
  &lt;li&gt;Detecting Overlapping Communities from Local Spectral Subspaces&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ceur-ws.org/Vol-1308/paper6.pdf&quot;&gt;Recipient suggestion for electronic messages using local social network data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nested-community-detection&quot;&gt;Nested community detection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2629511&quot;&gt;Uncoveirng Hierarchical and Overlapping Communities with a Local-First Approach&lt;/a&gt;
Related to the paper in role detection.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;interesting-papers-from-icdm&quot;&gt;Interesting papers from ICDM&lt;/h2&gt;

&lt;h3 id=&quot;mining-social-network&quot;&gt;Mining social network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Linyun Yu, Peng Cui, Fei Wang, Chaoming Song, and Shiqiang Yang, “&lt;a href=&quot;http://arxiv.org/pdf/1505.07193v1.pdf&quot;&gt;From Micro to Macro: Uncovering and Predicting Information Cascading Process with Behavioral Dynamics&lt;/a&gt;”, ICDM, 2015.
    &lt;ul&gt;
      &lt;li&gt;Problem: cascade size/time/process prediction&lt;/li&gt;
      &lt;li&gt;Related to email: how long/popular will this topic lasts?&lt;/li&gt;
      &lt;li&gt;Related topic: survival model(), influence modeling(selecting influential person to start a big cascade)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Suhas Ranganath, Suhang Wang, Xia Hu, Jiliang Tang and Huan Liu &lt;a href=&quot;http://www.public.asu.edu/~swang187/publications/ICDM_2015.pdf&quot;&gt;Finding Time-Critical Responses for Information Seeking in Social Media&lt;/a&gt;, ICDM 2015
    &lt;ul&gt;
      &lt;li&gt;Problem: rank responders for a question to provide timely and relevant answer&lt;/li&gt;
      &lt;li&gt;Seems that no network structural information is used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scott Deeann Chen, Ying-Yu Chen, Jiawei Han, and Pierre Moulin &lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/icdm13_schen.pdf&quot;&gt;A Feature-Enhanced Ranking-Based Classifier for Multimodal Data and Heterogeneous Information Networks&lt;/a&gt;, ICDM 2013
    &lt;ul&gt;
      &lt;li&gt;Rank and classify at the same time on heterogenous network by propagating class information via edges&lt;/li&gt;
      &lt;li&gt;Very interesting ideas in general&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Mining Multi-Aspect Reflection of News Events in Twitter: Discovery, Linking and Presentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;spatio-temporal-mining&quot;&gt;Spatio-temporal mining&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Jingrui He, Yan Liu andRichard Lawrence &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.6366&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Rare Category Detection on Time-Evolving Graphs&lt;/a&gt;, ICDM 2008
    &lt;ul&gt;
      &lt;li&gt;Problem: identify examples of rare classes in unlabeled dataset(fraud detection, network intrusion detection)&lt;/li&gt;
      &lt;li&gt;Extension: detect spam in forum/email network&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;network-mining&quot;&gt;Network mining&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Learning Predictive Substructures with Regularization for Network Data&lt;/li&gt;
  &lt;li&gt;Misael Mongiovi, &lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6729538&amp;amp;tag=1&quot;&gt;Mining Evolving Network Processes&lt;/a&gt;, ICDM 2013
    &lt;ul&gt;
      &lt;li&gt;Problem: mining smoothly evolving process(e.g, traffic jam, information foragin). Given a struturally static graph and time-varying edge weights, find snapshots of subgraphs that are smoothly evolving&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;text-mining&quot;&gt;Text mining&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Chi Wang, Marina Danilevsky, Jialu Liu, Nihit Desai, Heng Ji, Jiawei Han &lt;a href=&quot;http://nlp.cs.rpi.edu/paper/icdm13.pdf&quot;&gt;Constructing Topical Hierarchies in Heterogeneous Information Networks&lt;/a&gt;, ICDM 2013
    &lt;ul&gt;
      &lt;li&gt;Utilizes both &lt;strong&gt;linked&lt;/strong&gt; entity network and &lt;strong&gt;type&lt;/strong&gt; information to construct topical &lt;strong&gt;hierarchies&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Contains references to:
        &lt;ol&gt;
          &lt;li&gt;topical hierarchy construction&lt;/li&gt;
          &lt;li&gt;topic mining in hetero network&lt;/li&gt;
          &lt;li&gt;incorporating type information&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Chenguang Wang, Yangqiu Song, Ahmed El-Kishky, Dan Roth, Ming Zhang, Jiawei Han, &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2783374&quot;&gt;Incorporating World Knowledge to Document Clustering via Heterogeneous Information Networks&lt;/a&gt;, KDD 2015
    &lt;ul&gt;
      &lt;li&gt;A general framework for machine learning under supervision from world knowledge&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Topic Periodicity Discovery from Text Data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;random-stuff&quot;&gt;Random stuff&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Guo-Jun Qi, Charu C. Aggarwal, Jiawei Han, Thomas Huang&lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/www13_gqi.pdf&quot;&gt;Mining Collective Intelligence in Diverse Groups&lt;/a&gt;, WWW 2013
    &lt;ul&gt;
      &lt;li&gt;Goal: aggregate collective observations to infer the true values(e.g, annotation in Amazon Turk)&lt;/li&gt;
      &lt;li&gt;Not modeled as a graph problem. But somewhat interesting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/aistat13_qgu2.pdf&quot;&gt;Unsupervised Link Selection in Networks&lt;/a&gt;, JMLR 2013
    &lt;ul&gt;
      &lt;li&gt;Usercase: eliminating unreliable edges(for example, spam, deceptive “following/like”)&lt;/li&gt;
      &lt;li&gt;Can be used for spam detection, graph compression.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;possible-topics&quot;&gt;Possible topics&lt;/h1&gt;

&lt;h2 id=&quot;topic-evolution&quot;&gt;Topic evolution&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/um/people/weiweicu/images/flow.pdf&quot;&gt;TextFlow: Towards Better Understanding of Evolving Topics in Text&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;captures the merging and splitting of topics(the image in the paper is quite cool):&lt;/li&gt;
  &lt;li&gt;captures important events&lt;/li&gt;
  &lt;li&gt;cool visualization&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Extensions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can we incorporate network structure to constrain the topics, just like &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1646076&quot;&gt;Detecting topic evolution in scientific literature: how can citations help?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Can we capture the evolving topics for user/community/group?&lt;/li&gt;
  &lt;li&gt;Can we devise a visualization method for topic evolution taking group structure into account?&lt;/li&gt;
  &lt;li&gt;Can we add hierarchy to both/either topic or group structure?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1183653&quot;&gt;Topic evolution and social interactions: how authors effect research&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;models how topics are transitioned though Markov transition matrix&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Maybe the “topic transition” can be applied.&lt;/p&gt;

&lt;h2 id=&quot;outlieranomalyevent-detection&quot;&gt;Outlier/anomaly/event detection&lt;/h2&gt;

&lt;h3 id=&quot;definition-of-outlier&quot;&gt;Definition of outlier?&lt;/h3&gt;

&lt;p&gt;What does it mean by outlier/anomaly for email applications?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A new contact that haven’t contacted you before&lt;/li&gt;
  &lt;li&gt;Community outlier(some guy from a totally different area in terms of a specific research group)&lt;/li&gt;
  &lt;li&gt;Spam&lt;/li&gt;
  &lt;li&gt;Totally unrelated email to the current topic?&lt;/li&gt;
  &lt;li&gt;Some specific structure? Like a event(paper deadline, party discussion)?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the anomaly categorization in &lt;a href=&quot;http://cs.ucsb.edu/~victor/pub/ucsb/mae/references/ranshous-anomaly-detection-in-networks-survey-2014.pdf&quot;&gt;Anomaly Detection in Dynamic Networks: A Survey&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can we design a way to detect the hierachicy of events? For example, the start of an new “project-start” event often contains a set of smaller “task” event. Can we capture that using the graph structure? Or more genreally, can we capture the interaction between events? For example, the start of project A is closed related to the start of project B.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;some-ideas&quot;&gt;Some ideas&lt;/h3&gt;

&lt;p&gt;From the tutorial &lt;a href=&quot;http://www3.cs.stonybrook.edu/~leman/wsdm13/WSDM13-Tutorial%20-%20PartII.pdf&quot;&gt;Event detection in dynamic graphs&lt;/a&gt;, they seem to capture the graph similarity shift in terms of similarity.&lt;/p&gt;

&lt;p&gt;Can we leverage more on the content side? For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cis.uab.edu/zhang/Spam-mining-papers/Parameter.Free.Bursty.Events.Detection.in.Text.Streams.pdf&quot;&gt;Parameter Free Bursty Events Detection in Text Streams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;responder-recommentation&quot;&gt;Responder recommentation&lt;/h2&gt;

&lt;p&gt;Based on Suhas Ranganath, Suhang Wang, Xia Hu, Jiliang Tang and Huan Liu &lt;a href=&quot;http://www.public.asu.edu/~swang187/publications/ICDM_2015.pdf&quot;&gt;Finding Time-Critical Responses for Information Seeking in Social Media&lt;/a&gt;, ICDM 2015.&lt;/p&gt;

&lt;p&gt;This paper does not use graph information&lt;/p&gt;

&lt;h2 id=&quot;email-grouping&quot;&gt;Email grouping&lt;/h2&gt;

&lt;p&gt;Can we group the incoming/unread emails so that similar ones(course registraion by student) are together and for time saving purpose, the user can process them all together?&lt;/p&gt;

&lt;p&gt;A little like network compression problem&lt;/p&gt;

&lt;h2 id=&quot;labeling-edgesrelationships&quot;&gt;Labeling edges/relationships&lt;/h2&gt;

&lt;p&gt;Labeling relationship between two email users. For example, the relationship between A and B can be captured by “machine learning” and “thesis”&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Communication/social network:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;community detection(addition: hierarchy)&lt;/li&gt;
  &lt;li&gt;community labeling / topic mining(different variants: with/without network structure)&lt;/li&gt;
  &lt;li&gt;role discovery&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If time added, becomes dynamic network:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;event detection(structural change)&lt;/li&gt;
  &lt;li&gt;topic evolution&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;paper-sources&quot;&gt;Paper sources&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Google scholar&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://icdm2015.stonybrook.edu/program/schedules&quot;&gt;ICDM 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/resource/2015/10/01/graph_mining_topics</link>
                <guid>http://simpleyyt.github.io/resource/2015/10/01/graph_mining_topics</guid>
                <pubDate>2015-10-01T18:25:46+02:00</pubDate>
        </item>

        <item>
                <title>PGM: The Expectation-Maximization Algorithm </title>
                <description>&lt;p&gt;We are discussing EM for only &lt;strong&gt;BN&lt;/strong&gt; here.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-mixture-model&quot;&gt;Gaussian Mixture Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Task is to learn: \(\pi_k, \mu_k, \sigma_k\)&lt;/p&gt;

&lt;h3 id=&quot;difficulty-in-parameter-estimation&quot;&gt;Difficulty in parameter estimation&lt;/h3&gt;

&lt;p&gt;Decomposability(local estimation) in fully observed GMM:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/em_mixture_model_for_fully_observed_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In partially observed GMM, parameters are coupled together(sum within logarithm).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/em_mixture_model_for_partially_observed_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;expectation-maximization&quot;&gt;Expectation Maximization&lt;/h3&gt;

&lt;p&gt;In EM, label assignment of each data is fractional(probability).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expectation step&lt;/strong&gt;: calculate the expected value of hidden variables given the current estimation of parameters and data(\(p(z \vert D, \theta)\), posterior inference problem)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Maximization step&lt;/strong&gt;: compute the parameters by maximizing \(\mathcal{l}\) under the expected value of hidden variables(essentially, MLE for fully observed BN)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Related model is K-means. Compared to K-means, label assignment is “hard”(or MAP) in K-means while it’s “soft” in EM.&lt;/p&gt;

&lt;h3 id=&quot;e-step-for-glim&quot;&gt;E step for GLIM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/expected_complete_log_likelihood_for_glim.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;theory-why-em-works&quot;&gt;Theory: why EM works?&lt;/h3&gt;

&lt;p&gt;Why EM works? Essentially, we want to show the marginal likelihood is increasing as EM runs.&lt;/p&gt;

&lt;p&gt;Definition of expected complete log likelihood:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/expected_complete_log_likelihood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Essentially the expectation of complete log likelihood over the distribution of hidden variables.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why do we use the &lt;em&gt;expected complete log likelihood&lt;/em&gt;?
    &lt;ol&gt;
      &lt;li&gt;For notational convenience in deriving the lower bound of \(\mathcal{l}\).&lt;/li&gt;
      &lt;li&gt;Have the picture of &lt;strong&gt;what to infer about&lt;/strong&gt; in the learning process&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For E step:&lt;/p&gt;

&lt;p&gt;Jensen’s equality leads to lower bound for \(\mathcal{l}\).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray*}
\mathcal{l}(\theta; \mathbf{x})
&amp;=&amp; \log p(\mathbf{x} \vert \theta) \\
&amp;=&amp; \log \sum\limits_z p(\mathbf{x}, z \vert \theta) \\
&amp;=&amp; \log \sum\limits_z q(z\vert x) \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vert x)} \\
&amp;\ge&amp; \sum\limits_z  q(z\vert x) \log \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vert x)}
&amp;=&amp; &lt;\mathcal{l}(\theta, x, z)&gt;_q + H(q)
\end{eqnarray*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Running EM is coordinate-ascent where both E closes the gap between \(F(\theta, x)\) and loss function, while M step maximizes the lower bound of \(\mathcal{l}(\theta, x)\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/em_in_terms_of_lower_bound.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lower bound is coined &lt;em&gt;free energy function&lt;/em&gt;, \(F(q, \theta)\).&lt;/p&gt;

&lt;p&gt;Or in the information theoretic view:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{l}(\theta, x) - F(q, \theta) = KL(q \vert \vert p(z \vert x, \theta))&lt;/script&gt;

&lt;p&gt;By pushing up the lower bound, we wish to increase \(\mathcal{l}(\theta, x)\).&lt;/p&gt;

&lt;p&gt;For M step, setting \(q(z \vert x)\) to \(p(z \vert x, \theta)\) leads to the \(F(q, \theta) = \mathcal{l}(\theta, x)\), which indicates \(F(\theta, x)\) is maximized.&lt;/p&gt;

&lt;p&gt;For M step, given the optimal \(q\) in previous step(which can be treated as known) and the entropy term does not depend on \(\theta\), it reduces to MLE of fully observed model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/m_step_for_lower_bound.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus, we are pushing up the lower bounds until convergence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: pushing up upper bound only means the minimum value is increasing. Does it necessarily mean the actual value is increasing as well?&lt;/p&gt;

&lt;h2 id=&quot;hmmbaum-welch-algorithm&quot;&gt;HMM(Baum Welch Algorithm)&lt;/h2&gt;

&lt;p&gt;The labels are not observed(POS tags, NER labels, etc).&lt;/p&gt;

&lt;p&gt;Complete log likelihood:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/hmm_complete_log_likelihood.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Expected complete log likelihood:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/hmm_expected_complete_log_likelihood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note how the above formula is constructed: for each type of parameter, we make its likelihood expectation over all the hidden variables that are involved in the parameter. In this way, we know that we want to perform inference on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(p(y_{t} \vert \mathbf{x})\)(actually, once we know this, the next term is easy to compute)&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_{t-1}, y_{t} \vert \mathbf{x})&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The EM algorithm(also called Baum Welch algorithm) for HMM:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E step:
&lt;img src=&quot;/assets/images/pgm/hmm_e_step.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;M step:
&lt;img src=&quot;/assets/images/pgm/hmm_m_step.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;em-for-bn-in-general&quot;&gt;EM for BN in general&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/em_for_BN_in_general.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For E step, we are essentially doing inference and accumulating expected sufficient statistics&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: are we collection ESS for each node? For Baum Welch algorithm, it’s not the case.
Actually, once we collected the expected value for all hidden nodes, then we ca use them to infer whatever quantity. In the HMM case, &lt;script type=&quot;math/tex&quot;&gt;p(y_{t}, y_{t+1} \vert \mathbf{x})&lt;/script&gt; can be calculated directly/easily from the single node case. So the above procedure is general enough.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: what are the ESS for the Baum Welch case?
The expected value for \(p(y_t \vert \mathbf{x})\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: how to cope with partially observed MN?
Two inference sub problem: infer the hidden nodes and partition part.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-mixture-models&quot;&gt;Other mixture models&lt;/h2&gt;

&lt;h3 id=&quot;conditional-mixture-model&quot;&gt;Conditional mixture model&lt;/h3&gt;

&lt;p&gt;Application example: give real-value data points, we would like to fit several lines each of which explain non-overlapping parts of the data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/conditiona_mixture_model_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where input data’s cluster membership is conditionally dependent on its value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/conditiona_mixture_model_graph.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The membership is determined by \(x\) through &lt;em&gt;softmax&lt;/em&gt; function and the value \(y\) via linear regression.&lt;/p&gt;

&lt;p&gt;Model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/conditiona_mixture_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Expected complete log likelihood:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/conditiona_mixture_model_expected_complete_log_likelihood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E step:
&lt;img src=&quot;/assets/images/pgm/conditiona_mixture_model_e_step.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;M step:
    &lt;ul&gt;
      &lt;li&gt;normal equation for standard LE with data re-weighted by \(\tau\)&lt;/li&gt;
      &lt;li&gt;or using IRLS&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: how?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mixture-of-overlapping-experts&quot;&gt;Mixture of overlapping experts&lt;/h3&gt;

&lt;p&gt;In contrast to the previous model, the cluster membership is marginally independent on \(x\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/mixture_of_overlapping_experts_graph.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note the difference in e step:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/mixture_of_overlapping_experts_e_step.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;incomplete-datapartially-hidden-data&quot;&gt;Incomplete data(partially hidden data)&lt;/h3&gt;

&lt;p&gt;For example, in HMM, only part of the labels are given. Then what to do? Easy, treat the given one as fixed while the missing one as latent and estimate them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/partially_hidden_data_log_likelihood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The EM we are talking about previously can be seen as a special case of EM for partially hidden data. In the previous EM, all data are missing while here part are missing.&lt;/p&gt;

&lt;h2 id=&quot;em-variants&quot;&gt;EM variants&lt;/h2&gt;

&lt;h3 id=&quot;sparse-em&quot;&gt;Sparse EM&lt;/h3&gt;

&lt;p&gt;For the latent variable of some data points, its value can be quite certain(posterior is closes to 0 or 1). Thus, it won’t help much in estimating them at &lt;strong&gt;every&lt;/strong&gt; iteration. Instead, we can estimate those “dead” points once a while. By keeping track of the change rate of the posterior value, we can have a active list of data points and only recompute those in the list.&lt;/p&gt;

&lt;h3 id=&quot;generalizedincomplete-em&quot;&gt;Generalized(incomplete) EM&lt;/h3&gt;

&lt;p&gt;Sometimes, closed form of solution in E/M step is not available, we improve the likelihood a bit for example, via gradient step.&lt;/p&gt;

&lt;h2 id=&quot;summary-on-em&quot;&gt;Summary on EM&lt;/h2&gt;

&lt;p&gt;Good:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;very general(can be applied to all graphical model)&lt;/li&gt;
  &lt;li&gt;no learning rate&lt;/li&gt;
  &lt;li&gt;guaranteed to improve likelihood&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bad:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;stuck in local minimum&lt;/li&gt;
  &lt;li&gt;slower than conjugate gradient&lt;/li&gt;
  &lt;li&gt;expensive inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture9-EM.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/30/pgm-8-em</link>
                <guid>http://simpleyyt.github.io/course/2015/09/30/pgm-8-em</guid>
                <pubDate>2015-09-30T01:33:45+02:00</pubDate>
        </item>

        <item>
                <title>Graphical Model: Learning in Fully Observed Markov Networks</title>
                <description>&lt;h2 id=&quot;structural-learning&quot;&gt;Structural Learning&lt;/h2&gt;

&lt;p&gt;Precision matrix(Q matrix) and covariance matrix(C matrix): different implication.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;C matrix: captures marginal dependence and independence&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Q matrix: captures conditional dependence and independence&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why this conclusion?
When \(q_{ij}=0\), we have
&lt;img src=&quot;/assets/images/pgm/q_matrix_conditional_independence.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why precision matrix more meaningful than covariance matrix?
Because many seemingly unrelated factors can be correlated(even butterfly and hurricane) thus there is the densely connected graph under then C matrix implication. However under Q matrix, we only consider conditional independence, thus a more sparsely connected graph. Easier to interpret and compute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, some contrast between covariance matrix and precision matrix:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/covariance-and-precision-matrix-example.png&quot; alt=&quot;cov-prec-mat&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If \(n\) is sufficiently large compared to \(p\), which means the covariance matrix is well-conditioned, then it’s easy to get \(Q\).&lt;/p&gt;

&lt;p&gt;However, if \(p \gg n\), data sample size much smaller than data dimension. Covariance matrix is &lt;em&gt;ill-conditioned&lt;/em&gt;, thus not invertible(&lt;strong&gt;why?&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;It’s a hot topic on estimation of the precision matrix for high-dimensional data from limited amount of samples. One way is called &lt;em&gt;Graph Regression&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Graph Regression&lt;/em&gt; works as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use Lasso to selected neighbors for each node
For each &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;, treat it as the output of the linear regression on the rest of variables &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_{-i}&lt;/script&gt;, or &lt;script type=&quot;math/tex&quot;&gt;x_i \sim \mathcal{N}(\mathbf{q}_{i} \mathbf{x}_{-i}, \sigma^2)&lt;/script&gt;. By adding a Lasso regularizer, we can obtain a sparse \(\mathbf{q}_{i}\) and only the non-zero ones are selected as the neighbors.
The &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}_i&lt;/script&gt; might be wrong, but using the below theorem,  the neighbors are correct.&lt;/li&gt;
  &lt;li&gt;Estimate \(Q\) by constraining the nonzero entries in \(Q\) to correspond to the selected neighbors of each node. In this way, the problem reduces to parameter estimation given known graph structure(discussed next)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lasso gives sparse parameter estimation, which is attractive because we would like a sparsely connected graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/linear-regression-with-lasso.png&quot; alt=&quot;lasso&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tuning \(\lambda\): cross validation. At some point, \(\mathbf{q}_{i}\) drops to zero more quickly.&lt;/p&gt;

&lt;p&gt;One theorem on the graph consistency(sparsistency) of Graph Regression algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/sparsistent.png&quot; alt=&quot;sparsistent&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are actually using Lasso to recover the graphical structure.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;discrete&lt;/strong&gt; values, we can use logistic regression.&lt;/p&gt;

&lt;h2 id=&quot;parameter-estimation-given-known-structure&quot;&gt;Parameter Estimation given known structure&lt;/h2&gt;

&lt;p&gt;No decomposibility. Partition function \(Z\) defined on all parameters.&lt;/p&gt;

&lt;p&gt;In order to maximize the log-likelihood:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/log_p_mn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we get the derivative:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/derivative_log_p.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And setting the derivative to zero leads to:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/p_mle.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;which is the condition that must be satisfied for MLE, though this result does not tell us &lt;strong&gt;how&lt;/strong&gt; to estimate.&lt;/p&gt;

&lt;h3 id=&quot;decomposable-ugm&quot;&gt;Decomposable UGM&lt;/h3&gt;

&lt;p&gt;Roadmap:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/estimation-method-table-for-ugm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;decomposable-ugm-1&quot;&gt;Decomposable UGM&lt;/h3&gt;

&lt;p&gt;For triangulated graph and potentials defined on maximal cliques(&lt;strong&gt;why this?&lt;/strong&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/joint-distribution-of-decomposable-ugm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can verify that if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_{MLE}(\mathbf{x}) = \frac{\prod \widetilde{p}(\mathbf{x}_c)}{\prod \widetilde{p}(\mathbf{x}_s)}&lt;/script&gt;

&lt;p&gt;then above condition is satisfied.&lt;/p&gt;

&lt;p&gt;So if the UGM is decomposable, its potentials defined on maximal cliques and potential function is tabular, the potential functions’ value can be directly inspected(easily computed) using the MLE condition.&lt;/p&gt;

&lt;h3 id=&quot;iterative-proportional-fittingipf&quot;&gt;Iterative Proportional Fitting(IPF)&lt;/h3&gt;

&lt;p&gt;Observing that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/ipf_p_over_psi.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can know that this is a iterated function, meaning \( X \rightarrow X \) and finding the closed form solution is hard.&lt;/p&gt;

&lt;p&gt;Fixed point iteration can help:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/ipf_fixed_point_iteration.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Calculating \(p^{(t)}(\mathbf{x}_c)\) is actually an inference problem. What makes learning in MN different to that in BN is the the inference problem nested in the learning problem.&lt;/p&gt;

&lt;p&gt;IPF from the information theoretic view(I-projection).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: &lt;a href=&quot;https://en.wikipedia.org/wiki/Coordinate_descent&quot;&gt;Coordinate ascent&lt;/a&gt; algorithm? Convergence proof?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/ipf_convergence_proof.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above proof relies on \(Z^{(t)}=Z^{(t+1)}\). &lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As we can verify that each step sets the gradient to zerp. Thus \(\mathcal{l}\) increases at each iteration. As \(\mathcal{l}\) is convex(&lt;strong&gt;why?&lt;/strong&gt;), we can show it converges to global optimum.&lt;/p&gt;

&lt;h3 id=&quot;feature-based-clique-potentialsgeneralized-iterative-scaling&quot;&gt;Feature-based clique potentials(Generalized Iterative Scaling)&lt;/h3&gt;

&lt;p&gt;Motivation for feature-based one: fewer parameters while using the same graphical model.&lt;/p&gt;

&lt;p&gt;Basic ideas in learning: instead of directly taking the derivative(because computing \(\log Z \) term is expensive), lower bound is derived using the maximum value of log(tagent line) and Jensen’s inequality for \(\exp\)&lt;/p&gt;

&lt;p&gt;The result is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/gis_result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The \(\sum\limits_x \tilde{p}(x)f_i(x)\) is actually the sum of \(f_i\) in training dataset. If \(f_i\) is binary, it’s essentially counting.&lt;/p&gt;

&lt;p&gt;GIS is more general than IPF as the tabular potential function is a special case of feature based potential function. However, inference needs to be done for \(\sum\limits_x p^{(t)}(x)f_i(x)\).&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture8-LearningObsMRF.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/29/pgm-7-parameter-estimation-in-fully-observed-MN</link>
                <guid>http://simpleyyt.github.io/course/2015/09/29/pgm-7-parameter-estimation-in-fully-observed-MN</guid>
                <pubDate>2015-09-29T01:09:56+02:00</pubDate>
        </item>

        <item>
                <title>Graphical Model: Learning in Fully Observed Bayesian Networks </title>
                <description>&lt;h2 id=&quot;structural-learning&quot;&gt;Structural learning&lt;/h2&gt;

&lt;p&gt;An interesting problem but not many statistically optimal solutions by far. Optimal means maximizing some objective function such as log-likelihood.&lt;/p&gt;

&lt;p&gt;For \(n\)nodes, there are \(O(2^{n^2})\) possible graphs, while \(n!\) trees.&lt;/p&gt;

&lt;h3 id=&quot;chow-liu-algorithm&quot;&gt;Chow-Liu algorithm&lt;/h3&gt;

&lt;p&gt;Chow-Liu algorithm finds the exact solution for an optimal &lt;strong&gt;directed&lt;/strong&gt; tree(under MLE)&lt;/p&gt;

&lt;p&gt;Tricks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;MLE decomposable&lt;/li&gt;
  &lt;li&gt;Each child has only one child&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;decomposing-mle&quot;&gt;Decomposing MLE&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/chowliu-mle-decompose-1.png&quot; alt=&quot;chowliu-mle-1&quot; /&gt;
&lt;img src=&quot;/assets/images/pgm/chowliu-mle-decompose-2.png&quot; alt=&quot;chowliu-mle-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MLE can be decomposed into sum of mutual information between nodes and their parents.&lt;/p&gt;

&lt;p&gt;Several transformation note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Changing the index order \(n\) and \(t\)&lt;/li&gt;
  &lt;li&gt;Use \(count(x_i, x_{\pi_i(G)})\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;single-parent-constraint&quot;&gt;Single parent constraint&lt;/h4&gt;

&lt;p&gt;As each node can only have at most one parent, we only consider the \(\hat{I}\) between &lt;strong&gt;each pair&lt;/strong&gt; of nodes.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{I}(X_i, X_j) = \sum\limits_{x_i, x_j} \hat{p}(x_i, x_j) \log \frac{\hat{p}(x_i, x_j)}{\hat{p}(x_i)\hat{p}(x_j)}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hat{p}(x_i, x_j) = \frac{count(x_i, x_j)}{M}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Given the edge weights(MI) between each pair of nodes, we want to find the set of edges that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;form a tree&lt;/li&gt;
  &lt;li&gt;maximize the sum&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This reduces to maximum spanning tree problem. You can solve it using &lt;a href=&quot;http://mathworld.wolfram.com/KruskalsAlgorithm.html&quot;&gt;Kruskal’s Algorithm&lt;/a&gt; for example.&lt;/p&gt;

&lt;h4 id=&quot;multiple-optimal-trees&quot;&gt;Multiple optimal trees&lt;/h4&gt;

&lt;p&gt;Given one tree topology, there can be multiple BNs with the same loss value. For example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/chow-liu-i-equivalent.png&quot; alt=&quot;ChowLiu-i-equivalent&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, those BNs are I-equivalent.&lt;/p&gt;

&lt;p&gt;However, those I-equivalent BNs have different interpretations.&lt;/p&gt;

&lt;h4 id=&quot;application-dna-sequence-classification&quot;&gt;Application: DNA sequence classification&lt;/h4&gt;

&lt;p&gt;The motif/junk DNA sequence classification example.&lt;/p&gt;

&lt;p&gt;Basic idea: use the tree and CPD as define a density function, apply the pairwise CPD to give the likelihood that a sequence(a list of nodes can be seen as list of pairs of nodes) appear.&lt;/p&gt;

&lt;h2 id=&quot;parameter-learning-for-fully-observable-gm&quot;&gt;Parameter learning for fully observable GM&lt;/h2&gt;

&lt;p&gt;Estimation principles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLE&lt;/li&gt;
  &lt;li&gt;Bayesian estimation&lt;/li&gt;
  &lt;li&gt;Maximal conditional likelihood(part of the graph)&lt;/li&gt;
  &lt;li&gt;Maximal margin(origin from SVM)&lt;/li&gt;
  &lt;li&gt;Maximum entropy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The log likelihood can be decomposed into a set of smaller CPDs, each parametrized by local parameter, \(\theta_i\). Density estimation for the GM can be done independently for each node. As the CPD is actually multinomial, which is an instance of exponential family distribution. We can get the MLE for natural parameter via moment matching.&lt;/p&gt;

&lt;h3 id=&quot;discrete-example-multinomial-distribution&quot;&gt;Discrete example: multinomial distribution&lt;/h3&gt;

&lt;h4 id=&quot;mle&quot;&gt;MLE&lt;/h4&gt;

&lt;p&gt;Objective function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/ll-for-multinomial.png&quot; alt=&quot;ll-for-multinomial&quot; /&gt;&lt;/p&gt;

&lt;p&gt;with constraint:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_k=1^K \theta_k = 1&lt;/script&gt;

&lt;p&gt;Which leads to constraint cost function with Lagrange multiplier&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/cost-function-for-multinomial-in-lagrange-form.png&quot; alt=&quot;cost-for-multinomial-in-lagrange&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus we have \(\lambda = N\). So \(\hat{\theta}_{k, MLE} = \frac{n_k}{N}\)&lt;/p&gt;

&lt;h4 id=&quot;bayesian-estimationdirichlet-prior&quot;&gt;Bayesian Estimation(Dirichlet prior)&lt;/h4&gt;

&lt;p&gt;We use Dirichlet distribution as prior(conjugate prior):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta \vert \alpha) = C(\alpha) \prod\limits_k \theta_{k}^{\alpha_k-1}&lt;/script&gt;

&lt;p&gt;Posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta \vert \mathbf{X}) \propto  \prod\limits_k \theta_{k}^{n_k + \alpha_k-1}&lt;/script&gt;

&lt;p&gt;Posterior mean estimation &lt;script type=&quot;math/tex&quot;&gt;\theta_k = E_{p(\theta \vert \mathbf{X})} (\theta_k) = \frac{n_k + \alpha_k}{N + \sum \alpha_k}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Sequential Bayesian update is equivalent to batch update.&lt;/p&gt;

&lt;p&gt;Beside manually setting \(\alpha\) another way is through Empirical Bayes, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{MLE} = argmax_{\alpha} p(D | \alpha) = \int p(D \vert \theta) p(\theta \vert \alpha) d \theta&lt;/script&gt;

&lt;h4 id=&quot;logistic-normal-prior&quot;&gt;Logistic Normal prior&lt;/h4&gt;

&lt;p&gt;Compared to Dirichlet, it’s more flexible because of co-variance structure, but non-conjugate for multinomial distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/dirichlet-3d.png&quot; alt=&quot;dirichlet-3d&quot; /&gt;
&lt;img src=&quot;/assets/images/pgm/logistic-normal-3d.png&quot; alt=&quot;logistic-normal-3d&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Used in &lt;a href=&quot;https://www.cs.princeton.edu/~blei/papers/BleiLafferty2006.pdf&quot;&gt;Correlated Topic Modeling&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;continuous-example-multivariate-gaussian&quot;&gt;Continuous example: multivariate Gaussian&lt;/h3&gt;

&lt;h4 id=&quot;mle-1&quot;&gt;MLE&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/mle-multivariate-gaussian.png&quot; alt=&quot;mle-multivariate-gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall moment matching(from sufficient statistics to natural parameter), the sufficient statistics here are \(\sum x_n\) and \(\sum x_n x_n^T\)&lt;/p&gt;

&lt;h4 id=&quot;bayesian-estimation&quot;&gt;Bayesian estimation&lt;/h4&gt;

&lt;p&gt;Unknown \(\mu\) and \(\sigma\), the resulting posterior is parametrized by:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/bayesian-estimation-result-for-multivariat-gaussian.png&quot; alt=&quot;mle-multivariate-gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;See the details in &lt;em&gt;Bayesian Data Analysis&lt;/em&gt; book.&lt;/p&gt;

&lt;p&gt;The list of prior distributions for unknown/known \(\mu\) and \(\sigma\)…&lt;/p&gt;

&lt;h2 id=&quot;learning-for-fully-observable-bn-in-general&quot;&gt;Learning for fully observable BN in general&lt;/h2&gt;

&lt;p&gt;Parameter estimation for fully observable BN can be decomposed into local conditional or marginal probability(smaller BNs) whose parameters are globally independently. So we can break down the global parameter estimation into local pieces. We rely on this property to perform efficient parameter estimation.&lt;/p&gt;

&lt;p&gt;An illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/parameter-estimation-for-BN-illustration.png&quot; alt=&quot;illustration for learning for BN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, if the smaller BNs belongs to exponential family, MLE amounts to moment matching.&lt;/p&gt;

&lt;h3 id=&quot;defining-prior-for-graphical-model&quot;&gt;Defining prior for graphical model&lt;/h3&gt;

&lt;p&gt;This is not arbitrary. Guidelines are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Global Parameter Independence: if we want local estimation for efficiency, we cannot couple the priors of different nodes. Instead, the priors should be separated by nodes. For example, \( p(\Theta \vert G)=\prod\limits_i p(\theta_i \vert G)\)&lt;/li&gt;
  &lt;li&gt;Local Parameter Independence: parameters for a single node under different &lt;em&gt;configuration/conditions&lt;/em&gt; shouldn’t be coupled. For example,\(p(\theta_i) = \prod\limits_j p(\theta_{x_i \vert \pi_i(x)_j} \vert G)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some safe priors satisfying the requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dirichlet for discrete DAG&lt;/li&gt;
  &lt;li&gt;Normal prior for Gaussian DAG(\(\mu\) is unknown) and Normal-Wishart prior when \(\sigma\) also unknown.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parameter-sharing&quot;&gt;Parameter sharing&lt;/h3&gt;

&lt;p&gt;Example: the transition and emission probability in HMM. Instead of using one set of parameters indexed by the position, we replicate them along all positions so that only one set of parameters are shared.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Structural learning: ChowLiu algorithm guarantees optimality for directed tree.&lt;/li&gt;
  &lt;li&gt;Learning in fully observable BN is easy and it can be decomposed into local estimation problems, which usually belongs to exponential family&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture7-LearningObs.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/25/pgm-6-parameter-estimation-in-fully-observed-BN</link>
                <guid>http://simpleyyt.github.io/course/2015/09/25/pgm-6-parameter-estimation-in-fully-observed-BN</guid>
                <pubDate>2015-09-25T12:25:05+02:00</pubDate>
        </item>

        <item>
                <title>小彻小悟</title>
                <description>&lt;p&gt;最近再看PGM，看到GLIM的时候各种不懂，加上中午才起来很愧疚，破罐子破摔，下午学了几个小时，晚上便开始忘我的玩耍了。&lt;/p&gt;

&lt;p&gt;玩倒好，结果发现自己玩的也没劲，脑中不停嘀咕，唉，没救了，连玩都不会了。真可悲，学也学不好，玩也玩不好。&lt;/p&gt;

&lt;p&gt;正值风华正茂时，发现自己陷入两难境地，学也学的不尽兴，玩也玩的不过瘾。经常玩的时候想学，学的时候想玩。这是在提醒我要劳逸结合吗？&lt;/p&gt;

&lt;p&gt;愿自己能彻底点，要玩就好好玩，要学就好好学。不要觉得玩是浪费时间，学累了就应该去玩，不是吗？&lt;/p&gt;

&lt;p&gt;如果连玩的劲头也没了，那就去给别人做做好事吧，比如给晴儿买点小礼物啥的；或者找人去聊聊天，毕竟人是社交性动物。&lt;/p&gt;

&lt;p&gt;关于学，我需要去相信积累的力量，理解现在投入不一定会马上有回报，但是如果我足够耐心，能够坚持，回报总会出现的。&lt;/p&gt;

&lt;p&gt;关键的关键在于，我要努力把每天过好，要相信自己能够过好每天，没有人能够对我负责，只有自己对自己负责！&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/thoughts/2015/09/25/thoughts</link>
                <guid>http://simpleyyt.github.io/thoughts/2015/09/25/thoughts</guid>
                <pubDate>2015-09-25T02:39:04+02:00</pubDate>
        </item>

        <item>
                <title>Graphical Model: Exponential Family and Generalized Linear Models</title>
                <description>&lt;h2 id=&quot;exponential-family&quot;&gt;Exponential family&lt;/h2&gt;

&lt;h3 id=&quot;why-do-we-study-it&quot;&gt;Why do we study it?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;it’s a generic form for many ML models&lt;/li&gt;
  &lt;li&gt;if we can apply generic parameter estimation method for this generic form, we can apply it to all its instance models&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;For any numerical random variable \(X\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray*}
p(x \vert \eta)
&amp;=&amp;
h(x)\exp\{\eta^T T(x) - A(\eta)\} \\
&amp;=&amp;
\frac{1}{Z(x)}h(x)\exp\{\eta^T T(x)\} \\
\end{eqnarray*} %]]&gt;&lt;/script&gt;

&lt;p&gt;is an exponential family distribution with natural parameter \(\eta\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(T(x)\): sufficient statistics&lt;/li&gt;
  &lt;li&gt;\(h(x)\): for anything that is not interacting with the natural parameters(for example in Multivariate Gaussian)&lt;/li&gt;
  &lt;li&gt;\(A(\eta)=\log Z(\eta)\): log normalizer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Fundamental models: Bernoulli, multinomial, Gaussian, Poisson, gamma,&lt;/li&gt;
  &lt;li&gt;More complex ones: Ising model(MRS), RBM, CRF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Benefit of CRF:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\mathbf{X}\) is assumed to be inter-dependent&lt;/li&gt;
  &lt;li&gt;we can use global features on \(\mathbf{X}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multivariate Gaussian: moment parameters, \(\sigma\) and \(\mu\)&lt;/li&gt;
  &lt;li&gt;Multinomial distribution: \(\prod\limits_i^K \pi_i^{x_i} = \exp [\sum\limits_i x_i \ln \phi_i] \)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;properties&quot;&gt;Properties&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Moment generating property&lt;/strong&gt;: \(\frac{dA}{d\eta} = E[T(x)] = \mu \) and \( \frac{d^2A}{d\eta^2} = Var[T(x)] &amp;gt; 0 \)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;one-to-one correspondence between \(\mu\) and \(\eta\)&lt;/strong&gt;: convexity of \(A(\eta)\)  =&amp;gt; invertibily of \(\frac{dA}{d\eta} = E[T(x)] = \mu \) =&amp;gt; \(\eta = \Psi(\mu)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;moment matching&lt;/strong&gt;: set \(\frac{\partial \mathcal{l}}{\partial \eta} = 0\) leads to &lt;script type=&quot;math/tex&quot;&gt;\mu_{MLE} = \frac{1}{N} \sum\limits_n T(X_n)&lt;/script&gt; =&amp;gt; &lt;script type=&quot;math/tex&quot;&gt;\eta_{MLE} = \Psi(\mu_{MLE})&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moment estimation leads to parameter estimation&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: why do we need such moments?
 Moment is easily defined for exponential family and once we know the moment as well as the inverse function(from moment to parameter), we can get the parameter value quite easily&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: how is the inverse function, \(\Psi\) defined?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sufficiency&quot;&gt;Sufficiency&lt;/h3&gt;

&lt;p&gt;Bayesian/Frequentist/Neyman View&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: why this views matter?&lt;/p&gt;

&lt;h3 id=&quot;exponential-family-and-bayesian&quot;&gt;Exponential family and Bayesian&lt;/h3&gt;

&lt;p&gt;Exponential family for both the model and prior&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X \vert \eta) = \exp \{\eta^T T(X) - A(\eta)\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P( \eta) = \exp \{\xi^T T(\eta) - A(\xi)\}&lt;/script&gt;

&lt;p&gt;Then the posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\eta \vert X) = \exp \{\eta^T T(X) + \xi^T T(\eta) + A^{&#39;} + A^{&#39;&#39;}\}&lt;/script&gt;

&lt;p&gt;If \(\eta = T(\eta)\), then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\eta \vert X) = \exp \{T(\eta)(T(X) + \xi) + A^{&#39;} + A^{&#39;&#39;}\}&lt;/script&gt;

&lt;p&gt;In this case, posterior is also exponential family.&lt;/p&gt;

&lt;p&gt;Finally, if \( \eta = T(\eta)\),  \(P(\eta)\) is a conjugate prior for exponential family distribution.&lt;/p&gt;

&lt;h2 id=&quot;generalized-linear-modelsglims&quot;&gt;Generalized Linear Models(GLIMs)&lt;/h2&gt;

&lt;h3 id=&quot;why-do-we-study-it-1&quot;&gt;Why do we study it?&lt;/h3&gt;

&lt;p&gt;It is a general form for regression and classification models.&lt;/p&gt;

&lt;h3 id=&quot;definition-1&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pgm/glims-framework.png&quot; alt=&quot;GLIMs framework&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(Y \sim exponential-Family\)&lt;/li&gt;
  &lt;li&gt;\(\eta = \Psi(\mu = f(\xi = \theta^T X))\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GLIMs is a specific form of exponential family.&lt;/p&gt;

&lt;p&gt;More:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input, \(X\) enter the model through linear combination: \(\xi = \theta^T X\)&lt;/li&gt;
  &lt;li&gt;Conditional mean, \(\mu=f(\xi)\), where \(f\) is response function.&lt;/li&gt;
  &lt;li&gt;Output \(y\) characterized by exponential family distribution, \(p\), with conditional mean \(\mu\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Canonical response function&lt;/strong&gt;: simple case where\(f = \Psi^{-1}(.)\). Thus, \( \eta = \theta^T X\).&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Linear regression: \( P(Y \vert X, \theta) = N(\theta^T X, \sigma^2) \), \(\Psi, f\) both be identity function.&lt;/li&gt;
  &lt;li&gt;Logistic regression: \(P(Y \vert X, \theta) = \mu(X)^Y (1-\mu(X))^(1-Y)\), where \( \mu(X) = sigmoid(X)\). \(f\) be sigmoid and \(\Psi\) be identity function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can improve the input by transforming \(X\), for example, adding non-linearity.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: what’s the connection between exponential family and GLIM?
In GLIM, the interaction between parameter and input is &lt;strong&gt;linear&lt;/strong&gt;, however for exponential family, there is not such constraint.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mle-for-canonical-glim&quot;&gt;MLE for Canonical GLIM&lt;/h2&gt;

&lt;p&gt;Derivative of log-likelihood leads to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\mathbf{l}}{\theta} = X^T (y - \mu)&lt;/script&gt;

&lt;p&gt;Need to solve it iteratively.&lt;/p&gt;

&lt;h3 id=&quot;online-learning&quot;&gt;Online learning&lt;/h3&gt;

&lt;p&gt;One option is SGD, the update rule is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{new} = \theta_{old} + \alpha (y_n - \mu_n)x_n&lt;/script&gt;

&lt;p&gt;\(\alpha\) is the learning rate, which needs to be set.&lt;/p&gt;

&lt;h3 id=&quot;batching-learning-using-iterative-reweighting-least-squareirls&quot;&gt;Batching learning using Iterative Reweighting Least Square(IRLS)&lt;/h3&gt;

&lt;p&gt;\( \alpha\) needs to be set. Hessian can be used to adjust the learning rate.&lt;/p&gt;

&lt;p&gt;This gives rise to Newton Raphson method with cost function \(J\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{new} = \theta_{old} - H^{-1} \nabla_{\theta} J&lt;/script&gt;

&lt;p&gt;General form of Hessian \( H\) for GLIMs:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H = \frac{d^2 \mathbf{l}}{d\theta\theta} = -X^T W X&lt;/script&gt;

&lt;p&gt;Newton Raphson method for GLIMs is called Iteratively Reweighting Least Squares(IRLS), as it takes the form:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_{new} = (X^TW_{old}X)^{-1}X^TW_{old}z_{old}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;z_{old} = X \theta_{old} + W_{old}^{-1}(y-u_{old})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;which is similar to Least Square case, where \( \theta^* = (X^TX)^{-1}X^Ty\).&lt;/p&gt;

&lt;p&gt;In IRLS, we iteratively reweight objective function using \(W\) and solve &lt;script type=&quot;math/tex&quot;&gt;\theta^{new} = argmin_{\theta} (z - X\theta_{old})^T W (z - X\theta_{old})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This method is generic for any exponential family distribution, the difference is \(W\).&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;logistic regression&lt;/li&gt;
  &lt;li&gt;linear regression: \(W=1\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The take-home stuff is: &lt;strong&gt;generic&lt;/strong&gt; MLE for exponential family and GLIMs using IRLS.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: For the above, we only talked about natural/canonical response function, which limits the scope. What about other response functions(e.g, sigmoid)?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Exponential family and MLE amounts to moment matching&lt;/li&gt;
  &lt;li&gt;GLIMs, its canonical version and MLE on it using IRLS&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture6-GLIM.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/23/pgm-5-generalized-linear-model</link>
                <guid>http://simpleyyt.github.io/course/2015/09/23/pgm-5-generalized-linear-model</guid>
                <pubDate>2015-09-23T22:38:32+02:00</pubDate>
        </item>

        <item>
                <title>PGM course: Message Passing</title>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The lecture is organized by the following motivation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;variable elimination is inefficient for multiple queries –&amp;gt; message passing(belief propagation) which can cache &lt;em&gt;messages&lt;/em&gt; and recombine them later for different queries&lt;/li&gt;
  &lt;li&gt;message passing is consistent on trees while not for non-tree. However, tree-like graph can be converted to &lt;em&gt;Factor Tree&lt;/em&gt; in order to produce consistent result&lt;/li&gt;
  &lt;li&gt;Junction tree algorithm as a general exact inference algorithm for any graph structure.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;from-elimination-to-message-passing&quot;&gt;From Elimination to Message Passing&lt;/h2&gt;

&lt;p&gt;Elimination = message passing on &lt;em&gt;clique trees&lt;/em&gt;, where each clique is the resulting elimination clique from certain step of variable elimination.&lt;/p&gt;

&lt;p&gt;One run of variable elimination only answer &lt;strong&gt;one&lt;/strong&gt; query. When we have &lt;strong&gt;multiple&lt;/strong&gt; queries(say, an online inference system), &lt;em&gt;messages&lt;/em&gt; maybe reused.&lt;/p&gt;

&lt;h2 id=&quot;message-passing-on-tree&quot;&gt;Message passing on tree&lt;/h2&gt;

&lt;p&gt;Three types of trees:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;undirected&lt;/li&gt;
  &lt;li&gt;directed: no v-structure&lt;/li&gt;
  &lt;li&gt;poly tree: can have multiple parents(the v-structure)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We talk about undirected and directed tree. Undirected tree is equivalent to directed tree as they make the same conditional independencies.&lt;/p&gt;

&lt;p&gt;A unique path between any pair of node ensures the correctness of our algorithm.&lt;/p&gt;

&lt;p&gt;Definition of &lt;em&gt;message&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{ji}(x_i) = \phi(x_i) \sum\limits_{x_j} \phi(x_i, x_j) \prod\limits_{k \in N(j) \setminus i} m_{kj}(x_j)&lt;/script&gt;

&lt;p&gt;Interpretation of message: a belief from \(x_j\) to \(x_i\)&lt;/p&gt;

&lt;p&gt;Implication: node can send its message to one of its neighbor once it has received messages from &lt;strong&gt;all other&lt;/strong&gt; neighbors.&lt;/p&gt;

&lt;p&gt;Elimination and message passing are equivalent on trees: every elimination of variable can be considered as passing one message.&lt;/p&gt;

&lt;p&gt;Complexity of computing node marginals using dynamic programming: \(2C\), where \(C\) is the complexity of message passing on the tree in one direction. We need two passes(in each direction of the tree).&lt;/p&gt;

&lt;p&gt;Belief Propagation algorithm:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Evidence&lt;/li&gt;
  &lt;li&gt;Choose the root: determines how messages are passed(unique in tree)&lt;/li&gt;
  &lt;li&gt;Collect: recursive procedure that collects messages from neighbors. From leaf to root&lt;/li&gt;
  &lt;li&gt;Distribute: similar to Collect but in the reverse direction. From root to leaf&lt;/li&gt;
  &lt;li&gt;Compute marginal&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sequential and parallel(asynchronous) are both possible&lt;/p&gt;

&lt;p&gt;BP on tree is correct/consistent. For tree: only one way of passing message from one node to the other so that the result is consistent.&lt;/p&gt;

&lt;h2 id=&quot;message-passing-on-non-tree&quot;&gt;Message passing on non-tree&lt;/h2&gt;

&lt;p&gt;Inconsistency on non-tree, for example, the &lt;em&gt;Misconception&lt;/em&gt; example. The same query on two &lt;strong&gt;different&lt;/strong&gt; message passing ways get two different results.&lt;/p&gt;

&lt;p&gt;Basic idea: we can &lt;strong&gt;convert&lt;/strong&gt; the non-tree graph into a tree…&lt;/p&gt;

&lt;p&gt;Factor Graph: graph with variable node and factor node. This type of graph makes explicit the factorization.&lt;/p&gt;

&lt;p&gt;Factor tree: the factor graph in which the distinction between variable nodes and factor nodes are ignored and such graph is an undirected tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: general algorithm to perform this conversion?&lt;/p&gt;

&lt;p&gt;Once we have a tree, we can enjoy consistency of query result,&lt;/p&gt;

&lt;p&gt;Two types of messages during message passing on factor tree:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;from variable to factor: without marginalization&lt;/li&gt;
  &lt;li&gt;from factor to variable: with marginalization&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Poly-trees is treated differently in contrast to undirected/directed tree, we need to convert it to factor tree. (why?)&lt;/p&gt;

&lt;p&gt;Disadvantages of factor tree conversion: we will might encounter a large clique(which is the bottleneck of computation).&lt;/p&gt;

&lt;p&gt;Example: the \(X_2, X_3, X_4\) clique. When passing messages from \(X_2\) and \(X_3\) to \(X_4\) through their factor, we need to sum over \(X_2, X_3\) together.&lt;/p&gt;

&lt;p&gt;More extreme example: Ising model.&lt;/p&gt;

&lt;p&gt;One possible factor tree version is columns/rows of nodes as factors and being connected in a chain.&lt;/p&gt;

&lt;p&gt;BP works for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;tree&lt;/li&gt;
  &lt;li&gt;tree-like graphs&lt;/li&gt;
  &lt;li&gt;poly-trees&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;max-product-algorithm&quot;&gt;Max-product algorithm&lt;/h2&gt;

&lt;p&gt;computing maximum a posterior(MAP)&lt;/p&gt;

&lt;p&gt;replace &lt;em&gt;sum&lt;/em&gt; with &lt;em&gt;max&lt;/em&gt; and extra &lt;em&gt;bookkeeping(?)&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;inference-on-general-gm----junctionclique-tree-algorithm&quot;&gt;Inference on general GM – Junction(clique) tree algorithm&lt;/h2&gt;

&lt;p&gt;Junction tree algorithm: obsolete and complicated but illustrates the principles behind exact and approximate inference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: why obsolete?&lt;/p&gt;

&lt;p&gt;Moralize directed graph -&amp;gt; undirected graph&lt;/p&gt;

&lt;h3 id=&quot;why-triangulation&quot;&gt;why triangulation?&lt;/h3&gt;

&lt;p&gt;It can be shown that: if we want local consistency, we must triangulate.&lt;/p&gt;

&lt;p&gt;If no triangulation for example the Misconception example, we will disobey the running intersection property, thus inconsistency.&lt;/p&gt;

&lt;p&gt;Local consistency property: see slides.&lt;/p&gt;

&lt;p&gt;Guarantee of consistency: running interaction property, which is, when two factors share some node, there should be an edge connecting them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: how can we add edges for triangulation without changing the conditional independence assumption?&lt;/p&gt;

&lt;p&gt;We can manipulate the factors without changing the conditional independence. For example \(\phi(X_1, X_2, X_3)\) can equal to \(\phi(X_1, X_2) \phi(X_2, X_3)\). In this way, independence between \(X_1, X_3\) is ensured.&lt;/p&gt;

&lt;p&gt;How to triangulate? Different triangulation can produce complexity on inference.&lt;/p&gt;

&lt;p&gt;Several methods to update: Hugin update, Shafer-Shenoy update.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Junction tree is not equivalent to the graphical model. It’s just a device to specify how message passing is done in the graphical model.&lt;/p&gt;

&lt;p&gt;Workflow of junction tree algorithm:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;moralize the graph(because we don’t want to introduce more conditional independency)&lt;/li&gt;
  &lt;li&gt;triangulate(NP-hard, but heuristic exists)&lt;/li&gt;
  &lt;li&gt;build clique tree(using maximum-spanning tree algorithm. &lt;strong&gt;why using this algorithm?&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;BP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;First three steps ensure a junction tree with running intersection property, where local consistency guarantees.&lt;/p&gt;

&lt;p&gt;Works for exact inference on general graph. Complexity depends on maximum clique.&lt;/p&gt;

&lt;h2 id=&quot;example-forward-algorithm-for-hmm&quot;&gt;Example: forward algorithm for HMM&lt;/h2&gt;

&lt;p&gt;Define:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_t^k = P(Y_t = k, \mathbf{X})&lt;/script&gt;

&lt;p&gt;which can be seen as the message of \( Y_{t} \).&lt;/p&gt;

&lt;p&gt;Then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_t^k = P(X_t \vert Y_t = k) \sum\limits_i P(Y_t=k \vert Y_{t-1}=i) \alpha_{t-1}^i&lt;/script&gt;

&lt;p&gt;Last:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathbf{X}) = \sum\limits_i \alpha_T^i&lt;/script&gt;

&lt;p&gt;Viterbi decoding can be done via MAP inference, replacing \(\sum\) with \( \max\) and keep the back pointers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: How to view this procedure in terms of junction tree algorithm?&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture5-BP.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/22/pgm-4-message-passing</link>
                <guid>http://simpleyyt.github.io/course/2015/09/22/pgm-4-message-passing</guid>
                <pubDate>2015-09-22T18:20:46+02:00</pubDate>
        </item>

        <item>
                <title>PGM course: Variable Elimination</title>
                <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Two typical tasks of graphical model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inference: \(P(X \vert Y)\)&lt;/li&gt;
  &lt;li&gt;Estimate a model(learning): learning point estimate for model parameters or inference in Bayesian modeling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We talk about inference here. Several types of inference tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;likelihood: \(P(e)\), just marginalization the remaining uninterested variables&lt;/li&gt;
  &lt;li&gt;conditional probability(or posterior belief): \(P(X \vert e)\), which equals to \( \frac{P(X, e)}{P(e)}\)&lt;/li&gt;
  &lt;li&gt;most probable assignment or maximum a posterior: \(argmax_{y \in \mathcal{Y}} P(y \vert e)\), Usually, the unnormalized quantity is considered. Can be used for classification/prediction and explanation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this section, exact inference method called &lt;em&gt;variable elimination&lt;/em&gt; is discussed.&lt;/p&gt;

&lt;h2 id=&quot;complexity-in-general&quot;&gt;Complexity in general&lt;/h2&gt;

&lt;p&gt;Computing \(P(X = x \vert e)\) in a GM is NP-hard. This means this is not &lt;strong&gt;general&lt;/strong&gt; algorithm that can solve the problem in polynomial time.&lt;/p&gt;

&lt;p&gt;Two major inference approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;exact inference: variable elimination, belief propagation(message passing), junction tree algorithm&lt;/li&gt;
  &lt;li&gt;approximate inference: MCMC, variational methods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For certain structures, we can solve it efficiently and exactly.&lt;/p&gt;

&lt;h2 id=&quot;elimination-in-chains&quot;&gt;Elimination in chains&lt;/h2&gt;

&lt;p&gt;Given the graph:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A \rightarrow B \rightarrow C \rightarrow D \rightarrow E&lt;/script&gt;

&lt;p&gt;Calculating  \(P(e)\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray*}
P(e)
&amp; = &amp;
\sum\limits_{a,b,c,d} P(a) P(b \vert a) P(c \vert b) P(d \vert c) P(e \vert d) \\
&amp; = &amp;
\sum\limits_{b,c,d} P(c \vert b) P(d \vert c) P(e \vert d) \sum\limits_{a} P(a) P(b \vert a) \\
&amp; = &amp;
\sum\limits_{b,c,d} P(d \vert c) P(e \vert d) \sum\limits_{b} m(b) P(c \vert b) 
\end{eqnarray*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Complexity drops from \(O(k^n)\) to \(O(nk^2)\), where there are \(n\) nodes and each node takes \(k\) values.&lt;/p&gt;

&lt;p&gt;More examples: HMM, linear chain CRF&lt;/p&gt;

&lt;p&gt;Forward/backward algorithm in quadratic time&lt;/p&gt;

&lt;h2 id=&quot;exact-inference&quot;&gt;Exact inference&lt;/h2&gt;

&lt;p&gt;Inference is in this general form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{z}\prod\limits_{\phi \in \mathcal{F}} \phi&lt;/script&gt;

&lt;p&gt;Essentially elimination of variables. Called &lt;em&gt;sum-product&lt;/em&gt; inference&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-evidence&quot;&gt;Dealing with evidence&lt;/h2&gt;

&lt;p&gt;in a computationally homogeneous way&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tau(Y, e) = \sum\limits_{z, e} \prod\limits_{\phi \in \mathcal{F}} \phi \times \delta(E, e)&lt;/script&gt;

&lt;p&gt;where \(Y\) is the query, \(e\) is the evidence, \(z\) are the hidden variables and \(\delta\) is the evidence potential(indicator)&lt;/p&gt;

&lt;h2 id=&quot;elimination-algorithm&quot;&gt;Elimination algorithm&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;
Let \(\mathcal{F}\) is the full set of factors&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;
Multiply the factors with their evidence potential&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sum-product variable elimination&lt;/strong&gt;
Eliminate one hidden variable, \(z\) at a time
During each elimination, factors \(\mathcal{F}^{‘}\) whose scope contains \(z\) are first selected from \(\mathcal{F}\). Remaining factors \(\mathcal{F}^{‘’} = \mathcal{F}^ - \mathcal{F}^{‘}\).  \(\mathcal{F}^{‘’} \) are multiplied and summed over \(z\). The resulting new factors are \(\tau \cup \mathcal{F} \)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Normalization&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to determine the optimal variable elimination order? NP-hard&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What types&lt;/strong&gt; of graphical model can we perform efficient exact inference on?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;complexity-of-variable-elimination&quot;&gt;Complexity of variable elimination&lt;/h2&gt;

&lt;p&gt;For each elimination step, suppose \(X\) is to be eliminated, \( Y_c \) are in the factors and \( \vert Y_c \vert = k \)&lt;/p&gt;

&lt;p&gt;For the factor multiplication: \( k \times \vert Val(X) \vert \times \prod\limits_i \vert Val(Y_{c_i}) \vert \)&lt;/p&gt;

&lt;p&gt;For the summation:  \( \vert Val(X) \vert \times \vert Val(Y_{c_i}) \vert \)&lt;/p&gt;

&lt;p&gt;Thus, &lt;strong&gt;exponential&lt;/strong&gt; for the intermediate factor values.&lt;/p&gt;

&lt;p&gt;Variable elimination produces a sequence of elimination factors along with a corresponding sequence of elimination cliques for the graphical model.&lt;/p&gt;

&lt;p&gt;Elimination orders determines both the elimination factors and the elimination cliques.&lt;/p&gt;

&lt;p&gt;The complexity is determined by the clique with the largest cardinality, \( \prod\limits_i \vert Val(Y_{c_i}) \vert \)&lt;/p&gt;

&lt;p&gt;Good elimination order lead to cliques with small cardinality.&lt;/p&gt;

&lt;p&gt;Finding the best elimination ordering is NP-hard.&lt;/p&gt;

&lt;p&gt;Example that illustrates the importance of elimination order: star and tree model&lt;/p&gt;

&lt;p&gt;Benefit of graphical model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;figure out the capacity/complexity of a certain elimination order&lt;/li&gt;
  &lt;li&gt;easily figure out a good variable elimination order&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;intractable-exact-inference&quot;&gt;Intractable exact inference&lt;/h2&gt;

&lt;p&gt;Exact inference can be intractable. For example Ising model:&lt;/p&gt;

&lt;p&gt;Inference can lead to clique of size \(n\), the row/column number, which can be quite large(e.g, image)(try to eliminate variables in the Ising model)&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture4-Elimination.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/22/pgm-3-variable-elimination</link>
                <guid>http://simpleyyt.github.io/course/2015/09/22/pgm-3-variable-elimination</guid>
                <pubDate>2015-09-22T16:04:38+02:00</pubDate>
        </item>

        <item>
                <title>PGM course: Markov Network</title>
                <description>&lt;h1 id=&quot;learned-from-book&quot;&gt;Learned from book&lt;/h1&gt;

&lt;p&gt;BN and MN are different. They are incomparable in some sense.
Log-linear model, feature functions
They are be converted into I-map of each other. If MN(BN) is chordal, there exist BN(MN) that is a perfect I-map for it.
Conditional random field: no need to model the distribution for the observed variables.&lt;/p&gt;

&lt;h2 id=&quot;misconception-example&quot;&gt;Misconception Example&lt;/h2&gt;
&lt;p&gt;This example demonstrates one case where BN &lt;strong&gt;cannot&lt;/strong&gt; capture the desired independency assumptions while MN can.&lt;/p&gt;

&lt;h2 id=&quot;parametrization&quot;&gt;Parametrization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;factors(clique potentials): denoted as \(\phi_i(D_i)\)&lt;/li&gt;
  &lt;li&gt;Markov network factorization: a distribution of \(\phi_i(D_i)\) factorizes over \( \mathcal{H} \)if each \(D_i\) in \(\phi_i(D_i)\) is a &lt;em&gt;complete&lt;/em&gt; subgraph of \(\mathcal{H}\))&lt;/li&gt;
  &lt;li&gt;There are multiple ways to factorize a MN(maximal clique or pair-wise)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For CV:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Applications: image denoising, image segmentation, stereo reconstruction&lt;/li&gt;
  &lt;li&gt;Pixel value for each pixel is one variable&lt;/li&gt;
  &lt;li&gt;image denoising: recovering the real pixel value. Node potential for each pixel that penalizes discrepancy between observed value and predicted value. Edge potential that encourages continuity between adjacent pixel values. To avoid overpenalize, truncated norm is used.&lt;/li&gt;
  &lt;li&gt;stereo reconstruction: restore the depth of each pixel.&lt;/li&gt;
  &lt;li&gt;image segmentation: node potential for each superpixel and edge potential for adjacent superpixel to encourage continuity&lt;/li&gt;
  &lt;li&gt;values can be clustered to reduce dimensionality&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;markov-network-independencies&quot;&gt;Markov Network Independencies&lt;/h2&gt;

&lt;p&gt;Too many details here. The conclusion is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pairwise/local(Markov blanket)/global independency assumptions of \( \mathcal{H}\) that \( P \) factorizes on are equivalent if \( P \) is positive&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parametrization-revisited&quot;&gt;Parametrization Revisited&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;energy function: \(-\ln(\phi(D))\)&lt;/li&gt;
  &lt;li&gt;factor graph(consisting of variable node and factor node) makes explicit the factor structure in the network&lt;/li&gt;
  &lt;li&gt;feature: factor without nonnegativity requirement. More compact(something like 1 if .. and 0 otherwise)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;log-linear model: \(P(X_1, X_2, \cdots, X_n) = \frac{1}{Z}\exp[-\sum\limits_{i=1}^{k}w_if_i(\mathbf{D}_i)]\), where \(f_i\) is feature function, \(D_i\) is complete subgraph in \( \mathcal{H}\) and \(w_i\) are feature weights. More compact compared to the factor table that enumerates all combinations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ising model, Boltzmann distribution, their connection and Boltzmann distribution’s connection to activation model for neuron.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Overparametrization: more parameters than needed =&amp;gt; infinite ways to represent the same distribution&lt;/li&gt;
  &lt;li&gt;Canonical factorization defined over all cliques of \( \mathcal{H}\) avoid the parametrization ambiguity.&lt;/li&gt;
  &lt;li&gt;Canonical factorization used to prove the Hammersley-Clifford theorem: if \( \mathcal{H}\) is I-map of \( P\), then \( P\) factorizes over \( \mathcal{H}\)&lt;/li&gt;
  &lt;li&gt;Linearly dependent features lead to non-unique parametrization. Or for linearly independent features, each set of feature weights unique define \(P\)&lt;/li&gt;
  &lt;li&gt;To avoid overparametrization, choose linearly independent features(&lt;strong&gt;how?&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bn-and-mn&quot;&gt;BN and MN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The question is how to convert from one to the other so that independency assumption in the original one is satisfied?&lt;/li&gt;
  &lt;li&gt;Chordal network is the intersection between MN and BN in that it can be represented in the other type of model perfectly if it’s chordal&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;partially-directed-models&quot;&gt;Partially Directed Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Conditional Random Fields: models \(P(Y \vert X)\) instead of \(P(Y, X) \), \(Y\) target variables, \(X\) observed variables. Different partition function.&lt;/li&gt;
  &lt;li&gt;Benefit of conditioning on \(X\): avoid modeling \(X\), which can be quite complex and the possibility to incorporate rich set of features&lt;/li&gt;
  &lt;li&gt;The three different linear-chain graphical models: 1, fully undirected(CRF), 2, partially undirected and 3, fully directed(HMM). 1 and 2 differs in the feature function’s scope on \(X\) and 2 and 3 differs in the way they normalize(local and global) to probability and independency assumptions they make&lt;/li&gt;
  &lt;li&gt;Skip-chain CRF: one observation node can have multiple parents and long-range label dependency&lt;/li&gt;
  &lt;li&gt;Coupled linear-chain CRF: joint inference on two chains(POS and NER)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;learned-from-course-video&quot;&gt;Learned from course video&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Example of &lt;code&gt;sky or water?&lt;/code&gt;: need context information to infer. Introduction to the Ising model and UGM.&lt;/li&gt;
  &lt;li&gt;Local Markov independencies: useful for Gibbs sampling, which requires such knowledge&lt;/li&gt;
  &lt;li&gt;max clique and sub clique parametrization: two different ways for parametrization. They capture the same conditional independencies while max-clique can capture probability distribution that sub-clique cannot catch(for example &lt;em&gt;explicitly&lt;/em&gt; capture the multi-way interaction) but the representation cost(parameter space size) is bigger. Again, compactness vs expressive power.&lt;/li&gt;
  &lt;li&gt;potential function: more flexible/general than probability function. They can be probability function but not necessarily. They measure the compatibility. (Originates from atomic physics where we assign a number to various spin configurations of two atoms) We only have indirect measures sometimes.&lt;/li&gt;
  &lt;li&gt;The way BN and MN normalizes is quite different, which results in their different capabilities: &lt;strong&gt;local normalization&lt;/strong&gt; in BN, of which variables are independent of other variables, is not very good in some cases. For example, long range dependency in natural language where we like &lt;strong&gt;global normalization&lt;/strong&gt; is not possible in BN. Seems to be related to the &lt;em&gt;label bias problem&lt;/em&gt; discussed in some CRF paper.&lt;/li&gt;
  &lt;li&gt;Computing the partition function is one of the biggest problem in PGM.&lt;/li&gt;
  &lt;li&gt;Directed and undirected graphical model both have their both representation power(v-structure cannot be captured by UGM while the diamond structure cannot be captured by DGM) and they both cannot capture all probability distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Model examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Boltzmann distribution in physics refers to log-linear model in statistics&lt;/li&gt;
  &lt;li&gt;Boltzmann Machine defined on &lt;strong&gt;fully-connected&lt;/strong&gt; graph with &lt;strong&gt;pairwise&lt;/strong&gt; edges on nodes with &lt;strong&gt;binary&lt;/strong&gt; values: &lt;script type=&quot;math/tex&quot;&gt;P(\mathbf{X}) = \frac{1}{Z}\exp{[\sum\limits_{i,j}\theta_{i,j} x_i x_j + \sum\limits_i \alpha_i x_i + C]} = \frac{1}{Z} \exp{(\mathbf{X} - \mu)^T \Theta (\mathbf{X} - \mu) }&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Ising model: sparse version of Boltzmann machine&lt;/li&gt;
  &lt;li&gt;Potts model: multi-state Ising model&lt;/li&gt;
  &lt;li&gt;Restricted Boltzmann Machine: hidden variables are introduced(breaking the homogeneous property of nodes)&lt;/li&gt;
  &lt;li&gt;RBM properties: hidden variables &lt;strong&gt;conditionally independent&lt;/strong&gt; given observation variables(&lt;strong&gt;decoupling&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;RBM for text modeling: define conditional probability for \( P(h \vert x)\) and \( P(x \vert h)\) of your preference and you&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Soundness/completeness, those theorem between \( \mathcal{G}\) and \(P\) are not talked in detail. This might indicate I don’t need to pay too much attention on those topics.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture3-MRFrepresentation.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;

</description>
                <link>http://simpleyyt.github.io/course/2015/09/20/pgm-2-markov-network</link>
                <guid>http://simpleyyt.github.io/course/2015/09/20/pgm-2-markov-network</guid>
                <pubDate>2015-09-20T19:09:47+02:00</pubDate>
        </item>

        <item>
                <title>PGM course: Bayesian Network</title>
                <description>&lt;h1 id=&quot;learned-from-book&quot;&gt;Learned from book&lt;/h1&gt;

&lt;h2 id=&quot;exploiting-independence-properties&quot;&gt;Exploiting Independence Properties&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Why represent the joint probability in a compact way: computational(memory), cognitively(interpretability) and statistically(large amount of data for parameter estimation).&lt;/li&gt;
  &lt;li&gt;Compactness in terms of number of independent parameters&lt;/li&gt;
  &lt;li&gt;More compactness lead to less expressiveness&lt;/li&gt;
  &lt;li&gt;Naive Bayes model: assumptions and structure. Independence assumption lead to over-counting because single effect represented by highly correlated variables are considered multiple times.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayesian-network&quot;&gt;Bayesian Network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Bayesian network: structure + conditional probability distribution(CPD)&lt;/li&gt;
  &lt;li&gt;The &lt;code&gt;Student&lt;/code&gt; Bayesian network which is representative(4 types of typical \(X, Y, Z\) relation)&lt;/li&gt;
  &lt;li&gt;causal reasoning(cause observed), evidential reasoning(evidence observed) and intercausal reasoning(or explaining away, one cause of one effect can influence the other cause of the same effect)&lt;/li&gt;
  &lt;li&gt;\( \mathcal{I}_l(\mathcal{G})\): local independencies encoded by \( \mathcal{G} \): variable \(X\) independent of non-descendants given parents of \(X\)&lt;/li&gt;
  &lt;li&gt;I-map: \(\mathcal{G} \) is an I-map for \(P\) if \( \mathcal{I}_l(\mathcal{G}) \subseteq \mathcal{I}(P)\). There might be independencies not reflected in \(\mathcal{G}\)&lt;/li&gt;
  &lt;li&gt;Factorization: \( P \) factorizes according to \(\mathcal{G}\) if \( P \) can be expressed as &lt;script type=&quot;math/tex&quot;&gt;\prod\limits_{i} P(X_i \mid Pa_{X_i}^{\mathcal{G}})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;I-map and factorization are equivalent(&lt;strong&gt;?&lt;/strong&gt;): a strong correlation between graph structure and independence assertions&lt;/li&gt;
  &lt;li&gt;Knowledge engineering: picking variables, picking structures, picking probabilities&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;independencies-in-graphs&quot;&gt;Independencies in Graphs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;v-structure(\( X \rightarrow Z \leftarrow Y\)), active trial&lt;/li&gt;
  &lt;li&gt;D-separation: \(\text{d-sep}_\mathcal{G}(\mathbf{X}; \mathbf{Y} \mid \mathbf{Z})\) if no active trials between any nodes in \(\mathbf{X}\) and \(\mathbf{Y}\) given \(\mathbf{Z}\)&lt;/li&gt;
  &lt;li&gt;Global Markov independencies: \( \mathcal{I}(\mathcal{G})\) independencies indicated by D-separation.&lt;/li&gt;
  &lt;li&gt;soundness of d-separation: if \( X \) and \( Y\) are d-separated given \(Z \), then they are conditionally independent given \( Z \). Equivalently, ( \( \mathcal{I}(\mathcal{G}) \subseteq \mathcal{I}(P) \))&lt;/li&gt;
  &lt;li&gt;completeness of d-separation: d-separation detects &lt;em&gt;all&lt;/em&gt; possible independencies( \( \mathcal{I}(P) \subseteq \mathcal{I}(\mathcal{G})\)). &lt;strong&gt;Does not hold&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;However, for almost all \( P\) that factorizes over \( g\), ( \( \mathcal{I}(P) = \mathcal{I}(\mathcal{G})\)). d-separation can lead to independency however, it’s not the other reason. The actual values in CPD can indicate independency as well.&lt;/li&gt;
  &lt;li&gt;A linear time algorithm(via BFS) that detects the reachable node from \( X\) given \(Z\) via active trials.&lt;/li&gt;
  &lt;li&gt;I-equivalence: \( \mathcal{K}_1\) and \( \mathcal{K}_2\) are I-equivalent if \( \mathcal{I}(\mathcal{K}_1) = \mathcal{I}(\mathcal{K}_2)\)&lt;/li&gt;
  &lt;li&gt;If same skeleton and same v-structures, then I-equivalent(sufficient but not necessary) and more theorems on that.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;from-distributions-to-graphs&quot;&gt;From Distributions to Graphs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Fundamental question: how to construct the \(\mathcal{G}\) that reflects the \( \mathcal{I}(\mathcal{P})\)?&lt;/li&gt;
  &lt;li&gt;minimal I-map: the I-map of \( P \) such that removal of any edge will make it not an I-map. this is an approximation to \( \mathcal{I}(\mathcal{G})\)&lt;/li&gt;
  &lt;li&gt;For the algorithm that finds minimal I-map: different variable ordering may produce different result. Even for the same ordering, this result may not be unique&lt;/li&gt;
  &lt;li&gt;Perfect I-map: graph \( \mathcal{K} \) such that \( \mathcal{I}(\mathcal{K}) = \mathcal{I}(\mathcal{P})\)&lt;/li&gt;
  &lt;li&gt;Two good examples that reflects the type of independencies that BN &lt;strong&gt;fails to capture&lt;/strong&gt;: 1, independencies that are inferred from specific problems, 2, the &lt;em&gt;misconception&lt;/em&gt; example&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;learned-from-video&quot;&gt;Learned from video&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;If you gain something(parametrization), you lose something(dependency). It’s conversely true. representation cost vs representation power or expressiveness vs #parameters&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Dishonest casino&lt;/em&gt; example: evaluation(how likely is the sequence), decoding(what are the dices used for each toss), learning(how often does the casino player changes the dice and how loaded is the dice)&lt;/li&gt;
  &lt;li&gt;Purpose of using pgm: transform a computation(margin/posterior proba) which can be exponential into something polynomial. For example, HMM&lt;/li&gt;
  &lt;li&gt;Goal of BN: for \( P \) to be represented in a &lt;em&gt;factorized&lt;/em&gt; way and represent &lt;em&gt;a set of conditional independence assumptions&lt;/em&gt; about \( P \)&lt;/li&gt;
  &lt;li&gt;Why do we say &lt;em&gt;independence&lt;/em&gt; instead of &lt;em&gt;dependence&lt;/em&gt;? Because &lt;em&gt;independence&lt;/em&gt; s a safer assumption(&lt;strong&gt;is that absolutely safe?&lt;/strong&gt;). For example \( A \rightarrow B\) does not guarantee \(A \) is independent of \(B\)(we can craft some numbers to make them dependent)&lt;/li&gt;
  &lt;li&gt;Prove that the independence assumption by \(P\) is consistent with \(\mathcal{I}(\mathcal{G})\)&lt;/li&gt;
  &lt;li&gt;I-map: connects graph with distribution&lt;/li&gt;
  &lt;li&gt;Alternative definition of D-separation: separated in the &lt;em&gt;moralized ancestral&lt;/em&gt; graph. Being &lt;em&gt;moral&lt;/em&gt; means being connected(or married). Ancestral graph is the graph where the variables-in-question’s  descendants are removed&lt;/li&gt;
  &lt;li&gt;Graph can be an representation for probability distribution. As a result, in order for \(\mathcal{G}\) to be useful, the independence properties from it should also hold for \(P\).&lt;/li&gt;
  &lt;li&gt;Refresher: I-map and factorization are equivalent&lt;/li&gt;
  &lt;li&gt;D-separation guarantees soundness and almost “completeness”(exceptions are zero-measure distribution)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;questions&quot;&gt;Questions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Why are factorization, i-map, d-separation and i-equivalence useful?
Factorization and i-map are used to connect graph structure to conditional independence assumptions back and forth.
D-separation: defines the criteria when \(X\) and \(Y\) are conditionally independent given \(Z\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Some portions of the content are directly taken from the &lt;a href=&quot;http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture2-BNrepresentation.pdf&quot;&gt;slides&lt;/a&gt; of CMU Probabilistic Graphical Model, 2014 by Eric Xing&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/course/2015/09/16/pgm-1-bayesian-network</link>
                <guid>http://simpleyyt.github.io/course/2015/09/16/pgm-1-bayesian-network</guid>
                <pubDate>2015-09-16T17:58:13+02:00</pubDate>
        </item>

        <item>
                <title>Low-Rank Tensors for Scoring Dependency Structures</title>
                <description>&lt;p&gt;&lt;a href=&quot;https://people.csail.mit.edu/regina/my_papers/tens14.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;For each sentence \(x\) and candidate dependency tree, \( y \in \mathcal{Y}(x)\), the score:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(x, y) = \sum\limits_{h \rightarrow m \in y} s(h \rightarrow m)&lt;/script&gt;

&lt;p&gt;where \(s(h \rightarrow m)\) is the score for each arc from head \(h\) to modifier \(m\).&lt;/p&gt;

&lt;p&gt;The prediction is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y} = argmax_{y \in \mathcal{Y}(x)} S(x, y)&lt;/script&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Tradional way for dependency parsing&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The arc score is simply vector inner product:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
s_{\theta}( h \rightarrow m) = &lt;\theta, \phi_{h \rightarrow m}&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(\theta \in \mathcal{R}^L \) is the parameter to be estimated.&lt;/p&gt;

&lt;p&gt;Problem with the vector space method:&lt;/p&gt;

&lt;p&gt;Feature number \(L\) is usually large. Feature selection is often used. However, manual way is laborsome while automatic way might get rid of useful features(as some feature lack clear linguistic meaning such as word embedding)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvement&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Basic idea:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with some simple features(e.g, word, lemma, morph, pos) for head, modifier and arc, and apply Kronecker product to combine differet features.&lt;/li&gt;
  &lt;li&gt;Use low-rank approximation to approximate the parameter used to compute the arc score&lt;/li&gt;
  &lt;li&gt;Use soft-margin maximization as the training objective and passive-aggressive algorithm for training&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;Now they propose a different way to compute \(s(h \rightarrow m)\).&lt;/p&gt;

&lt;p&gt;Given head, modifier and the arc, we first extract three feature vectors, &lt;script type=&quot;math/tex&quot;&gt;\phi_{h}, \phi_{m}, \phi_{h \rightarrow m}&lt;/script&gt; respectively.&lt;/p&gt;

&lt;p&gt;Kronecker product is applied on them:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\phi_{h} \otimes \phi_{m} \otimes \phi_{h \rightarrow m})_{i,j,k} = \phi_{h, i} \phi_{m, j} \phi_{h \rightarrow m, k}&lt;/script&gt;

&lt;p&gt;which aims at combine those features.&lt;/p&gt;

&lt;p&gt;The score is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
s(h \rightarrow m) = &lt;A, \phi_{h} \otimes \phi_{m} \otimes \phi_{h \rightarrow m}&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;equavalent to the inner product between A and combined features.&lt;/p&gt;

&lt;p&gt;For training, our task to to estimat \(A \), which can be potentially large.&lt;/p&gt;

&lt;p&gt;To reduce the number of parameters as well as adding genealization power, we approximate \(A\) via low-rank approximation, \( U, V \in \mathcal{R}^{r \times n}, W \in \mathcal{R}^{r \times d} \):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A = \sum\limits_{i=1}^r U(i, :) \otimes V(i, :) \otimes W(i, :)&lt;/script&gt;

&lt;p&gt;We say \(A\) is in the Kruskal form.&lt;/p&gt;

&lt;p&gt;The score becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^r [U\phi_h]_i [V\phi_m]_i [W\phi_{h \rightarrow m}]_i&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, the tensor score is combined by weight with the some other score.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;Soft margin maximization is used and online learning via alternatively updating parameters is used.&lt;/p&gt;

&lt;p&gt;The details cannot be understood for now.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Factorizing tensor via Kruskal form(a new way)&lt;/li&gt;
  &lt;li&gt;Combining features through tensor outer product/cross product.&lt;/li&gt;
  &lt;li&gt;Soft margin maximization(like the objective for SVM) is used for the training of structure prediction problem&lt;/li&gt;
  &lt;li&gt;Definition of online learning: update parameter successively based on each instance&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;How the prediction/decoding is done?&lt;/li&gt;
  &lt;li&gt;How Passive Aggresive training works?&lt;/li&gt;
  &lt;li&gt;How does Equation (2) comes up?&lt;/li&gt;
  &lt;li&gt;Can we apply the feature combination and tensor factorization to other prediction problem&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/14/low-rank-tensor-for-dependency-parsing</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/14/low-rank-tensor-for-dependency-parsing</guid>
                <pubDate>2015-09-14T12:49:50+02:00</pubDate>
        </item>

        <item>
                <title>GloVe: Global Vectors forWord Representation</title>
                <description>&lt;h1 id=&quot;glove-global-vectors-forword-representation&quot;&gt;GloVe: Global Vectors forWord Representation&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www-nlp.stanford.edu/pubs/glove.pdf&quot;&gt;paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It proposes a model that combines global co-occurence information with local context window methods.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;How to compute meaningful word representations from corpus in an unsupervised way?&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;Maximize the objective function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \sum_{i,j=1}^V f(X_{ij}) (w_i^T \widetilde{w}_j + b_i + \widetilde{b}_j - \log{X_{ij}})&lt;/script&gt;

&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(w_i,  \widetilde{w}_j\) are word representations&lt;/li&gt;
  &lt;li&gt;\(X_{ij}\) is the frequency of jth word occuring within the context windown of the ith word&lt;/li&gt;
  &lt;li&gt;\( b_i, \widetilde{b}_j\) are bias terms&lt;/li&gt;
  &lt;li&gt;\(f\) are weighting function to prevent the overwhelming effect of large \(X_{ij}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;connection-with-skip-gram-in-word2vec&quot;&gt;Connection with &lt;em&gt;Skip Gram&lt;/em&gt; in &lt;em&gt;word2vec&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;In skip gram,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \sum_{i \in corpus, j \in context(i)} log Q(i, j)&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(i, j) = \frac{\exp(w_i^T \widetilde{w}_j)}{\sum_{k=1}^V \exp{w_i^T \widetilde{w}_k}}&lt;/script&gt;

&lt;p&gt;which is the probability that word j appears in the context of word i.&lt;/p&gt;

&lt;p&gt;The above is the same as:(&lt;strong&gt;key transformation&lt;/strong&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray}
 J &amp; = \sum_{i, j=1}^V X_{i,j} log Q(i, j) \\
   &amp; = \sum_{i} X_i \sum_j  P_{i,j} log Q(i, j) \\
   &amp; = \sum_{i} X_i H(P_i, Q_i)
\end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: does the training algorithm differ for the above two objective functions?&lt;/p&gt;

&lt;p&gt;where \(H\) is cross entropy.&lt;/p&gt;

&lt;p&gt;And the following modifications are made with reasons given.&lt;/p&gt;

&lt;p&gt;The \( \log \) term in \( H\) approaches to \(-\infty\) if \(Q_i \rightarrow 0 \), which is bad(underflow) and often happen.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \sum_{i,j} X_i (\hat{P}_{ij} - \hat{Q}_{ij})^2&lt;/script&gt;

&lt;p&gt;where \(\hat{P}, \hat{Q}\) are the unnormalized quantity, as normalization for \(Q\) is expensive.&lt;/p&gt;

&lt;p&gt;However, the \(\exp\) in \( \hat{Q} \) makes it bad for training(large values), we can apply \(\log\) to it.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray}
J &amp; = \sum_{i,j} X_{ij} (\log\hat{P}_{ij} - \log\hat{Q}_{ij})^2 \\
  &amp; = \sum_{i,j} X_{ij} (w_i^T \widetilde{w}_j - \log X_{ij})^2 
\end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;p&gt;After applying reweighting on \(X_{i,j}\), it’s almost equivalent to the &lt;em&gt;GloVe&lt;/em&gt; objective function.&lt;/p&gt;

&lt;p&gt;So the \(J\) of &lt;em&gt;Skip Gram&lt;/em&gt; and &lt;em&gt;GloVe&lt;/em&gt; differs in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;GloVe&lt;/em&gt; &lt;strong&gt;direcly&lt;/strong&gt; uses global occurence information \(X_{ij}\) in two parts while &lt;em&gt;Skip Gram&lt;/em&gt; &lt;strong&gt;implicitly&lt;/strong&gt; uses it only in one part&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;GloVe&lt;/em&gt; reweights \(X_{ij}\) using \(f\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;connection-to-matrix-factorization&quot;&gt;Connection to Matrix Factorization&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i,j} X_{ij} (w_i^T \widetilde{w}_j - \log X_{ij})^2&lt;/script&gt;

&lt;p&gt;is a modified form of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(w_i^T \widetilde{w}_j - X_{ij})^2&lt;/script&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;p&gt;Stochastic Gradient Descent by sampling &lt;strong&gt;non-negative&lt;/strong&gt; elements from \(X\).&lt;/p&gt;

&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The final word embedding is \( w_i + \widetilde{w}_i\)&lt;/li&gt;
  &lt;li&gt;When calculating \(X_{ij}\), words that are \(d\) words apart is discounted by \(1/d\) to capture the fact that distant words provide less information&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;One way to infer meaning for word: use the global word-word co-occurence to infer. The intuition: word’s meaning can be defined by it’s co-occuring words.&lt;/li&gt;
  &lt;li&gt;Connection prediction-based model such as &lt;em&gt;Skip Gram&lt;/em&gt; with co-occurence matrix factorization model&lt;/li&gt;
  &lt;li&gt;Evaluation for word embedding: word analogy test, word similarity, downstream NLP tasks(using the embedding as features for tasks such as NER)&lt;/li&gt;
  &lt;li&gt;Why &lt;em&gt;Skip Gram&lt;/em&gt; and &lt;em&gt;GloVe&lt;/em&gt; perform matrix factorization &lt;em&gt;implicitly&lt;/em&gt; and &lt;em&gt;explicitly&lt;/em&gt; due to their different ways to formulating the objective function&lt;/li&gt;
  &lt;li&gt;Connection between language model and word embeddings: \( P(w_1 \cdots w_n) = \prod_i P(w_i | context)\) where \( P(w_i | context) \) is calculated using word embeddings.&lt;/li&gt;
  &lt;li&gt;Application of word embedding: as feature for downstream NLP tasks(NER, parsing, etc), representing document (&lt;a href=&quot;http://www.aclweb.org/anthology/W13-3212&quot;&gt;Aggregating ContinuousWord Embeddings for Information Retrieval&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Recap on &lt;em&gt;word2vec&lt;/em&gt; and Bengio’s 2003 neural language model paper: the goal is to maximize the log-likelihood of the corpus.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Why word analogy test is used? Does it indicate a better performance on other NLP tasks such as parsing and question-answering&lt;/li&gt;
  &lt;li&gt;Similarly, why do we bother learn word embedding that captures &lt;em&gt;linear&lt;/em&gt; (\( w_i - w_j\)) relationships with other word embeddings?&lt;/li&gt;
  &lt;li&gt;When is &lt;em&gt;word2vec&lt;/em&gt; useful? Especially, when is the linear property useful?&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/13/glove</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/13/glove</guid>
                <pubDate>2015-09-13T21:52:45+02:00</pubDate>
        </item>

        <item>
                <title>A Three-Way Model for Collective Learning on Multi-Relational Data</title>
                <description>&lt;h1 id=&quot;a-three-way-model-for-collective-learning-on-multi-relational-data&quot;&gt;A Three-Way Model for Collective Learning on Multi-Relational Data&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cip.ifi.lmu.de/~nickel/data/paper-icml2011.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;An efficient algorithm for relation learning based on entity-relation-entity tensor factorization.&lt;/p&gt;

&lt;p&gt;Given 3D tensor \(\mathcal{X}\) with binary value, where \(\mathcal{X}_{ijk}\) denotes whether there exists a fact that involves &lt;code&gt;(i-th entity, k-th relation, j-th entity)&lt;/code&gt;, find the low-rank factorization \(A\) and \(R_k\) such that&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}_k \approx A R_k A^T&lt;/script&gt;, where \(\mathcal{X}_k\) is the slice for the kth relation.&lt;/p&gt;

&lt;p&gt;Training objective:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(A, R) = \frac{1}{2}\left\| \mathcal{X}_k - A R_k A^T \right\|^2_F +  \frac{1}{2} \lambda (\left\| A \right\|^2_F + \sum_k\left\| R_k \right\|^2_F )&lt;/script&gt;

&lt;p&gt;which is squared error + L2 regularization.&lt;/p&gt;

&lt;p&gt;The error term can be expressed as matrix operation once stacking the tensor variable.&lt;/p&gt;

&lt;p&gt;Parameter estimation:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alternating least-squares&lt;/em&gt;: alternatively optimize either \(R\) or \(A\) which fixing the either.&lt;/p&gt;

&lt;p&gt;For \(A\)  which appears at both sides of &lt;script type=&quot;math/tex&quot;&gt;A R_k A^T&lt;/script&gt;, we can fix one side of \(A\) and solve the other one alternativley.&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;There are multiple ways to factorize 3D tensor. In this paper(&lt;code&gt;RESCAL&lt;/code&gt;) \(A R_k A^T\). In &lt;code&gt;DEDICOM&lt;/code&gt; \(A D_k R D_k A^T \), where \(R\) is global across relations(more restrictive)&lt;/li&gt;
  &lt;li&gt;ALS as parameter estimation method for the tensor factorization problem.&lt;/li&gt;
  &lt;li&gt;Collective classification where prediction is made (for one relation) collaborative using other data(from other relations). See the experiment part&lt;/li&gt;
  &lt;li&gt;Entity resolution by using the entity embedding as representation&lt;/li&gt;
  &lt;li&gt;chord graph as a way to visualize entity entity connection/relationship strength&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mnick/scikit-tensor&quot;&gt;scikit-tensor&lt;/a&gt; includes RESCAL&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Using PBR to train?&lt;/li&gt;
  &lt;li&gt;How to scale up? The series of matrix multiplication seems expensive&lt;/li&gt;
  &lt;li&gt;For each \(k\), \(R_k\) is independent. How to aggregate/connect them? Also, can we connect those entities?&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/paper/2015/09/11/three-way-model-for-collective-learning</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/11/three-way-model-for-collective-learning</guid>
                <pubDate>2015-09-11T23:32:38+02:00</pubDate>
        </item>

        <item>
                <title>Relational Learning via Collective Matrix Factorization</title>
                <description>&lt;h1 id=&quot;relational-learning-via-collective-matrix-factorization&quot;&gt;Relational Learning via Collective Matrix Factorization&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf&quot;&gt;paper link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;collective-matrix-factorization&quot;&gt;Collective Matrix Factorization&lt;/h2&gt;

&lt;p&gt;Given two data matrices, \(X\), user to movie rating and \(Y\), movie to genre mapping, we wish to learn \(U, V, Z\) such that:&lt;/p&gt;

&lt;p&gt;\( \mathit{X} \approx f_1(UV^T)\) and \( \mathit{Y} \approx f_2(VZ^T)\)&lt;/p&gt;

&lt;p&gt;\(V\) is shared among different matrix factorization.&lt;/p&gt;

&lt;p&gt;A stochastic approximation(sampling based etimation on gradient and Hessian) optimization method can be used to handle large, sparse matrices.&lt;/p&gt;

&lt;p&gt;The method can be generalized to arbitary relational schemas.&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Collective Matrix Factorization to improve performance by simultaneously factorizing several matrices and sharing parameters&lt;/li&gt;
  &lt;li&gt;The existence of Bregman Divergence as a general way for loss function in MF&lt;/li&gt;
  &lt;li&gt;Weights can be used to rescale the loss function(large/small matrix, cope with missing values)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;importantnot-yet-understood-topics&quot;&gt;Important/not-yet-understood topics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;How to interprese Newton optimization method? When can it be used? Why is it useful?&lt;/li&gt;
  &lt;li&gt;How Newton method compared with gradient descent? More learning on convex optimization.&lt;/li&gt;
  &lt;li&gt;The ‘updating-each-row’ argument in Section 4.1&lt;/li&gt;
  &lt;li&gt;How stochastic optimization works?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions--extensions&quot;&gt;Questions / extensions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Can rank-based learning applied here? for example Personalized Bayesian Ranking?
&lt;a href=&quot;http://www.ismll.uni-hildesheim.de/pub/pdfs/artus_lucas_wsdm2012.pdf&quot;&gt;Multi-Relational Matrix Factorization using Bayesian Personalized Ranking for Social Network Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hybird of collective tensor and matrix factorization
&lt;a href=&quot;http://ttic.uchicago.edu/~ryotat/papers/TakTomIshKimSaw13.pdf&quot;&gt;Non-negative Multiple Tensor Factorization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/11/relation-learning-via-cmf</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/11/relation-learning-via-cmf</guid>
                <pubDate>2015-09-11T16:58:23+02:00</pubDate>
        </item>

        <item>
                <title>Relation Extraction with Matrix Factorization and Universal Schemas</title>
                <description>&lt;h1 id=&quot;relation-extraction-with-matrix-factorization-and-universal-schemas&quot;&gt;Relation Extraction with Matrix Factorization and Universal Schemas&lt;/h1&gt;

&lt;p&gt;Paper &lt;a href=&quot;https://people.cs.umass.edu/~lmyao/papers/univ-schema-tacl.pdf&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;“Relation extraction”” here is essentially “relation prediction”.&lt;/p&gt;

&lt;p&gt;The problem at hand is:&lt;/p&gt;

&lt;p&gt;Given a matrix of facts, \( \mathcal{O} \) where rows are (entity, entity) tuples, \(t\) and columns are relations, \(r\), how to make predictions on the missing entries? For example, is &lt;code&gt;FUGURSON a-professor-at HARVARD&lt;/code&gt; True given &lt;code&gt;FUGURSON is-historian-at HARVARD&lt;/code&gt;?&lt;/p&gt;

&lt;h2 id=&quot;approaches&quot;&gt;Approaches&lt;/h2&gt;

&lt;p&gt;For each fact, \( (t, r) \), its probability is measured by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y_{t,r} = 1 | \theta_{t,r}) = \frac{1}{1+exp(-\theta_{t,r})}&lt;/script&gt;

&lt;p&gt;where \(\theta_{t,r} \) is parameter for each \((t,r\) combination and it can be constructed in different ways and later summed up to from a total compatibility score.&lt;/p&gt;

&lt;p&gt;In this paper, three sub-models for \(\theta_{t,r} \) are used.&lt;/p&gt;

&lt;h3 id=&quot;universal-schema&quot;&gt;Universal Schema&lt;/h3&gt;

&lt;p&gt;Different schemas(list of relations) are concatenated to form a universal one, without further processing those relations(like relation clustering).&lt;/p&gt;

&lt;h3 id=&quot;three-submodels&quot;&gt;Three submodels&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Latent feature model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Matrix factorization in the following ways:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta = (\theta_{t,r}) \approx = \mathbf{A} \mathbf{V}^T&lt;/script&gt;

&lt;p&gt;In other words:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t,r} = A_{t,}\dot V_{r,}^T&lt;/script&gt;

&lt;p&gt;where \(A_{t,}\) is the feature vector for the tuple \( t\) and  \(V_{r,}\) the feature vector for relation \( v \).&lt;/p&gt;

&lt;p&gt;This model alone achieves better result than a state-of-the-art distant supervision model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Neighborhood model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The confidence of a tuple and a relation can be given by the confidence scores between similar relations for the same tuple.&lt;/p&gt;

&lt;p&gt;$$ \theta_{t,r} = \sum\limit_{((t, r^{‘}) \in \mathcal{O} \ {(t,r)} w_{r,r^{‘}})}&lt;/p&gt;

&lt;p&gt;\( w_{r,r^{‘} \) is the parameter to be estimated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: this is also a type of matrix factorization.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Q&lt;/em&gt;: Vector similarity between \(r\) and \(r^{‘}\) should also account for \( w_{r,r^{‘} \).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Entiy model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each entity and each argumetn slot of each relation has embedding.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t,r} = \sum\limit_{i=1}^{arity(r)} r_{i,} \dot t_{i}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;\( r_{i,}\): the embedding for the \(ith \) slot of \(r\)&lt;/li&gt;
  &lt;li&gt;\(t_{i}\): the embedding for the \(ith\) element of tuple \(t)\&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, the three scores are summed.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;Bayesian Personalized Ranking is used to construct the objective function. Stochastic Gradient Descent is used for parameter estimation. At each iteration, one ranked pair is randomly sampled and used to calculate the gradient.&lt;/p&gt;

&lt;h2 id=&quot;connection-with-other-types-of-methods&quot;&gt;Connection with other types of methods&lt;/h2&gt;

&lt;h3 id=&quot;distant-supervisionds&quot;&gt;Distant Supervision(DS)&lt;/h3&gt;
&lt;p&gt;Uses KB to “label” the text corpus. Requires KB of the desired schema.&lt;/p&gt;

&lt;p&gt;Relation surface form is mapped to schema of KD.&lt;/p&gt;

&lt;p&gt;Here relations can be free text form. So the number of relations is greater, thus more expressiveness. Also, less compactness or more sparsity?&lt;/p&gt;

&lt;h3 id=&quot;openie&quot;&gt;OpenIE&lt;/h3&gt;

&lt;p&gt;It’s only for extraction from textual data, not prediction.&lt;/p&gt;

&lt;h3 id=&quot;relation-clustering&quot;&gt;Relation clustering&lt;/h3&gt;

&lt;p&gt;Don’t quite understand&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I Learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Matrix factorization in the general sense(other than the \( A = UV \) case). Two more ways to factorize a matrix&lt;/li&gt;
  &lt;li&gt;Matrix completion problem in the context of implicit feedback can be formulated as a ranking problem instead of prediction problem.&lt;/li&gt;
  &lt;li&gt;The training method(&lt;em&gt;Bayesian Personalized Ranking&lt;/em&gt;) is quite similar to that in the &lt;a href=&quot;/paper/2015/09/06/paper/&quot;&gt;personalized entity recommednation paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Relative simple way for relation extraction compared to other methods(distant supervision which uses graphical model)&lt;/li&gt;
  &lt;li&gt;At each iteration during SGD, one ranked pair is randomly sampled and only part of parameter is updated.&lt;/li&gt;
  &lt;li&gt;Relation clustering is used in relation extraction(&lt;strong&gt;for what purpose?&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Given unseen tuples, even member appears in the training tuples, is it possible to predict their relations?&lt;/li&gt;
  &lt;li&gt;Can model matrix completion for entity typing problem? Or even collective matrix factorization(multiple matrices((entity mention, type), (eneity mention, relation), (relation, type)))?
How are the collective matrix factorization done? When is it applicable?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;this-paper-is-based-on&quot;&gt;This paper is based on&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Michael Collins, Sanjoy Dasgupta, and Robert E. Schapire. 2001. A generalization of principal component analysis to the exponential family. In Proceedings of NIPS&lt;/li&gt;
  &lt;li&gt;Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08&lt;/li&gt;
  &lt;li&gt;Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/paper/2015/09/10/relation-extraction-using-matrix-factorization-and-universal-schema</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/10/relation-extraction-using-matrix-factorization-and-universal-schema</guid>
                <pubDate>2015-09-10T19:33:13+02:00</pubDate>
        </item>

        <item>
                <title>ACL 2015 tutorial summary: Matrix/Tensor Factorization for NLP</title>
                <description>&lt;h1 id=&quot;matrix-and-tensor-factorization-methods-for-natural-language-processing&quot;&gt;Matrix and Tensor Factorization Methods for Natural Language Processing&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://acl2015.org/tutorials-t5.html&quot;&gt;Tutorial link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;matrix-factorization&quot;&gt;Matrix Factorization&lt;/h1&gt;

&lt;h2 id=&quot;basics&quot;&gt;Basics&lt;/h2&gt;

&lt;p&gt;inner/outter product, Hadamard product(element-wise), element-wise p-norm(Frobenius norm = 2-norm)&lt;/p&gt;

&lt;h3 id=&quot;matrix-completion-via-low-rank-factorization&quot;&gt;Matrix completion via Low-Rank Factorization&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Matrix completion&lt;/strong&gt;: recovery of a matrix&lt;/p&gt;

&lt;p&gt;Example applications: guessing missing value in survey data, or estimate distance in sensor node network in which each node has limited range of distance sensing&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low-Rank Factorization&lt;/strong&gt;: \( Y \approx UV^T \), \( Y \in \mathcal{R}^{N \times M}, U \in \mathcal{R}^{N \times L}, V \in \mathcal{R}^{M \times L}\)&lt;/p&gt;

&lt;p&gt;Assumption: \( rank(Y) = L \ll M,N \). In other words, \( Y \) contains &lt;strong&gt;redundancy&lt;/strong&gt; and &lt;strong&gt;noise&lt;/strong&gt;. We hope to reconstruct \( Y \) using less data based on redundancy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why use it?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;three-ways-to-factorize&quot;&gt;Three ways to factorize&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;
\( Y = UDV^T\), \( Y \in \mathcal{R}^{N \times M}, U \in \mathcal{R}^{N \times N}, D \in \mathcal{R} ^ {N \times M}, V \in \mathcal{R}^{M \times M}\), \( D \) is diagonal&lt;/p&gt;

    &lt;p&gt;We truncate \( D \) to achieve low-rank approximation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; and &lt;strong&gt;Alternating Least Squares&lt;/strong&gt;:
Minimize &lt;script type=&quot;math/tex&quot;&gt;\left\| \mathbf{Y} - \mathbf{U} \mathbf{V^T} \right\|_F^2&lt;/script&gt;.&lt;/p&gt;

    &lt;p&gt;SGD slower and counter-intuitive to parallelize compared to Alternating Least Squares, which fixes one parameter matrix, converting the problem to Least Square, which is convex and has closed-form solution.&lt;/p&gt;

    &lt;p&gt;The following regularization leads to better readability, compactness and retrieval: &lt;script type=&quot;math/tex&quot;&gt;\left\| \mathbf{Y} - \mathbf{U} \mathbf{V^T} \right\|_F^2 + \lambda_1 \left\| U \right\|_1 + \lambda_2 \left\| U \right\|_F^2&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;L1 norm induces sparsity on the user feedback(movie analogy) while L2 norm avoid over-fitting on the item feature vectors&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;word2vec&lt;/em&gt; and &lt;em&gt;GloVe&lt;/em&gt; also implicitly/explicitly used matrix factorization. See related paper section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Non-negative Matrix Factorization&lt;/strong&gt;
Adding constraint \( U \gt 0\) and  \( V \gt 0 \). Example: topic model, images pixels&lt;/p&gt;

    &lt;p&gt;Optimization: Multiplicative update rules(2001) or constrained Alternating Least Squares(2008).&lt;/p&gt;

    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Connection with PLSA and LDA. \( Y \) is the document-term matrix and \( L \) is the number of topics in this case. \( Y_ij \ = \sum\limit_k^L p(k&lt;/td&gt;
          &lt;td&gt;d_i) p(d_ij&lt;/td&gt;
          &lt;td&gt;k) \)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;misc-for-matrix-factorization&quot;&gt;Misc for matrix factorization&lt;/h2&gt;

&lt;h3 id=&quot;logisitc-loss&quot;&gt;Logisitc loss&lt;/h3&gt;

&lt;p&gt;Instead of squared loss, sign rank is prposed. See paper section.&lt;/p&gt;

&lt;h3 id=&quot;relation-extraction-using-matrix-factorization&quot;&gt;Relation extraction using matrix factorization&lt;/h3&gt;

&lt;p&gt;Uses also negative data(sampled from unobserved cells).&lt;/p&gt;

&lt;p&gt;See &lt;em&gt;Riedel, 2013&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;inclusion-of-prior&quot;&gt;Inclusion of Prior&lt;/h3&gt;

&lt;p&gt;Even logic is used. See &lt;em&gt;Rocktäschel 2015&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;tensor-factorization&quot;&gt;Tensor Factorization&lt;/h1&gt;

&lt;h2 id=&quot;basics-1&quot;&gt;Basics&lt;/h2&gt;

&lt;p&gt;Example: entity-relation-eneity tensor. 3D&lt;/p&gt;

&lt;p&gt;Tensor-vector product, tensor-matrix product. outer product of three vectors: rank-1 tensor. &lt;strong&gt;What’s the rank for tensor?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;tucker-decomposition&quot;&gt;Tucker decomposition&lt;/h2&gt;

&lt;p&gt;\( \mathcal{T} = \mathcal{G} \times \mathbf{A} \times \mathbf{B} \times \mathbf{C} \)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why this strange form?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\( \mathcal{T} \in \mathbb{R}^{p \times r \times q } \)&lt;/li&gt;
  &lt;li&gt;\( G\): core tensor. \( \mathbb{R}^{N_1 \times N_2 \times N_3 }\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( \mathbf{A} \in \mathbb{R}^{N_1 \times p } \mathbf{B} \in \mathbb{R}^{N_2 \times r } \mathbf{C} \in \mathbb{R}^{N_3 \times q }\): loading matrices&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tucker 3&lt;/strong&gt;: nothing is keped fixed&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tucker 2&lt;/strong&gt;: 1 loading matrix is keped fixed. For example, \( \mathcal{T} = \mathcal{G} \times \mathbf{A} \times \mathbf{B} \times \mathit{I} \)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tucker 1&lt;/strong&gt;: 2 loading matrix is keped fixed, \( \mathcal{T} = \mathcal{G} \times \mathit{I} \times \mathit{I} \times \mathbf{C}  \)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;papers&quot;&gt;Papers&lt;/h3&gt;

&lt;p&gt;Old: CANDECOMP and PARAFAC
New: RESCAL(Nickel, 2012)
Applied for semantic compositionality: Van de , 2013&lt;/p&gt;

&lt;p&gt;Also related to neural network, Nickel, 2015&lt;/p&gt;

&lt;h2 id=&quot;collective-matrix-decomposition&quot;&gt;Collective matrix decomposition&lt;/h2&gt;

&lt;p&gt;See the references&lt;/p&gt;

&lt;h2 id=&quot;discriminative-factorial-models&quot;&gt;Discriminative factorial models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Factorization machine&lt;/strong&gt;: Rendle (2010). SVM + MF -&amp;gt; model interaction bewteen variables. &lt;strong&gt;HOW?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reduced rank regression&lt;/strong&gt;: the coefficient is factorized. &lt;strong&gt;Why doing so?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-task learning&lt;/strong&gt;: label embedding. &lt;strong&gt;What? Why? How?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Structured prediction&lt;/strong&gt;: learn feature templates(laborsome to produce) from data using MF.
Non-convex optimization using variant of Passive-Aggressive(Crammer, 2006). Or (Lei, 2014)
Applied on SRL and parsing&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;convexification&quot;&gt;Convexification&lt;/h1&gt;

&lt;p&gt;If non-convex problems can be converted to convex, then we have theoretical guarantees and a bunch of mature tools.&lt;/p&gt;

&lt;p&gt;But &lt;strong&gt;how to do the transformation?&lt;/strong&gt;: add convex penalty(what? and how?) sum of trace norm&lt;/p&gt;

&lt;p&gt;Application: spectral learning in NLP(combined with trace norm). polynomial learning of HMM, Grammar and NMF&lt;/p&gt;

&lt;h1 id=&quot;related-papers&quot;&gt;Related papers&lt;/h1&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Kolda, Tamara G., and Brett W. Bader. “&lt;a href=&quot;http://epubs.siam.org/doi/pdf/10.1137/07070111X&quot;&gt;Tensor decompositions and applications.&lt;/a&gt;” SIAM review 51.3 (2009): 455-500.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word-embedding-and-matrix-factorization&quot;&gt;Word Embedding and Matrix factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “&lt;a href=&quot;http://www-nlp.stanford.edu/pubs/glove.pdf&quot;&gt;Glove: Global vectors for word representation.&lt;/a&gt;” Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12 (2014): 1532-1543.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plsalda-and-nmf&quot;&gt;PLSA/LDA and NMF&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Gaussier, Eric, and Cyril Goutte. “&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1076148&quot;&gt;Relation between PLSA and NMF and implications.&lt;/a&gt;” Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2005.&lt;/li&gt;
  &lt;li&gt;Arora, Sanjeev, Rong Ge, and Ankur Moitra. “&lt;a href=&quot;http://arxiv.org/abs/1204.1956&quot;&gt;Learning topic models–going beyond SVD.&lt;/a&gt;” Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE, 2012.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sign-rank&quot;&gt;Sign rank&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bouchard, Guillaume, Sameer Singh, and Théo Trouillon. “&lt;a href=&quot;http://sameersingh.org/files/papers/logicmf-krr15.pdf&quot;&gt;On approximate reasoning capabilities of low-rank vector spaces.&lt;/a&gt;” AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches (2015).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;injecting-prior--domain-knowledge&quot;&gt;Injecting prior / domain knowledge&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Chang, &lt;a href=&quot;http://research.microsoft.com/apps/pubs/default.aspx?id=226677&quot;&gt;Typed Tensor Decomposition of Knowledge Bases for Relation Extraction&lt;/a&gt;, ACL, 2014&lt;/li&gt;
  &lt;li&gt;Sebastian Riedel, &lt;a href=&quot;https://people.cs.umass.edu/~lmyao/papers/univ-schema-tacl.pdf&quot;&gt;Relation Extraction with Matrix Factorization and Universal Schemas&lt;/a&gt;, Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ‘13) 2013&lt;/li&gt;
  &lt;li&gt;Rocktäschel, Tim, Sameer Singh, and Sebastian Riedel. “&lt;a href=&quot;http://rockt.github.io/pdf/rocktaschel2015injecting.pdf&quot;&gt;Injecting Logical Background Knowledge into Embeddings for Relation Extraction.&lt;/a&gt;” Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. 2015.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tensor-factorization-on-relation-learning&quot;&gt;Tensor factorization on relation learning&lt;/h3&gt;

&lt;p&gt;An instance of Tucker 2&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nickel, Maximilian, Volker Tresp, and Hans-Peter Kriegel. “&lt;a href=&quot;http://www.cip.ifi.lmu.de/~nickel/data/paper-icml2011.pdf&quot;&gt;A three-way model for collective learning on multi-relational data.&lt;/a&gt;” Proceedings of the 28th international conference on machine learning (ICML-11). 2011.&lt;/li&gt;
  &lt;li&gt;Nickel, Maximilian, Volker Tresp, and Hans-Peter Kriegel. &lt;a href=&quot;http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf&quot;&gt;Factorizing YAGO: scalable machine learning for linked data&lt;/a&gt;, WWW, 2012&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tensor-factorization-on-semantic-compositionaliy&quot;&gt;Tensor factorization on semantic compositionaliy&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Van de Cruys, Tim, Thierry Poibeau, and Anna Korhonen. “&lt;a href=&quot;http://www.aclweb.org/anthology/N13-1134.pdf&quot;&gt;A tensor-based factorization model of semantic compositionality.&lt;/a&gt;” Conference of the North American Chapter of the Association of Computational Linguistics (HTL-NAACL). 2013.&lt;/li&gt;
  &lt;li&gt;Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich,  &lt;a href=&quot;http://arxiv.org/pdf/1503.00759v2.pdf&quot;&gt;A Review of Relational Machine Learning for Knowledge Graphs&lt;/a&gt;, IEEE, 2015 (&lt;strong&gt;relation ship to neural network&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;combined-matrix--tensor-factorization&quot;&gt;Combined matrix / tensor factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Singh, Sameer and Rocktaschel, Tim and Riedel, Sebastian, &lt;a href=&quot;http://rockt.github.io/pdf/singh2015towards.pdf&quot;&gt;Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction&lt;/a&gt;, NAACL Workshop on Vector Space Modeling for NLP (VSM), 2015&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;collective-matrix-factorization&quot;&gt;Collective matrix factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Singh, Ajit P., and Geoffrey J. Gordon. “&lt;a href=&quot;http://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf&quot;&gt;Relational learning via collective matrix factorization.&lt;/a&gt;” Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2008.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bayesian-matrix-factorization&quot;&gt;Bayesian matrix factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Singh, Ajit P., and Geoffrey Gordon. “&lt;a href=&quot;http://www.cs.cmu.edu/~ggordon/singh-gordon-relational.pdf&quot;&gt;A Bayesian matrix factorization model for relational data.&lt;/a&gt;” arXiv preprint arXiv:1203.3517 (2012).&lt;/li&gt;
  &lt;li&gt;Salakhutdinov, Ruslan, and Andriy Mnih. “&lt;a href=&quot;https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf&quot;&gt;Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.&lt;/a&gt;” Proceedings of the 25th international conference on Machine learning. ACM, 2008.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bayesian-collective-matrix-factorization&quot;&gt;Bayesian collective matrix factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Klami, Arto, Guillaume Bouchard, and Abhishek Tripathi. “&lt;a href=&quot;http://arxiv.org/pdf/1312.5921.pdf&quot;&gt;Group-sparse embeddings in collective matrix factorization.&lt;/a&gt;” arXiv preprint arXiv:1312.5921 (2013).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bayesian tensor factorization?
Collective tensor factorization?&lt;/p&gt;

&lt;h3 id=&quot;factorization-machine&quot;&gt;Factorization machine&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Steffen Rendle (2010): &lt;a href=&quot;http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf&quot;&gt;Factorization Machines&lt;/a&gt;, in Proceedings of the 10th IEEE International Conference on Data Mining (ICDM 2010), Sydney, Australia.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reduced-rank-regression&quot;&gt;Reduced rank regression&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Alan Julian Izenman, Reduced-rank regression for the multivariate linear model, 1975&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-task-learning&quot;&gt;Multi task learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Kishan Wimalawarne, &lt;a href=&quot;http://papers.nips.cc/paper/5628-multitask-learning-meets-tensor-factorization-task-imputation-via-convex-optimization.pdf&quot;&gt;Multitask learning meets tensor factorization: task imputation via convex optimization&lt;/a&gt;, NIPS, 2014&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;structured-prediction-for-nlp&quot;&gt;Structured prediction for NLP&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Lei, Tao, et al. “&lt;a href=&quot;http://www.anthology.aclweb.org/P/P14/P14-1130.pdf&quot;&gt;Low-rank tensors for scoring dependency structures.&lt;/a&gt;” Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Vol. 1. 2014.&lt;/li&gt;
  &lt;li&gt;Lei, Tao, et al. “&lt;a href=&quot;https://people.csail.mit.edu/taolei/papers/naacl2015.pdf&quot;&gt;High-order lowrank tensors for semantic role labeling&lt;/a&gt;.” Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics–Human Language Technologies (NAACLHLT 2015), Denver, Colorado. 2015.&lt;/li&gt;
  &lt;li&gt;Crammer, Koby, et al. “&lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/CrammerDKSS06.pdf&quot;&gt;Online passive-aggressive algorithms.&lt;/a&gt;” The Journal of Machine Learning Research 7 (2006): 551-585.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convexification-1&quot;&gt;Convexification&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bouchard, Guillaume, Dawei Yin, and Shengbo Guo. “&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v31/bouchard13a.pdf&quot;&gt;Convex collective matrix factorization.&lt;/a&gt;” Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics. 2013.&lt;/li&gt;
  &lt;li&gt;Ryota Tomioka, Taiji Suzuki, &lt;a href=&quot;http://papers.nips.cc/paper/4453-statistical-performance-of-convex-tensor-decomposition.pdf&quot;&gt;Statistical Performance of Convex Tensor Decomposition&lt;/a&gt;, NIPS,  2011&lt;/li&gt;
  &lt;li&gt;Hsu, Daniel, Sham M. Kakade, and Tong Zhang. “&lt;a href=&quot;http://colt2009.cs.mcgill.ca/papers/011.pdf&quot;&gt;A spectral algorithm for learning hidden Markov models.&lt;/a&gt;” Journal of Computer and System Sciences 78.5 (2012): 1460-1480.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.google.com/#q=convex+penalty+sum+of+trace+norms&quot;&gt;Useful Google search&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-i-have-learned&quot;&gt;What I have learned&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;What is matrix completion lower-rank factorization. Connection with rank, redundancy and the inherit nature of guessing missing values.&lt;/li&gt;
  &lt;li&gt;PLDA/LDA/word2vec/GloVe uses MF either explicitly/implicitly(&lt;strong&gt;more to read&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;sign rank is another type of loss function(&lt;strong&gt;more to read&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;SVD is a special case of LRF&lt;/li&gt;
  &lt;li&gt;How LRF/NMF can be solved using SGD and ALS&lt;/li&gt;
  &lt;li&gt;MF can be used for relation extraction. Logic and background knowledge can be incorporated(&lt;strong&gt;more be read&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;Tensor and Tucker decomposition&lt;/li&gt;
  &lt;li&gt;Tensor factorization on relation learning and semantic compositionality(&lt;strong&gt;more to read&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;New subject: (Bayesian)collective matrix decomposition and intuition on its usefulness: sharing parameters among factors(&lt;strong&gt;more be read&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;MF can be used in predictive/discrinative models: factorization machine, multi-task learning, reduced rank regression, structured prediction for NLP(&lt;strong&gt;more be read&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;New subject: convexification (&lt;strong&gt;more be read&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/tutorial/2015/09/09/matrix-factorization-acl-2015</link>
                <guid>http://simpleyyt.github.io/tutorial/2015/09/09/matrix-factorization-acl-2015</guid>
                <pubDate>2015-09-09T16:12:19+02:00</pubDate>
        </item>

        <item>
                <title>Dense Subgraph Discovery: Applications</title>
                <description>&lt;h2 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h2&gt;

&lt;p&gt;Given a graph (network), &lt;em&gt;static&lt;/em&gt; or &lt;em&gt;dynamic&lt;/em&gt;, find a subgraph that has many edges and is &lt;em&gt;densely&lt;/em&gt; connected.&lt;/p&gt;

&lt;h2 id=&quot;example-application&quot;&gt;Example Application&lt;/h2&gt;

&lt;h3 id=&quot;community-detection&quot;&gt;community detection&lt;/h3&gt;

&lt;p&gt;Detect densed connected webpages as they are usually of the same thematic group.&lt;/p&gt;

&lt;p&gt;Or help organizational reconstruction by analyzing the employee email network.&lt;/p&gt;

&lt;p&gt;Or recommend potential friends from the same community.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dense subgraph extraction with application to community detection.&lt;/li&gt;
  &lt;li&gt;Kumar, R., Raghavan, P., Rajagopalan, S., and Tomkins, A. (1999). Trawling the Web for emerging cyber-communities. Computer Networks, 31(11{16):1481{1493.}
### Community search&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given a set of nodes, find the densely connected subgraph that contain the nodes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;M. Sozio and A. Gionis. The community-search problem and how to plan a successful cocktail party. In KDD, pages 939{948, 2010.}&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;organizing-social-events&quot;&gt;Organizing social events&lt;/h4&gt;

&lt;p&gt;Given a list of persons you’d like to invite to a party, find other densed connected friends/acquitances of those persons.&lt;/p&gt;

&lt;p&gt;Or to organize a workshop, you already have a few seed candidates, but you’d like to invite more.&lt;/p&gt;

&lt;h4 id=&quot;tag-suggestion&quot;&gt;Tag suggestion&lt;/h4&gt;

&lt;p&gt;Given a set of tags typed by the user for his blog article, recommend more relevant tags.&lt;/p&gt;

&lt;h4 id=&quot;task-driven-team-formulation&quot;&gt;Task driven team formulation&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;F. Bonchi, F. Gullo, A. Kaltenbrunner, and Y. Volkovich. Core decomposition of uncertain graphs. In KDD, 2014.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;story-detection&quot;&gt;Story detection&lt;/h3&gt;

&lt;p&gt;Find set of entiy nodes(person/location/organization, etc) that are densely connected and popular.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A. Angel, N. Sarkas, N. Koudas, and D. Srivastava. Dense subgraph maintenance under streaming edge weight updates for real-time story identication. PVLDB, 5(6), 2012.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also another about event detection. What’s the difference?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rozenshtein, P., Anagnostopoulos, A., Gionis, A., and Tatti, N. (2014a). Event detection in activity networks. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fraud-detection--graph-based-anomaly-detection&quot;&gt;Fraud detection / graph based anomaly detection&lt;/h3&gt;

&lt;p&gt;Detect the same set of users that like the same set of pages at almost the same time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Beutel, A., Xu, W., Guruswami, V., Palow, C., and Faloutsos, C. (2013). Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119{130.}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;e-commerce---sub-markets-detection&quot;&gt;E commerce - sub-markets detection&lt;/h3&gt;

&lt;p&gt;Context: advertisers by querys for some search engine(eg, Baidu)&lt;/p&gt;

&lt;p&gt;Detect the advertisers and queries that are densely connected.&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-dense-subgraphs-detection&quot;&gt;Hierarchical dense subgraphs detection&lt;/h3&gt;

&lt;p&gt;Similar to find hierarchy of clusters&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Saryuce, A. E., Seshadhri, C., Pinar, A., and Catalyurek, U. V. (2015). Finding the hierarchy of dense subgraphs using nucleus decompositions. In Proceedings of the 24th International Conference on World Wide Web,&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;point-to-point-distance-query&quot;&gt;Point-to-point distance query&lt;/h3&gt;

&lt;p&gt;Example: Google Map Directions, routing in sensor network, indorr/terrain navigation&lt;/p&gt;

&lt;p&gt;For large graph, two steps are performed, preprocessing and querying.&lt;/p&gt;

&lt;p&gt;Hierarchical hub labeling is DSD problem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Delling, D., Goldberg, A. V., Pajor, T., and Werneck, R. (2014). Robust distance queries on massive networks. In Algorithms-ESA 2014, pages 321{333. Springer.}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;frequent-item-set-mining&quot;&gt;Frequent item set mining&lt;/h3&gt;

&lt;p&gt;Similar case: frequent item mining in transaction data(user buys what).&lt;/p&gt;

&lt;p&gt;Transaction data -&amp;gt; binary data -&amp;gt; bipartite graph&lt;/p&gt;

&lt;p&gt;frequent itemsets -&amp;gt; bi-cliques -&amp;gt; DSD&lt;/p&gt;

&lt;h3 id=&quot;social-piggybacking&quot;&gt;social piggybacking&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Gionis, A., Junqueira, F., Leroy, V., Serani, M., and Weber, I. (2013). Piggybacking on social networks. Proceedings of the VLDB Endowment, 6(6):409{420.}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;graph-compression&quot;&gt;Graph compression&lt;/h3&gt;

&lt;p&gt;Compressed graphs(through virtual node) are faster for graph mining tasks, which involve matrix-vector computation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Karande, C., Chellapilla, K., and Andersen, R. (2009). Speeding up algorithms on compressed web graphs. Internet Mathematics, 6(3):373{398.}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;graph-visualization&quot;&gt;Graph visualization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Alvarez-Hamelin, J. I., Dall’Asta, L., Barrat, A., and Vespignani, A. (2005). Large scale networks ngerprinting and visualization using the k-core decomposition. In NIPS.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;some-distributed-graph-mining-library&quot;&gt;Some distributed graph mining library&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Kang, U., Tsourakakis, C. E., and Faloutsos, C. (2009). Pegasus: A peta-scale graph mining system implementation and observations. In Data Mining, 2009. ICDM’09. Ninth IEEE International Conference on, pages 229{238. IEEE.}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next&quot;&gt;Next&lt;/h2&gt;

&lt;p&gt;As in the tutorial:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Density measures&lt;/li&gt;
  &lt;li&gt;Algorithms for static graphs&lt;/li&gt;
  &lt;li&gt;Algorithms for dnyamic graphs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Streamming algorithsm perhaps?&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/07/dsd-applications</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/07/dsd-applications</guid>
                <pubDate>2015-09-07T12:04:15+02:00</pubDate>
        </item>

        <item>
                <title>Personalized Entity Recommendation: A Heterogeneous Information Network Approach</title>
                <description>&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Existing approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NMF directly on the implicit feedback matrix to get the representation for users and items&lt;/li&gt;
  &lt;li&gt;global recommendation model for all users&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Novelty here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;User preference dillusion&lt;/strong&gt;: Use meta-path to derive user-item similarity matrix and apply NMF on each of them to get the latent features&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;personalized recommendation model&lt;/strong&gt;: cluster users into differnt groups and for each group, derive group-specific feature weights. For final score, user-cluster similarity is also considered.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;h3 id=&quot;user-preference-dillusion&quot;&gt;User preference dillusion&lt;/h3&gt;

&lt;p&gt;Define different meta-paths for different semantics. Using movie context for example, we can define the following math-paths:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;user-movie-tag-movie&lt;/code&gt;: user might be interested another movie with the same tag that is contained by one move he watches&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;user-movie-actor-movie&lt;/code&gt;: user might be interested to another movie by the same actor in one movie he watched before&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the &lt;code&gt;PathSim&lt;/code&gt; measure, we can define &lt;em&gt;user-movie&lt;/em&gt; measure as &lt;code&gt;\sum\limit_e^&#39; (s(e, e^&#39;))&lt;/code&gt;, where &lt;code&gt;e \in &lt;/code&gt; all the movies the user watches. It’s simply matrix multiplication.&lt;/p&gt;

&lt;p&gt;For &lt;code&gt;L&lt;/code&gt; meta-paths, we will have &lt;code&gt;L&lt;/code&gt; user-item matrices. And we apply NMF on each of them to derive the path-specific user/item representaiton.&lt;/p&gt;

&lt;p&gt;The final score can be the weighted sum of the individual scores, &lt;code&gt;\sum\limit_l \theta_l U_i V_j&lt;/code&gt;, where &lt;code&gt;U_i&lt;/code&gt; and &lt;code&gt;V_j&lt;/code&gt; are the representation for the &lt;code&gt;ith&lt;/code&gt; user and &lt;code&gt;jth&lt;/code&gt; item respectively. However, this is referred to the global model, where &lt;code&gt;L&lt;/code&gt; weight parameters are shared across all the users.&lt;/p&gt;

&lt;h3 id=&quot;personalized-recommendation-model&quot;&gt;Personalized recommendation model&lt;/h3&gt;

&lt;p&gt;Global model does not distinguish between different users. Users might be interested in the same movie for different reasons.&lt;/p&gt;

&lt;p&gt;Users are clustered into &lt;code&gt;K&lt;/code&gt; groups. The cluster algorithm and user representation is flexible to choose. And there are &lt;code&gt;KL&lt;/code&gt; weight parameters, &lt;code&gt;L&lt;/code&gt; for each group. When calculating the final scores, &lt;code&gt;\sum_k c(i, k)\sum\limit_l \theta_{kl} U_i V_j&lt;/code&gt;, where &lt;code&gt;c(i, k)&lt;/code&gt; are the similarity between the &lt;code&gt;ith&lt;/code&gt; user and &lt;code&gt;kth&lt;/code&gt; group.&lt;/p&gt;

&lt;h3 id=&quot;ranking-based-learning&quot;&gt;Ranking-based learning&lt;/h3&gt;

&lt;p&gt;For implicit feedback, it’s unreasonable to assume negative label for entry with value 0. 0 is a mixture of “not-interested” and “unwatched”. A &lt;em&gt;weaker&lt;/em&gt; assumption is user is more interested in items with value 1 than those with value 0.&lt;/p&gt;

&lt;p&gt;The likelihood is &lt;code&gt;\prod\limit_{u_i} \prod\limit_{(e_a, e_b)} P(e_a &amp;gt; e_b; u_i | \theta)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Adding Gaussian prior, the posterior &lt;code&gt;P(\theta|R)&lt;/code&gt; is sum of log-likelihood plus L2 regularization.&lt;/p&gt;

&lt;p&gt;Stochastic gradient descent can be applied here.&lt;/p&gt;

&lt;p&gt;This method is directly optimized for ranking.&lt;/p&gt;

&lt;h2 id=&quot;what-i-have-learned&quot;&gt;What I have learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Matrix factorization can be used for recommendation problem. Content information as well as network information can be leveraged to fight sparsity.&lt;/li&gt;
  &lt;li&gt;Meta path here is used for user preference dillusion&lt;/li&gt;
  &lt;li&gt;Ranking-based learning: there should be resources and variants on it&lt;/li&gt;
  &lt;li&gt;Clustering is here used as bridge to add personalization(more granularity) while keeping the parameter space relatively modest.&lt;/li&gt;
  &lt;li&gt;Using Gaussian as the prior in the ranking-based optimization lead to L2 optimization&lt;/li&gt;
  &lt;li&gt;precision-at-position and top-k mean reciprocal as the evaluation for ranking-based models.&lt;/li&gt;
  &lt;li&gt;Root mean square error as the evaluation for explicit feedback model&lt;/li&gt;
  &lt;li&gt;Performance analysis on the user feedback sparsity and item popularity is necessary for deeper understanding&lt;/li&gt;
  &lt;li&gt;For collaborative filtering, sparsity is a problem. Possible direction is to leverage external knowledge(network structure, user profile)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;question--future&quot;&gt;Question / future&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Why matrix factorization is better than other methods for CF problem?&lt;/li&gt;
  &lt;li&gt;How NMF is done?&lt;/li&gt;
  &lt;li&gt;How content information can be incorporated? Maybe as links between entities&lt;/li&gt;
  &lt;li&gt;How to co online recommendation? New user/item is being added.&lt;/li&gt;
  &lt;li&gt;Learn from explicit and implicit feedback simultaneously? Rating scores plus watch history&lt;/li&gt;
  &lt;li&gt;How to scale it up, e.g,  when there are millions of movies to recommend?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-paper&quot;&gt;Related paper&lt;/h2&gt;

&lt;h3 id=&quot;cf-technique&quot;&gt;CF technique&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;T. Hofmann. Collaborative filtering via gaussian probabilistic latent semantic analysis. In SIGIR, 2003&lt;/li&gt;
  &lt;li&gt;B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative filtering recommendation algorithms. In WWW, 2001.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;matrix-factorization-for-cf&quot;&gt;Matrix factorization for CF&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In SIGKDD, 2008.&lt;/li&gt;
  &lt;li&gt;Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30–37, 2009.&lt;/li&gt;
  &lt;li&gt;J. D. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In ICML, 2005.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cf-that-uses-content-based-information&quot;&gt;CF that uses content-based information&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Q. Gu, J. Zhou, and C. Ding. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In SDM, 2010.&lt;/li&gt;
  &lt;li&gt;P. Melville, R. J. Mooney, and R. Nagarajan. Content-boosted collaborative filtering for improved recommendations. In AAAI, 2002.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cf-that-uses-network-information&quot;&gt;CF that uses network information&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;H. Ma, I. King, and M. R. Lyu. Learning to recommend with social trust ensemble. In SIGIR, 2009.&lt;/li&gt;
  &lt;li&gt;M. Jamali and M. Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In SIGKDD, 2010.&lt;/li&gt;
  &lt;li&gt;H. Ma, H. Yang, M. Lyu, and I. King. Sorec: social recommendation using probabilistic matrix factorization. In CIKM, 2008.&lt;/li&gt;
  &lt;li&gt;I. Guy, N. Zwerdling, D. Carmel, I. Ronen, E. Uziel, S. Yogev, and S. Ofek-Koifman. Personalized recommendation of social software items based on social relations. In RecSys, 2009.&lt;/li&gt;
  &lt;li&gt;Q. Yuan, L. Chen, and S. Zhao. Factorization vs. regularization: fusing heterogeneous social relationships in top-n recommendation. In RecSys, 2011.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cf-that-uses-entity-similarity&quot;&gt;CF that uses entity similarity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;X. Yu, X. Ren, Q. Gu, Y. Sun, and J. Han. Collaborative filtering with entity similarity regularization in heterogeneous information networks. In IJCAI HINA, 2013.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bayesian-personalized-ranking&quot;&gt;Bayesian personalized ranking&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In UAI, 2009.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nmf&quot;&gt;NMF&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;C. H. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations. PAMI, 2010.&lt;/li&gt;
  &lt;li&gt;Q. Gu, J. Zhou, and C. Ding. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In SDM, 2010.&lt;/li&gt;
  &lt;li&gt;H. Ma, H. Yang, M. Lyu, and I. King. Sorec: social recommendation using probabilistic matrix factorization. In CIKM, 2008.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;information-network-applications&quot;&gt;Information network applications&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu. Integrating meta-path selection with user guided object clustering in heterogeneous information networks. In KDD, 2012&lt;/li&gt;
  &lt;li&gt;Y. Sun, Y. Yu, and J. Han. Ranking-based clustering of heterogeneous information networks with star network schema. In KDD, 2009.&lt;/li&gt;
  &lt;li&gt;M. Ji, J. Han, and M. Danilevsky. Ranking-based classification of heterogeneous information networks. In SIGKDD, 2011.&lt;/li&gt;
  &lt;li&gt;T.-T. Kuo, R. Yan, Y.-Y. Huang, P.-H. Kung, and S.-D. Lin. Unsupervised link prediction using aggregative statistics on heterogeneous social networks. In SIGKDD. ACM, 2013.&lt;/li&gt;
  &lt;li&gt;X. Yu, Q. Gu, M. Zhou, and J. Han. Citation prediction in heterogeneous bibliographic networks. In SDM, 2012.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/06/paper</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/06/paper</guid>
                <pubDate>2015-09-06T18:55:47+02:00</pubDate>
        </item>

        <item>
                <title>PathSim: Meta PathBased TopK Similarity Search in Heterogeneous Information Networks</title>
                <description>&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;In heterogoues information network(multiple types of nodes/edges), similarity measure between entities should have finer semantics. For example, an author can be similar to an another in terms of their influence, published conferences, published topics, co-authorship, etc.&lt;/p&gt;

&lt;h2 id=&quot;contribution&quot;&gt;Contribution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Defined meta-path based similarity measure that distinguishes connection semantics and selects candidates that matches the visibility of the query object(&lt;strong&gt;meta-path framework&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;Efficient algorithms to ranks the top-k similar objects(&lt;strong&gt;fast top-k ranking&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pathsim&quot;&gt;PathSim&lt;/h2&gt;

&lt;p&gt;Given meta-path &lt;code&gt;P&lt;/code&gt;, &lt;strong&gt;PathSim&lt;/strong&gt; is defined as:&lt;/p&gt;

&lt;p&gt;s(i, j) = 2 * M(i, j) / (M(i, i) + M(j, j))&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;M&lt;/code&gt;: &lt;em&gt;commuting matrix&lt;/em&gt;. Shape is &lt;code&gt;(n, n)&lt;/code&gt;, where &lt;code&gt;n&lt;/code&gt; is the number of target objects&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;M(i, j)&lt;/code&gt;: number of paths from object &lt;code&gt;i&lt;/code&gt; to object &lt;code&gt;j&lt;/code&gt; according to &lt;code&gt;P&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;M(i, i)&lt;/code&gt;: number of paths from object &lt;code&gt;i&lt;/code&gt; to itself according to &lt;code&gt;P&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The denomenator assures balance of visibility.&lt;/p&gt;

&lt;p&gt;In this paper, we only consider &lt;em&gt;symmetric&lt;/em&gt; path, something like &lt;code&gt;ABCBA&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;computation-reduction&quot;&gt;Computation reduction&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;M&lt;/code&gt; can be pre-computed. However, saving &lt;code&gt;(n, n)&lt;/code&gt; can be space consuming(&lt;code&gt;40G&lt;/code&gt; in the paper’s example).&lt;/p&gt;

&lt;p&gt;To address this issue. It’s reasonable to save the commuting matrix, &lt;code&gt;M_P&lt;/code&gt; of its half path, (e.g, &lt;code&gt;ABC&lt;/code&gt;) and compute the similarities(vector multiply matrix) on line. Time complexity is &lt;code&gt;O(mn)&lt;/code&gt;, where &lt;code&gt;m&lt;/code&gt; is the number of objects of type &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As there can be many objects not accessible to the target, those objects can be dropped by checking if they are among the neighbors of target object’s neighbors according to &lt;code&gt;M_P&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Time complexity can be further reduced by co-clustering the rows and columns and prune unpromising candidate cluster according to its similarity upper bound. For example, if the cluster’s similarty upperbound is lower than the minimum of the current top-k list’s similarities, the whole cluster can be dropped. Therefore, only part of the similarity computation are materialized.&lt;/p&gt;

&lt;p&gt;In total, two levels of computation reduction is done:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;prune non-accessible nodes&lt;/li&gt;
  &lt;li&gt;prune object cluster which is not promising according to their upperbound.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h2&gt;

&lt;h3 id=&quot;basics&quot;&gt;Basics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Similarity search for graph as an alternative to the traditonal IR techniques(ranking a list of independent documents)&lt;/li&gt;
  &lt;li&gt;A list of application in similarity search for graph: author, paper, image search..&lt;/li&gt;
  &lt;li&gt;Several graph similarity measure: PageRank, SimRank, etc and the problems with them: bias towards high visibility/concentration&lt;/li&gt;
  &lt;li&gt;Basic idea of Random Walk as similarity measure: the probability of starting from x and ending at y&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Meta-Path&lt;/em&gt; as a framework to captures richer semantics. Other measure can be plugged in. For example, Random Walk that follows a certain paths.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;PathSim&lt;/code&gt; as an implementation of &lt;code&gt;Meta-Path&lt;/code&gt; and it considers  among &lt;em&gt;peers&lt;/em&gt;(similar visibility)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;PathSim&lt;/code&gt; can be computing by summing weighted different similarities on different meta-path.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;more-technical-stuff&quot;&gt;More technical stuff&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Commuting matrix: the result of multiplication of adjacancy matrices. For example, given &lt;code&gt;A&lt;/code&gt;, the adj matrix from objects of type &lt;code&gt;a&lt;/code&gt; to type &lt;code&gt;b&lt;/code&gt; and B, the adj matrix from type &lt;code&gt;b&lt;/code&gt; to &lt;code&gt;c&lt;/code&gt;. Then &lt;code&gt;A * B&lt;/code&gt; is the adj matrix from &lt;code&gt;a&lt;/code&gt; to &lt;code&gt;c&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Time consumption reduction can be gained in top-k ranking problem in this scenario as we don’t have to compute all the similarity values in order to only get the top-k&lt;/li&gt;
  &lt;li&gt;Co-clustering as another technique/problem and using it to further reduce computation by reducing the column dimension&lt;/li&gt;
  &lt;li&gt;Primary eigen vector can be used as &lt;em&gt;authority ranking&lt;/em&gt; of objects&lt;/li&gt;
  &lt;li&gt;Jaccard coefficient: similarity between two sets&lt;/li&gt;
  &lt;li&gt;Linear algebra theorems: Holder’s Inequality and Cauchy-Schwarz inequality, are used to derive the similarty upper bound.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methedologies&quot;&gt;Methedologies&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Detailed empiricial analysis of influencing factors of pruning power(commuting matrix density, size of neighborhood of query)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;How to find relevant objects of &lt;strong&gt;different&lt;/strong&gt; types?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similar images to this tag/keywords?&lt;/p&gt;

&lt;p&gt;We need to compute the whole matrix multiplication?&lt;/p&gt;

&lt;p&gt;Maybe first get similar images to the tag and then use meta-path to get similar images to the selected images? Local minimum?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can we learn such path automatically given several matching pairs?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Start with a list of candidates and adjusting the weights?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Is balance of invisibility necessarily important?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It depends on the senario. Sometimes, we might want popular objects, for example, users similar to the query user but are much more famous.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are the other applications of co-clustering?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-papers&quot;&gt;Related Papers&lt;/h2&gt;

&lt;h3 id=&quot;similarity-search-in-homogenous-network&quot;&gt;Similarity search in homogenous network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;G. Jeh and J. Widom. Simrank: a measure of structural-context similarity. In KDD’02, 538–543, 2002&lt;/li&gt;
  &lt;li&gt;G. Jeh and J. Widom. Scaling personalized web search. In WWW’03, 271–279, 2003.&lt;/li&gt;
  &lt;li&gt;X. Xu, N. Yuruk, Z. Feng, and T. A. J. Schweiger. Scan: a structural clustering algorithm for networks. In KDD’07,824–833, 2007.&lt;/li&gt;
  &lt;li&gt;D. Lizorkin, P. Velikhov, M. Grinev, and D. Turdakov. Accuracy estimate and optimization techniques for simrank computation. PVLDB, 1(1):422–433, 2008.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eigen-value-as-authority-measure&quot;&gt;eigen value as authority measure&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Y. Sun, J. Han, P. Zhao, Z. Yin, H. Cheng, and T. Wu. Rankclus: integrating clustering with ranking for heterogeneous information network analysis. In EDBT’09, 565–576, 2009&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;co-clustering&quot;&gt;Co-clustering&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;I. S. Dhillon, S. Mallela, and D. S. Modha. Information-theoretic co-clustering. In KDD’03, 89–98,2003&lt;/li&gt;
  &lt;li&gt;F. Pan, X. Zhang, and W. Wang. Crd: fast co-clustering on large datasets utilizing sampling-based matrix decomposition. In SIGMOD’08, 173–184, 2008.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;random-walk&quot;&gt;Random Walk&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;D. Fogaras, B. R´acz, K. Csalog´any, and T. Sarl´os. Towards scaling fully personalized pageRank: algorithms, lower bounds, and experiments. Int. Math., 2(3):333–358, 2005&lt;/li&gt;
  &lt;li&gt;H. Tong, C. Faloutsos, J. Pan. Fast Random Walk with Restart and Its Applications. In ICDM’06, 613–622, 2006.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-top-k-ranking&quot;&gt;Fast top-k ranking&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;M. Gupta, A. Pathak, and S. Chakrabarti. Fast algorithms for topk personalized pagerank queries. In WWW’08, 1225–1226, 2008.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pagerank-related&quot;&gt;PageRank related&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A. Balmin, V. Hristidis, and Y. Papakonstantinou. Objectrank: authority-based keyword search in databases. In VLDB’04, 564–575, 2004.&lt;/li&gt;
  &lt;li&gt;Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. Object-level ranking: bringing order to web objects. In WWW’05, 567–574, 2005.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Should RandomWalk, PageRank and similarity each be merged?&lt;/p&gt;

</description>
                <link>http://simpleyyt.github.io/paper/2015/09/05/paper</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/05/paper</guid>
                <pubDate>2015-09-05T21:00:00+02:00</pubDate>
        </item>

        <item>
                <title>ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering</title>
                <description>&lt;h2 id=&quot;what-it-solves&quot;&gt;What it solves&lt;/h2&gt;

&lt;p&gt;Given a knowledge base, document corpus and a set of target types, T, the method:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detects candidate entity mentions&lt;/li&gt;
  &lt;li&gt;Types the mentions to &lt;em&gt;one&lt;/em&gt; of T or Not-Of-Interest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a side-product, it also:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detects relation phrases&lt;/li&gt;
  &lt;li&gt;Clusters the phrases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;basic-workflow&quot;&gt;Basic workflow&lt;/h2&gt;

&lt;p&gt;It works as follow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mining possible entity mentions&lt;/li&gt;
  &lt;li&gt;Construct heterogenous graph on entity mentions, mention names and relation phrases&lt;/li&gt;
  &lt;li&gt;Based on confidently mapped entity mentions, perform label propagation and relation phrase clustering simultaneously by modeling it a mix integer optimization problem&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Three challenges it tries to address:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Domain restriction&lt;/strong&gt;: if using chunkers to detect entity mention, it might perform badly in domain-specific corpus. This papers makes little linguistic assumptions and mine the phrases in a data-driven way.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Name ambiguity&lt;/strong&gt;: some approaches treat mentions with the same surface names as one entity. This is probelmatic. This paper predicts each mention’s type individually&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context sparsity&lt;/strong&gt;: some relation phrase occurs rarely, making it hard to provide useful/reliable information for its argument. This paper clusters the phrases so that more information can be propagated/shared&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;steps-in-detail&quot;&gt;Steps In detail&lt;/h2&gt;

&lt;h3 id=&quot;entity-mention-and-relation-phrase-detection&quot;&gt;Entity mention and relation phrase detection&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Mine &lt;em&gt;frequent continuous patterns&lt;/em&gt; up to a maximum length&lt;/li&gt;
  &lt;li&gt;Merge the patterns agglomeratively and greedily. At each step, the decision is made by choosing the one with the highest &lt;em&gt;global significance score&lt;/em&gt;, kind of &lt;em&gt;rectified phrase frequency&lt;/em&gt;. Meanwhile, the merged phrases should obey certain POS tag patterns. &lt;em&gt;NN+&lt;/em&gt; for mentions&lt;/li&gt;
  &lt;li&gt;Merging stops when the score is below some threshold.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This step can be replaced by the method in “SegPhrase” paper.&lt;/p&gt;

&lt;h3 id=&quot;heterogenous-graph-construction&quot;&gt;Heterogenous graph construction&lt;/h3&gt;

&lt;p&gt;Three types of nodes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Entity mention&lt;/li&gt;
  &lt;li&gt;Mention surface name(number far fewer than number of entity mentions)&lt;/li&gt;
  &lt;li&gt;Relation phrase&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mention surface name is included to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;reduce time complexity / sparsity&lt;/li&gt;
  &lt;li&gt;(indirectly) allow more efficient information propagation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Three types of edges/subgraphs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;mention-name: type info on name directly gives cues on types of mentions
For each mention or name, its type is represented by a one-hot vector&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;name-phrase: if we know “XX” is “Food” and “serve XX”, then if we see “serve YY”, then “YY” might be “Food”. “serve” serves like a bridge. &lt;strong&gt;Hypothesis 1&lt;/strong&gt;&lt;br /&gt;
Type for phrase is represented by two(left/right arg) one-hot vector&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;mention-mention: if two mentions are &lt;em&gt;strongly correlated&lt;/em&gt;, they tend to share similar type. &lt;strong&gt;Hypothesis 2&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;clustype-algorithm&quot;&gt;&lt;em&gt;ClusType&lt;/em&gt; algorithm&lt;/h3&gt;

&lt;p&gt;Two more hypo:
&lt;em&gt;Hypothesis 3&lt;/em&gt;: if two relation phrases are in the same cluster, they tend have similar types
&lt;em&gt;Hypothesis 4&lt;/em&gt;: if two relation phrases tend to 1, words, 2, context words, 3, similar types signatures, they tend to be in similar cluster&lt;/p&gt;

&lt;p&gt;And these hypothesis are encoded in an optimization problem, 3 terms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Minimize the distance between mention types and corresponding phrase type signature(&lt;strong&gt;Hypothesis 1&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;Modeled as clustering as an non-negative matrix factorization problem plus some consensus matrix(because of multiple views). &lt;strong&gt;Hypothesis 3 and 4&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Miminize distance between mention type and aggregation(e.g, &lt;em&gt;sum&lt;/em&gt;) from corresponding name type and relation phrase type. Plus, strongly correlated mentions should have similar types(&lt;strong&gt;Hypothesis 2&lt;/strong&gt;). Last, should be consistent with types of initial seed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Optimization:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mix integer problem. NP-hard. Using relaxation and alternating algorithm(Updating subsets of parameters step by step) as approximation.&lt;/p&gt;

&lt;p&gt;Don’t quite understand.&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A cool thing: entity typing&lt;/li&gt;
  &lt;li&gt;Limitations of NER: domain restrictness(human effort)&lt;/li&gt;
  &lt;li&gt;Limitations of entity linking: limited coverage/freshness(entity may not be in KB)&lt;/li&gt;
  &lt;li&gt;The three challenges: entity detection, name ambiguity and context sparsity&lt;/li&gt;
  &lt;li&gt;The idea of mutually enhancing is also applied in the &lt;em&gt;SegPhrase&lt;/em&gt; paper, in which segmentation and quality estimation enhances each other&lt;/li&gt;
  &lt;li&gt;Another way to do phrase mining: agglomeratively merging patterns under POS pattern constraint. The decision is based on some kind of &lt;em&gt;rectified frequency&lt;/em&gt;, which also appeared in &lt;em&gt;SegPhrase&lt;/em&gt; paper&lt;/li&gt;
  &lt;li&gt;Multiplication of N co-occurence matrices(of N+1 types of objects) lead to then co-occurence matrix between 1st type of obj and (N+1)th type of obj&lt;/li&gt;
  &lt;li&gt;I might need to know more about discrete optimization / integer programming&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;splotlight.dbpedia.org&lt;/code&gt; can be used for entity linking&lt;/li&gt;
  &lt;li&gt;Very stunnhing entity coverage (&lt;code&gt;&amp;lt;7%&lt;/code&gt;) by some entity linking method&lt;/li&gt;
  &lt;li&gt;Non-negtive matrix factorization(NMF), &lt;code&gt;min_{U,V}||F - UV||&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Clustering as an NMF problem&lt;/li&gt;
  &lt;li&gt;Relaxation and alternating minimization as approximation techniqu for mix-integer programming&lt;/li&gt;
  &lt;li&gt;Two broad types of efforts for entity typing: weak-supervision(bootstrapping)and distant supervision&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;my-questions-and-thinking&quot;&gt;My questions and thinking&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Phrase have left/right argument: is it reasonable to distinguish the arguments by left/right? How about three or more arguments?&lt;/li&gt;
  &lt;li&gt;More cues for surface name, besides the relation or mention? How about from knowledge base, even though the name is ambigous&lt;/li&gt;
  &lt;li&gt;Vector representation for surface name can be an over-simplification, as name can be ambiguous. Can we apply some (non)-linear transformation function that transforms the type by context(also some vector)?&lt;/li&gt;
  &lt;li&gt;Can we also use entity mention clustering to further reduce the sparsity?&lt;/li&gt;
  &lt;li&gt;Should we normalize the relation phrases(passive/active voice)?&lt;/li&gt;
  &lt;li&gt;Representation for the phrases is sparse for now(BoW). Any stuff like &lt;em&gt;phrase2vec&lt;/em&gt; like &lt;em&gt;word2vec&lt;/em&gt;?&lt;/li&gt;
  &lt;li&gt;Can we &lt;strong&gt;link&lt;/strong&gt; the mentions to KB also? For example, KB contain the  mentioned entity but doesn’t record its surface name.&lt;/li&gt;
  &lt;li&gt;Can we model &lt;strong&gt;multiple&lt;/strong&gt; types? For now, only one type is assigned.&lt;/li&gt;
  &lt;li&gt;Is the POS tagger sensitive to domain? If so, the entity mention detection might be problematic? Also, for Twitter, specific taggers for Twitter might be used.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;related-interesting-topics&quot;&gt;Related interesting topics&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Online learning/prediction(forever learner)?&lt;/li&gt;
  &lt;li&gt;Scalability: can the integer programming algorithm be applied to large scale entity typing&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;related-papers-and-comparison&quot;&gt;Related papers and comparison&lt;/h2&gt;

&lt;h3 id=&quot;entity-typing--distance-supervision&quot;&gt;Entity typing / distance supervision&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;T. Lin, O. Etzioni, et al. No noun phrase left behind: detecting and typing unlinkable entities. In EMNLP, 2012.&lt;/li&gt;
  &lt;li&gt;N. Nakashole, T. Tylenda, and G. Weikum. Fine-grained semantic typing of emerging entities. In ACL, 2013.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;entity-linking&quot;&gt;Entity linking&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;W. Shen, J. Wang, and J. Han. Entity linking with a knowledge base: Issues, techniques, and solutions. TKDE, (99):1-20, 2014.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;label-propagation&quot;&gt;Label propagation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;W. Shen, J. Wang, P. Luo, and M. Wang. A graph-based approach for ontology population with named entities. In CIKM, 2012.&lt;/li&gt;
  &lt;li&gt;P. P. Talukdar and F. Pereira. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In ACL, 2010.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relation-extraction&quot;&gt;Relation extraction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP, 2011.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nmfclustering&quot;&gt;NMF&amp;amp;Clustering&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;J. Liu, C. Wang, J. Gao, and J. Han. Multi-view clustering via joint nonnegative matrix factorization. In SDM, 2013.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dimension-reduction&quot;&gt;Dimension reduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;X. He and P. Niyogi. Locality preserving projections. In NIPS, 2004.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;discrete-optimization&quot;&gt;Discrete optimization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. JOTA, 109(3):475-494,2001.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/02/paper</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/02/paper</guid>
                <pubDate>2015-09-02T20:49:00+02:00</pubDate>
        </item>

        <item>
                <title>Mining Quality Phrase from Massive Text Corpora</title>
                <description>&lt;h2 id=&quot;basic-ideas&quot;&gt;Basic ideas&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: mine quality phrases&lt;/li&gt;
  &lt;li&gt;Traditional approach: use raw frequency as the indicator&lt;/li&gt;
  &lt;li&gt;Problem: raw frequency is inaccurate(“support vector” in “support vector machine”)&lt;/li&gt;
  &lt;li&gt;Solution: &lt;strong&gt;rectify&lt;/strong&gt; the frequency by segmenting the documents(ambiguity can be resolved)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Train a prhase quality classifier based on labeled quality phrases and bad phrases(&lt;em&gt;concordance&lt;/em&gt; and &lt;em&gt;informative&lt;/em&gt; feautures)&lt;/li&gt;
  &lt;li&gt;Segment the documents by leveraging the classifier(some dyanmic programming algorithm)&lt;/li&gt;
  &lt;li&gt;Rectify the raw frequency based on the segmentation(Hard-EM/Viterbi training algorithm)&lt;/li&gt;
  &lt;li&gt;Retrain the classifier using the new features based on the rectified frequency&lt;/li&gt;
  &lt;li&gt;Repeat 2 and 3&lt;/li&gt;
  &lt;li&gt;Filter low rectified phrases&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I/O:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input: positive/negative phrases, documents&lt;/li&gt;
  &lt;li&gt;Output: segmented documents and ranked list of phrases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Knew&lt;/th&gt;
      &lt;th&gt;Learned&lt;/th&gt;
      &lt;th&gt;Question&lt;/th&gt;
      &lt;th&gt;Pointers&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Similar problems to phrase segmentation: query segmentation, Chinese word segmentation&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PMI as concordance measure&lt;/td&gt;
      &lt;td&gt;point-wise KL: takes frequency into account, gives higher score for more frequent phrases&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IDF as weighting&lt;/td&gt;
      &lt;td&gt;Interpretation: informativeness&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;New measurements on “informativeness”: contain-stopwords, in-quotation&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Ways to reduce labeling effort: PU learning, active learning&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Li, 2003, Learning to classify texts using positive and unlabeled data. Settles, 2012, Active Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Substring frequency counting&lt;/td&gt;
      &lt;td&gt;A linear time version that utilises some properties&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dynamic programming&lt;/td&gt;
      &lt;td&gt;DP for phrase segmentation&lt;/td&gt;
      &lt;td&gt;Possible for query segmentations?&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Generative model, HMM&lt;/td&gt;
      &lt;td&gt;How this framework is fitted in this problem. Length penalty&lt;/td&gt;
      &lt;td&gt;Can we use discriminative models to use more features&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Viterbi decoding&lt;/td&gt;
      &lt;td&gt;Viterbi training, aka Hard-EM. Bawn-Welch as Soft-EM.&lt;/td&gt;
      &lt;td&gt;What’s the difference?&lt;/td&gt;
      &lt;td&gt;Book: Bishop, 2006, Pattern recognition and machine learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Binary search&lt;/td&gt;
      &lt;td&gt;Used for finding the optimial hyper-parameter, length penalty term&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Def of odd&lt;/td&gt;
      &lt;td&gt;Applied here to compare proba of two events(‘phrase’ and ‘should be splitted’)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aho-Corasick&lt;/td&gt;
      &lt;td&gt;Match-string matching: locates a finite set of strings&lt;/td&gt;
      &lt;td&gt;Time/space complexity&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;K-means&lt;/td&gt;
      &lt;td&gt;can be used for samping representative labeling instances&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Wikipedia&lt;/td&gt;
      &lt;td&gt;Phrases can be collected fro Wikipedia&lt;/td&gt;
      &lt;td&gt;How to narrow down the ranges(for CS only)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Downsample: sample a small portion to make labeled data more balanced&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bag of Word&lt;/td&gt;
      &lt;td&gt;Bag of Phrase&lt;/td&gt;
      &lt;td&gt;How they compare in different tasks(IR, text classification)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;word2vec&lt;/td&gt;
      &lt;td&gt;word2phrase&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;paper-writingorganization-discvoery&quot;&gt;Paper writing/organization discvoery&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;When display rec/recall/f1, display the count as well(support, #match, etc)&lt;/li&gt;
  &lt;li&gt;Performance comparison: time consumption, data file size can be included, proportion of different modules&lt;/li&gt;
  &lt;li&gt;Feature contribution(weight as the indicator for example)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;possible-improvement&quot;&gt;Possible improvement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;No need for human labeling, distant training(phrases from Wikipedia/Freebase)&lt;/li&gt;
  &lt;li&gt;Linguistic-rich patterns to extract candidates&lt;/li&gt;
  &lt;li&gt;For other languages: tranfer learning&lt;strong&gt;?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://simpleyyt.github.io/paper/2015/09/02/paper</link>
                <guid>http://simpleyyt.github.io/paper/2015/09/02/paper</guid>
                <pubDate>2015-09-02T01:54:00+02:00</pubDate>
        </item>

        <item>
                <title>Useful resources from Kaggler Meetup Helsinki</title>
                <description>&lt;h2 id=&quot;machine-learning-theory&quot;&gt;Machine Learning theory&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://work.caltech.edu/telecourse.html&quot;&gt;Learning from Data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interesting topics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;VC Dimension&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bias-Variance Trade-off&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Kernel Methods&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Radial Basis Functions&lt;/li&gt;
  &lt;li&gt;3 Learning Principles&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://lagunita.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about&quot;&gt;Statistical Learing&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;boosting&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;random forests&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;boostrap&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;splines and generalized additive models&lt;/li&gt;
  &lt;li&gt;tree-based methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-analysis&quot;&gt;Data Analysis&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://cs109.github.io/2015/&quot;&gt;Harvard CS109 Data Science&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interesting topics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;data wrangling, cleaning&lt;/li&gt;
  &lt;li&gt;sampling to get a suitable data set&lt;/li&gt;
  &lt;li&gt;data management to be able to access big data quickly and reliably&lt;/li&gt;
  &lt;li&gt;exploratory data analysis to generate hypotheses and intuition&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-mining&quot;&gt;Data Mining&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/course/patterndiscovery&quot;&gt;Pattern Discovery in Data Mining&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Some topics on graph mining&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;big-data--spark&quot;&gt;Big Data / Spark&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x&quot;&gt;Big Data with Apache Spark&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apply Log Mining&lt;/li&gt;
  &lt;li&gt;Textual Entity Recognition&lt;/li&gt;
  &lt;li&gt;Collaborative Filtering&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x&quot;&gt;Scalable Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;develop &lt;em&gt;scalable?&lt;/em&gt; machine learning pipelines&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kaggle-competition&quot;&gt;Kaggle competition&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/c/dato-native&quot;&gt;Predict which web pages served by StumbleUpon are sponsored&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Might involve text mining techniques&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/resources/2015/09/02/resources</link>
                <guid>http://simpleyyt.github.io/resources/2015/09/02/resources</guid>
                <pubDate>2015-09-02T01:00:00+02:00</pubDate>
        </item>

        <item>
                <title>Summary: Successful Data Mining Methods for NLP(ACL2015)</title>
                <description>&lt;h2 id=&quot;topic-modeling--phrase-mining&quot;&gt;Topic modeling &amp;amp; phrase mining&lt;/h2&gt;

&lt;p&gt;What I learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Three ways to combine phrase mining and topic mining(pre, post and joint).&lt;/li&gt;
  &lt;li&gt;First phrase mining and then topic modeling has its justification(words in phrase tend to exhbit the same topic) and &lt;code&gt;PhraseLDA&lt;/code&gt; is a LDA-variant.&lt;/li&gt;
  &lt;li&gt;There are some  ways to &lt;strong&gt;evaluate&lt;/strong&gt; topic models, for example: coherence and phrase intrusion.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;phrase-mining&quot;&gt;Phrase mining&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/apps/pubs/default.aspx?id=241783&quot;&gt;Mining Quality Phrases from Massive Text Corpora, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Several aspects to model the quality of phrase: &lt;em&gt;popularity&lt;/em&gt;(frequency), &lt;em&gt;concordance&lt;/em&gt;(the words tend to appear together), &lt;em&gt;informativeness&lt;/em&gt;(they don’t appear everywhere), &lt;em&gt;completeness&lt;/em&gt;(nothing is left nor extra)&lt;/li&gt;
  &lt;li&gt;Frequency indicates the &lt;code&gt;phraseness&lt;/code&gt;. However, raw frequency doesn’t tell the whole truth. &lt;code&gt;cnt(&quot;vector machine&quot;) &amp;gt;= cnt(&#39;support vector machine&#39;)&lt;/code&gt;. It should be rectified. One way is to segment the document into phrases and then count.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to segment the document into phrases?&lt;/li&gt;
  &lt;li&gt;How to bootstrap to get the training data for the classifier?&lt;/li&gt;
  &lt;li&gt;How to use distant supervision to avoid human labeling?&lt;/li&gt;
  &lt;li&gt;Can it be combined with the paper in the following section?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;entity-linkingtyping&quot;&gt;Entity linking/typing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.engr.illinois.edu/~xren7/fp611-ren.pdf&quot;&gt;ClusType: Effective Entity Recognition and Typing by Relation Phrase‐Based Clustering, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intuition:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;label propagation on the network where nodes are relation phrase, surface name and mentions&lt;/li&gt;
  &lt;li&gt;relation phrase can tell something about the types of surface names&lt;/li&gt;
  &lt;li&gt;surface name can tell something about the types of mentions&lt;/li&gt;
  &lt;li&gt;mentions can also tell something about the types of other mentions(e.g, with the same surface names)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Three challenges in entity linking/typing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mention extraction(chunker is used which cannot adapt to changing/specific domains)&lt;/li&gt;
  &lt;li&gt;surface name ambiguity(prev approaches map the same surface names to one entity)&lt;/li&gt;
  &lt;li&gt;context sparsity(two entities in the same relation described in different ways)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;minimum linguistic knowledge&lt;/li&gt;
  &lt;li&gt;model each mention&lt;/li&gt;
  &lt;li&gt;clustering the relation phrases to allow more typing information sharing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is semi-supervised way to perform entity linking/typing!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If the POS tagger is also sensitive to domain(e.g, news and twitter), any better ways to extract entity mention?&lt;/li&gt;
  &lt;li&gt;How to model the type inference as a joint optimization problem?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to use the KB and annotate part of the heterogenous graph to get started? It’s a entity linking/typing problem?&lt;/li&gt;
  &lt;li&gt;Modeling the typing of phrase relation might be over-simplistic(e.g, passive/active voices, tri-relation)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More related papers on the &lt;a href=&quot;http://hanj.cs.illinois.edu/slides/dmnlp15.pdf&quot;&gt;original slides&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;graph-mining-and-nlp&quot;&gt;Graph-mining and NLP&lt;/h2&gt;

&lt;h3 id=&quot;applications&quot;&gt;Applications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Q&amp;amp;A&lt;/li&gt;
  &lt;li&gt;pattern-based classification&lt;/li&gt;
  &lt;li&gt;Entity morph resolution&lt;/li&gt;
  &lt;li&gt;personalized recommendation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;related-papers&quot;&gt;Related papers&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Unsupervised entity linking: &lt;a href=&quot;http://nlp.cs.rpi.edu/paper/amrel.pdf&quot;&gt;Unsupervised Entity Linking with Abstract Meaning Representation, 2015&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Similarity search: &lt;a href=&quot;http://www-dev.ccs.neu.edu/home/yzsun/papers/vldb11_topKSim.pdf&quot;&gt;PathSim: Meta Path-Based Top-K Similarity Search in Heterogeneous Information Networks, 2011&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Personalized recommendation: &lt;a href=&quot;http://web.engr.illinois.edu/~hanj/pdf/wsdm14_xyu.pdf&quot;&gt;Personalized Entity Recommendation: A Heterogeneous Information Network Approach, 2014&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Co-author prediction: &lt;a href=&quot;http://www.ccs.neu.edu/home/yzsun/papers/asonam11_pathpredict.pdf&quot;&gt;CoAuthor Relationship Prediction in Heterogeneous Bibliographic Networks, 2011&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Entity morph resolution: &lt;a href=&quot;http://www.aclweb.org/anthology/P13-1107&quot;&gt;Resolving Entity Morphs in Censored Data, 2013&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Question&amp;amp;answering: &lt;a href=&quot;http://www.vldb.org/pvldb/vol7/p565-yang.pdf&quot;&gt;Schemaless and Structureless Graph Querying, 2014&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Authorship classification: &lt;a href=&quot;http://hanj.cs.illinois.edu/pdf/sigir11_skim.pdf&quot;&gt;Authorship Classification: A Discriminative Syntactic Tree Mining Approach, 2011&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/paper-reading/2015/08/29/dmnlp2015</link>
                <guid>http://simpleyyt.github.io/paper-reading/2015/08/29/dmnlp2015</guid>
                <pubDate>2015-08-29T19:09:41+02:00</pubDate>
        </item>

        <item>
                <title>Studying Note: Gibbs Sampling</title>
                <description>&lt;p&gt;This is &lt;strong&gt;NOT&lt;/strong&gt; a tutorial on Gibbs Sampling. But just a bunch of questions I had when reading the tutorial &lt;a href=&quot;http://leonidzhukov.net/hse/2013/stochmod/papers/intro_to_mcmc_mackay.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;my-questions&quot;&gt;My Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;How to model the full conditional probability, e.g., \( P(X_i | X_1, \cdots, X_{i-1},  X_{i+1}, \cdots, X_{K}) \)?&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;If we know the explicit form of \( P(\bf{X}) \), then we should know the full conditional probability.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: is the \( P(\bf{X}) \) always known? Or even it is know, is it the case that the full conditional probability is known?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Why sampling from \( P(\bf{X}) \) is harder than sampling each variable in turn?&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;One dimensional data should be simpler to sample from because of the dimension. Think of the sampling process as sample from a list of bars with different lengths. If there are many dimensions, the landscape will be too large.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;If every proposal is always accepted, then how to make sure the proposal is within the range of landscape?&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Another question might be:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Why are there refusal for the sample points for Metropolis Sampling?&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;My answer:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Because jumping probability distribution is defined by us, thus the sample point might jump into the extremely low probability region.&lt;/p&gt;

    &lt;p&gt;Back to this question, the intuition might be, because we are sampling &lt;em&gt;quite&lt;/em&gt; approximately from \(P(\bf{X})\) using the full conditional probability, there are no such problem in Metropolis Sampling?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Why is it called Heat Bath Algorithm also?&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;\(P(X,Y)\) can be rewritten using \( P(X|Y) \) and \(P(Y|X) \) (The Hammersley-Clifford Theorem). So what?&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;So \(P(X,Y)\) can known through the knowledge of \(P(X|Y)\) and \(P(Y|X) \).&lt;/p&gt;

    &lt;p&gt;This sampling is equivalent to:&lt;/p&gt;

    &lt;p&gt;Sampling \( (X_{i+1}, Y_{i+1}) \) by \( X_{i+1} \sim P(X|Y_i) \) and \( Y_{i+1} \sim P(Y|X_{i+1}) \) approximates sampling \( (X,Y) \) from \( P(X,Y) \).&lt;/p&gt;

    &lt;p&gt;Intuitively, this &lt;em&gt;seems&lt;/em&gt; to be right.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comparison-between-gibbs-sampler-and-metropolis-sampler&quot;&gt;Comparison between Gibbs Sampler and Metropolis Sampler&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Similarity:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They are both random walk methods. The next sample point depends on the previous one.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Difference:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gibbs &lt;em&gt;has to&lt;/em&gt; work with more than two parameters.&lt;/p&gt;

&lt;p&gt;Gibbs has no refusal for sample points, this could be a good thing as it might accelerate the sampling process, while Metropolis requires that.&lt;/p&gt;

&lt;p&gt;Gibbs requires that the full conditional probability is known, while Metropolis does not.&lt;/p&gt;

&lt;h2 id=&quot;futhur-questions&quot;&gt;Futhur Questions&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;What is Gibbs distribution?&lt;/li&gt;
  &lt;li&gt;What is &lt;em&gt;Hammersley-Clifford Theorem&lt;/em&gt;? And why is so important for Gibbs sampling and for other methods as mentioned &lt;a href=&quot;http://www.idi.ntnu.no/~helgel/thesis/forelesning.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In what cases the full conditional distribution is unknown so that Gibbs sampling can be used?&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>http://simpleyyt.github.io/study-note/2014/09/04/gibbs-sampling</link>
                <guid>http://simpleyyt.github.io/study-note/2014/09/04/gibbs-sampling</guid>
                <pubDate>2014-09-04T22:43:00+02:00</pubDate>
        </item>

        <item>
                <title>Study Note: Introduction to Monte Carlo Method</title>
                <description>&lt;p&gt;This is the study note of this &lt;a href=&quot;http://leonidzhukov.net/hse/2013/stochmod/papers/intro_to_mcmc_mackay.pdf&quot;&gt;Introduction to Monte Carlo Methods&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Problem we are facing:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To generate samples from \(P(x)\).&lt;/li&gt;
  &lt;li&gt;To estimate the expectation under the distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we know 1, the 2 can be solved by sampling.&lt;/p&gt;

&lt;p&gt;Assume \(P(x)\) can be &lt;em&gt;evaluated&lt;/em&gt; to be multiplicative constant, that is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x) = P^{*}(x) / Z&lt;/script&gt;

&lt;p&gt;Where \(Z\) is the normalising constant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My questions&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;why called &lt;strong&gt;evaluated&lt;/strong&gt; and how is it done?&lt;/li&gt;
  &lt;li&gt;why it is a multiplicative constant, not additive constant?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But sampling from \(P(x)\) is still difficult because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(Z\) is unknown.&lt;/li&gt;
  &lt;li&gt;Even if \(Z\) is known, sampling(especially from high-dimensional space) is challenging for computers(in the numerical sense), because knowing how to &lt;strong&gt;plot&lt;/strong&gt; the density function does not necessarily mean it is easy to &lt;strong&gt;sample&lt;/strong&gt; from it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s assume we have the explicit form of the distirbution we want to sample from.  We can discretize the x axis and make the plot into a list of bins. Then use uniform distribution to sample based on the height of the bins.&lt;/p&gt;

&lt;p&gt;The essay’s idea is that given \( P^{*}(x) \), in order to get \(P(X)\), we need to calculate \(Z\). Given high data dimension, the state space is large.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My questions&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can I say discretization is the general approach to sample numerically?&lt;/li&gt;
  &lt;li&gt;\(Z\) seems to be an important element, but why do we introduce it? Again, how \( P^{*}(x) \) is retrieved.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;uniform-sampling&quot;&gt;Uniform Sampling&lt;/h2&gt;

&lt;p&gt;We can use sampling to estimate the expectation. But given the existence of typical set especially for high dimensional data. It is often fairly possible not to hit any point in the typical set at all when doing the sampling!&lt;/p&gt;

&lt;p&gt;So we need to use different sampling methods.&lt;/p&gt;

&lt;h2 id=&quot;importance-sampling&quot;&gt;Importance Sampling&lt;/h2&gt;

&lt;p&gt;This is used to solve the expectation estimation problem.&lt;/p&gt;

&lt;h3 id=&quot;basic-idea&quot;&gt;Basic Idea&lt;/h3&gt;

&lt;p&gt;Given that we know the form of the density function, \(P(x)\), which could be hard to sample from, we use a simpler density function to approximate the \(P(x)\). And we call it sampler density and denote it as \(Q(x)\), which could be a Gaussian or Cauchy distribution.&lt;/p&gt;

&lt;p&gt;As their might be inaccuracy when sampling from \(Q(x)\) compared to the original \(P(x)\), we introduce a weight, \( P(x_i) / Q(x_i) \) to modify the effect of the sampling points.&lt;/p&gt;

&lt;p&gt;For example, if \( Q(x_i) &amp;gt; P(x_i) \), \( x_i \) is over-presented, a weight, \( P(x_i) / Q(x_i) \) below 1, is multiplied to the value \( \phi_{x_i} \). The idea goes vice versa.&lt;/p&gt;

&lt;p&gt;And as the sampling goes on, it is proved to converge.&lt;/p&gt;

&lt;h3 id=&quot;practical-problem&quot;&gt;Practical Problem&lt;/h3&gt;

&lt;p&gt;There might be region where \(Q(x)\) is small while \(P(x)\) are large and \( \phi(x)P(x) \) are large. Then x might had little chance falling into that region. And the big contribution from that region is absent&lt;/p&gt;

&lt;p&gt;In one example provided by the author, Cauchy sampler is used and converges only after 5000 samplings, while Gaussian sampler does not converge even after ten thousand samplings.&lt;/p&gt;

&lt;p&gt;In contrast, Cauchy sampler has heavier tails than Gaussian sampler.&lt;/p&gt;

&lt;h3 id=&quot;multiple-dimension&quot;&gt;Multiple Dimension&lt;/h3&gt;

&lt;p&gt;Assume a uniform distribution inside a sphere, the proposal density is multi dimensional Guassian.&lt;/p&gt;

&lt;p&gt;It can be shown that the weights of several sample points will dominate the others. For \(N=1000\), the largest weight will be \( 10^{19} \) larger than the median weight.&lt;/p&gt;

&lt;p&gt;Problem with importance sampling:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(Q\) should be a good approximation of \(P\) so that \(Q\) lies in the typical set of \(P\). It could take time to find. BTW, how to find?&lt;/li&gt;
  &lt;li&gt;Large weights are likely to dominate by exponential factor.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;rejection-sampling&quot;&gt;Rejection Sampling&lt;/h2&gt;

&lt;h3 id=&quot;basic-idea-1&quot;&gt;Basic Idea&lt;/h3&gt;
&lt;p&gt;Let’s use a one dimentional example. Sampling from \(P(x)\) is equavalent to uniformly sampling from the &lt;strong&gt;area&lt;/strong&gt; of its plot.&lt;/p&gt;

&lt;p&gt;We can define another distribution, \(Q(x)\), which hopefully approximates well \(P(x)\). Before sampling we must make sure that \(Q(x)\) is larger than \(P(x)\) up to a multiplicative factor. That is:&lt;/p&gt;

&lt;p&gt;\( P(x) &amp;lt; c \cdot Q(x) \) for all \(x\)&lt;/p&gt;

&lt;p&gt;We first sample from \(Q(x)\), then do a uniform sampling from \( [0, c \cdot Q(x)] \). If the value is less than \(P(x)\), we &lt;em&gt;accept&lt;/em&gt; the sample. Otherwise, we &lt;em&gt;reject&lt;/em&gt; it.&lt;/p&gt;

&lt;p&gt;In this way, we can make sure that we are sampling uniformly from the area of the plot.&lt;/p&gt;

&lt;p&gt;However, this requires that \(Q(x)\) approximates \(P(x)\) well, otherwise, c could be very large. Consequently, acceptance will be very rare.&lt;/p&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;
&lt;p&gt;For high-dimensional data, it is hard to find the value of c as we enumerating all values of \(P(x)\) is computationally expensive.&lt;/p&gt;

&lt;p&gt;Also, c could be very large(approximately 20,000) even for two normal distributions, \(P(x)\) and \(Q(x)\) where the std of \(Q(x)\) is only 1% larger than \(P(x)\).&lt;/p&gt;

&lt;p&gt;The acceptance ratio(volume under \(P(x)\) to volume under c \(Q(x)\) ) will be 1/20,000. And volume under \(P(x)\) and \(Q(x)\) are 1(normalized).&lt;/p&gt;

&lt;h2 id=&quot;metropolis-method&quot;&gt;Metropolis method&lt;/h2&gt;

&lt;h3 id=&quot;basic-idea-2&quot;&gt;Basic Idea&lt;/h3&gt;
&lt;p&gt;Essentially a random walk method.&lt;/p&gt;

&lt;p&gt;Acceptance:&lt;/p&gt;

&lt;p&gt;If the move is accepted, we are in a new state. If not, we have some chances(equal to the acceptance probability) of moving to the new state.&lt;/p&gt;

&lt;h3 id=&quot;lower-bound-for-iteration-number&quot;&gt;Lower Bound for Iteration Number&lt;/h3&gt;

&lt;p&gt;Approximately equal to  \( (L / \sigma)^{2} \)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(L\): largest length scale of the state space.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-question-and-answering&quot;&gt;Self Question and Answering&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;**What is the intuition of sampling in Markov way? **&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Random walk(current walk depends on the previous state/position) on the landscape of the target density function hoping to cover almost every where.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Why use such a formula to determine acceptance?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm#Formal_derivation_of_the_Metropolis-Hastings_algorithm&quot;&gt;This Wikipedia page&lt;/a&gt; explains it.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;**What is the form of \( Q(x^{‘}, x_i) \)? How could Q change based on the current state \( x_i \)? **&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remeber the \(Q\) function essentially implements the random walk mechanism. For example we can move left or right by 1 unit. This equals to \( Q(x_i, x^{‘}) = 1/2 \) if \( x_i = x^{‘} \pm 1 \).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;**How is dependent sampling turn into independent samples after certain amount of iterations? **&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Intuitively, it seems to be the case once you have being randomly walking for a long long time. Then the places you hace been to seems to be independent.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;**Why the name involves Metropolis? **&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My guess: let’s imaging someone wandering in a big city?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;How does the Rule of Thumb come about?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I can only figure it out for two dimensional space.&lt;/p&gt;

&lt;p&gt;Gibbs sampling will be posted next.&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/study-note/2014/09/03/mc-intro-1</link>
                <guid>http://simpleyyt.github.io/study-note/2014/09/03/mc-intro-1</guid>
                <pubDate>2014-09-03T18:41:00+02:00</pubDate>
        </item>

        <item>
                <title>Tour of scala: learning note -- Part One</title>
                <description>&lt;p&gt;I am studying Scala now by starting with &lt;a href=&quot;http://docs.scala-lang.org/tutorials/tour/tour-of-scala.html&quot;&gt;tour-of-scala&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is a summary of the first half of tour.&lt;/p&gt;

&lt;h2 id=&quot;abstract-types&quot;&gt;Abstract Types&lt;/h2&gt;

&lt;p&gt;Types can be class members. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  class MyClass{
  	    type T
    val elems &amp;lt;: List[T]
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where  &lt;code&gt;elems&lt;/code&gt; is subtype of &lt;code&gt;List[T]&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;annotations&quot;&gt;Annotations&lt;/h2&gt;

&lt;p&gt;Just like the annotaiton in Java:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;scala.serializable&lt;/li&gt;
  &lt;li&gt;scala.deprecated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Java annotations can be used in Scala.&lt;/p&gt;

&lt;p&gt;Parameters(even optional parameter) can be passed in. The syntax between Java and Scala is slightly different.&lt;/p&gt;

&lt;h2 id=&quot;classes&quot;&gt;Classes&lt;/h2&gt;

&lt;p&gt;Methods inheritted from ancestors need should be marked with &lt;code&gt;override&lt;/code&gt; if overriding the super one, for example, &lt;code&gt;toString&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;case-classes&quot;&gt;Case classes&lt;/h2&gt;

&lt;p&gt;Useful for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;automatic class attribute setting through class initialization&lt;/li&gt;
  &lt;li&gt;recursive decomposition mechanism via pattern matching.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;compound-types&quot;&gt;Compound types&lt;/h2&gt;

&lt;p&gt;Use the &lt;code&gt;with&lt;/code&gt; keyword to combine for example &lt;code&gt;trait&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;And it can be used to require the type of function parameter to have certain trais. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def cloneAndReset(obj: Cloneable with Resetable): Cloneable = {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sequence-comprehension&quot;&gt;Sequence Comprehension&lt;/h2&gt;

&lt;p&gt;Syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for(v &amp;lt;- expr [if condition])  {
      yield expr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Iteration can be nested.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for(v1 &amp;lt;- expr1 [if condition];
    v2 &amp;lt;- expr2 [if condition])  {
      yield expr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Unit&lt;/code&gt; can be returned by omitting the &lt;code&gt;yield&lt;/code&gt; keyword.&lt;/p&gt;

&lt;h2 id=&quot;extractor-objects&quot;&gt;Extractor Objects&lt;/h2&gt;

&lt;p&gt;Case classes provide built-in packing and unpacking. Without case clasess, these can also be done by using &lt;code&gt;apply&lt;/code&gt; and &lt;code&gt;unapply&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Function/method can return something or nothing by using &lt;code&gt;Option[T]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Some&lt;/code&gt; and &lt;code&gt;None&lt;/code&gt; are subtypes of &lt;code&gt;Option[T]&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;generic-classes&quot;&gt;Generic classes&lt;/h2&gt;

&lt;p&gt;Class like this:
       class Stack[T] {
              …
       }&lt;/p&gt;

&lt;p&gt;&lt;code&gt;T&lt;/code&gt; is a type parameter. It is invariant(as opposed to covariant/contravariant).&lt;/p&gt;

&lt;h2 id=&quot;implicit-parameters&quot;&gt;Implicit parameters&lt;/h2&gt;

&lt;p&gt;If not argument given, implicit parameter takes effect and only objects/methods that are marked implicit can be used to fill in the gap.&lt;/p&gt;

&lt;h2 id=&quot;inner-classes&quot;&gt;Inner classes&lt;/h2&gt;

&lt;p&gt;Similar to Java’s inner class, one class can be defined in another.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Graph {
	class Node {
	}
}

val g1 = new G
val g2 = new G

g1.Node == g2.Node //false!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to refer to the same &lt;code&gt;Node&lt;/code&gt;, use &lt;code&gt;Graph#Node&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mixin-class-composition&quot;&gt;Mixin Class Composition&lt;/h2&gt;
&lt;p&gt;How scala implements multiple inheritance —- through &lt;code&gt;trait&lt;/code&gt;s.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Iter extends StringIterator(args(0)) with RichIterator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can even define class like this? &lt;code&gt;StringIterator(args(0))&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;StringIterator&lt;/code&gt; is the super class&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;RichIterator&lt;/code&gt; is the mixin&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nested-functions&quot;&gt;Nested Functions&lt;/h2&gt;

&lt;p&gt;Function definition can be nested. The inner function forms a closure.&lt;/p&gt;

&lt;h2 id=&quot;anonymous-function-syntax&quot;&gt;Anonymous Function Syntax&lt;/h2&gt;

&lt;p&gt;(x: Int) =&amp;gt; x + 1&lt;/p&gt;

&lt;p&gt;is equal to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new Function1[Int, Int] {
	def apply(x: Int): Int = x + 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: function in Scala is also object&lt;/p&gt;

&lt;h2 id=&quot;currying&quot;&gt;Currying&lt;/h2&gt;

&lt;p&gt;To use currying, define function like this:&lt;/p&gt;

&lt;p&gt;def func(arg1)(arg2)..(argn) = {…}&lt;/p&gt;

&lt;h2 id=&quot;automatic-type-dependent-closure-construction&quot;&gt;Automatic Type-Dependent Closure Construction&lt;/h2&gt;

&lt;p&gt;This is just a more advanced name of &lt;em&gt;call-by-name&lt;/em&gt;. &lt;em&gt;call by name&lt;/em&gt;, in contrast with &lt;em&gt;call by value&lt;/em&gt;, evaluates the parameter when it is actually used in the function body.&lt;/p&gt;

&lt;p&gt;This feature can be used to create syntax like &lt;em&gt;while&lt;/em&gt;, &lt;em&gt;do-until&lt;/em&gt;.&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/note/2014/08/24/scala-note1</link>
                <guid>http://simpleyyt.github.io/note/2014/08/24/scala-note1</guid>
                <pubDate>2014-08-24T21:08:00+02:00</pubDate>
        </item>

        <item>
                <title>Why MRO in Python 2.2 is bad?</title>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I just read about how MRO in Python 2.3 is different from that of Python 2.2 &lt;a href=&quot;https://www.python.org/download/releases/2.3/mro/&quot;&gt;here&lt;/a&gt;. It is quite good. Clear, with a lot of illustrative examples.&lt;/p&gt;

&lt;p&gt;Here, I summarized why MRO in Python 2.2 is bad. For how MRO in Python 2.3 works, refer to &lt;a href=&quot;https://www.python.org/download/releases/2.3/mro/&quot;&gt;the article I just mentioned&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;C3-MRO(Method Resolution Order) is introduced in Python 2.3 to replace the old MRO in Python 2.2.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-wrong-with-mro-in-python-22&quot;&gt;What’s wrong with MRO in Python 2.2?&lt;/h2&gt;

&lt;p&gt;Let me use an example from &lt;a href=&quot;https://www.python.org/download/releases/2.3/mro/&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; F=type(&#39;Food&#39;,(),{&#39;remember2buy&#39;:&#39;spam&#39;})
&amp;gt;&amp;gt;&amp;gt; E=type(&#39;Eggs&#39;,(F,),{&#39;remember2buy&#39;:&#39;eggs&#39;})
&amp;gt;&amp;gt;&amp;gt; G=type(&#39;GoodFood&#39;,(F,E),{}) #OK in Python 2.2 but raises error in Python 2.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The inheritance hierarchy is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;             O
             |
(buy spam)   F
             | \
             | E   (buy eggs)
             | /
             G

      (buy eggs or spam ?)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In Python 2.2, the program runs while in Python 2.3, it does not.&lt;/p&gt;

&lt;h3 id=&quot;error-prone&quot;&gt;Error-prone&lt;/h3&gt;

&lt;p&gt;In Python 2.2, it produces “spam”. It is counter-intuitive(therefore error-prone), isn’t it? As &lt;code&gt;E&lt;/code&gt; is more specific than &lt;code&gt;F&lt;/code&gt;, thus the more specific class, &lt;code&gt;E&lt;/code&gt;, should be used.&lt;/p&gt;

&lt;h3 id=&quot;ambiguous&quot;&gt;Ambiguous&lt;/h3&gt;

&lt;p&gt;For the attribute &lt;code&gt;remember2buy&lt;/code&gt; of &lt;code&gt;G&lt;/code&gt;, which class shall be used? &lt;code&gt;F&lt;/code&gt; or &lt;code&gt;E&lt;/code&gt;? This is essentially the problem of ambiguity.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-python-23-does&quot;&gt;What Python 2.3 does&lt;/h2&gt;

&lt;p&gt;Python 2.3 wants to force programmer to create good class hierarchies. So it &lt;strong&gt;raises an error&lt;/strong&gt; if such case happens.&lt;/p&gt;

&lt;p&gt;For a detailed/general explanation of &lt;em&gt;what&lt;/em&gt; such cases are. Please refer to &lt;a href=&quot;https://www.python.org/download/releases/2.3/mro/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, for how to work out the MRO of a class by hand, refer to &lt;a href=&quot;https://www.python.org/download/releases/2.3/mro/&quot;&gt;the same article&lt;/a&gt;&lt;/p&gt;

</description>
                <link>http://simpleyyt.github.io/note/2014/08/07/python-mro</link>
                <guid>http://simpleyyt.github.io/note/2014/08/07/python-mro</guid>
                <pubDate>2014-08-07T22:36:00+02:00</pubDate>
        </item>

        <item>
                <title>Graph based keyword/sentence extraction</title>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The blog introduces how to perform keyword/sentence extraction using graph algorithm similar to Page-rank.&lt;/p&gt;

&lt;h2 id=&quot;key-idea&quot;&gt;Key idea&lt;/h2&gt;

&lt;p&gt;We can see text as a graph where the vertices can be phrase, sentence, even single word. Just like the way human sees text as a network of interconnecting concepts.&lt;/p&gt;

&lt;p&gt;To define the edges we need some kind of relation between the text units.&lt;/p&gt;

&lt;p&gt;Then we can run page-rank algorithm to rank the text units.&lt;/p&gt;

&lt;p&gt;If we define the unit to be phrases, the task is actually keyword extraction.&lt;/p&gt;

&lt;p&gt;This approach is proposed by &lt;a href=&quot;http://www.cse.unt.edu/~rada/papers/mihalcea.emnlp04.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;advantage&quot;&gt;Advantage&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;unsupervised&lt;/li&gt;
  &lt;li&gt;easy to implement&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;h3 id=&quot;keyword-extraction&quot;&gt;Keyword extraction&lt;/h3&gt;
&lt;p&gt;Individual words are considered vertices and co-occurrence is used as relate one word to another.&lt;/p&gt;

&lt;p&gt;Post processing: we can combine top ranked words into one, if they form phrase in the text.&lt;/p&gt;

&lt;h3 id=&quot;sentence-extraction&quot;&gt;Sentence extraction&lt;/h3&gt;
&lt;p&gt;The degree to which two sentences are connected is the ratio of words that they share.&lt;/p&gt;

&lt;h2 id=&quot;problem--improvementfor-keyword-extraction&quot;&gt;Problem &amp;amp; Improvement(for keyword extraction)&lt;/h2&gt;

&lt;h3 id=&quot;single-occurrence-word-issue&quot;&gt;Single-occurrence word issue:&lt;/h3&gt;

&lt;p&gt;Some keywords occurs only once and are not likely to be ranked high.&lt;/p&gt;

&lt;p&gt;Possible causes of the problem:&lt;/p&gt;

&lt;h4 id=&quot;synonym&quot;&gt;Synonym&lt;/h4&gt;

&lt;p&gt;The keyword is just another (unusual) form of saying the same thing, for which is already mentioned in the text several times.&lt;/p&gt;

&lt;p&gt;For example, &lt;em&gt;North Atlantic Treaty Organization&lt;/em&gt; is mentioned only once. And later, it is abbreviated as &lt;em&gt;NATO&lt;/em&gt; many times.&lt;/p&gt;

&lt;p&gt;Possible solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic analysis: mapping phrases, like NATO, to Wikipedia concepts, like it full name.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;

&lt;p&gt;For example, Uncle Sam is mentioned once at the beginning. Later, it is referred as &lt;em&gt;he&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Possible solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reference resolution technique: mapping &lt;em&gt;he&lt;/em&gt; to &lt;em&gt;Uncle Sam&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/paper-reading/2014/08/07/text-rank</link>
                <guid>http://simpleyyt.github.io/paper-reading/2014/08/07/text-rank</guid>
                <pubDate>2014-08-07T16:00:00+02:00</pubDate>
        </item>

        <item>
                <title>Announcement: Maui 2.0, faster keyword extraction tool based on Wikipedia data</title>
                <description>&lt;h2 id=&quot;announcement&quot;&gt;Announcement&lt;/h2&gt;

&lt;p&gt;Maui 2.0, a faster keyword extraction tool compared to(and also based on) &lt;a href=&quot;https://code.google.com/p/maui-indexer/&quot;&gt;Maui&lt;/a&gt;, is published under MIT license at &lt;a href=&quot;https://github.com/xiaohan2012/maui-2&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Currently, the build script for Maui 2.0 is not present. Feel free to contribute to the repository.&lt;/p&gt;

&lt;h2 id=&quot;highlights&quot;&gt;Highlights&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Wikipedia-based keyword extraction that takes English text as input and produces a list of Wikipedia concepts(article name) as keywords, which make sense and can be easily interpreted.&lt;/li&gt;
  &lt;li&gt;10 times faster than the &lt;a href=&quot;https://code.google.com/p/maui-indexer/&quot;&gt;original maui&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Java API that can be used by Java or Scala program.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://github.com/xiaohan2012/maui-2&quot;&gt;the Github repository&lt;/a&gt;&lt;/p&gt;

</description>
                <link>http://simpleyyt.github.io/software/2014/08/03/maui-2</link>
                <guid>http://simpleyyt.github.io/software/2014/08/03/maui-2</guid>
                <pubDate>2014-08-03T22:45:00+02:00</pubDate>
        </item>

        <item>
                <title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis</title>
                <description>&lt;h2 id=&quot;problem-to-solve&quot;&gt;Problem to solve&lt;/h2&gt;
&lt;p&gt;Using information from Wikipedia to compute the semantic relatedness between:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Word and word&lt;/li&gt;
  &lt;li&gt;Text and text&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;highlight-of-the-paper&quot;&gt;Highlight of the paper&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;It exceeds state-of-art performance prior to its time.&lt;/li&gt;
  &lt;li&gt;It is &lt;em&gt;explicit&lt;/em&gt;, rather than latent in the way that human can  the &lt;em&gt;underlying&lt;/em&gt; representation of word and text.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;how-to-solve&quot;&gt;How to solve&lt;/h2&gt;

&lt;p&gt;The method is termed as Explicit Semantic Analysis due to the use of &lt;em&gt;article headline&lt;/em&gt;, or &lt;em&gt;concept&lt;/em&gt; in Wikipedia to represent words and text. In short, word is represented by a weighted vector of Wikipedia concepts and text as a weighted vector summation(vector weighted by word TD-IDF value) of all the  word vectors derived from the text.&lt;/p&gt;

&lt;p&gt;Word-word and text-text similarity can be calculated using any distance metric. Cosine distance metric is preferred as it calculates the angle similarity, which is preferable for text of diverging length.&lt;/p&gt;

&lt;h3 id=&quot;word-as-concept-vector&quot;&gt;Word as concept vector&lt;/h3&gt;

&lt;p&gt;Concept vector for each word can be calculated using TF-IDF schema. The model is essentially the same as the document-word TD-IDF calculation. Each Wikipedia concept/article has a unique page with words in it. Thus, each concept can be represented using word vector.&lt;/p&gt;

&lt;h3 id=&quot;text-as-concept-vector&quot;&gt;Text as concept vector&lt;/h3&gt;

&lt;p&gt;Text consists of words, each of which has a concept vector. Thus, the concept vector is simply the vector summation of the vectors of its containing words.&lt;/p&gt;

&lt;h2 id=&quot;practical-consideration&quot;&gt;Practical consideration&lt;/h2&gt;
&lt;p&gt;When deriving the concept vector for words, the following preprocessing is applied:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;word stemming, stop-word/rare-word removal is applied&lt;/li&gt;
  &lt;li&gt;overly specific concept/article(too few words, too few incoming/outgoing links) is removed from consideration.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;why-this-approachadvantage&quot;&gt;Why this approach/advantage&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;In contrast to Latent Semantic Analysis, it is &lt;em&gt;explicit&lt;/em&gt; for words/text are represented by &lt;strong&gt;explicit&lt;/strong&gt; Wikipedia concepts. Thus it is easier to interpret.&lt;/li&gt;
  &lt;li&gt;According to observation, word sense disambiguation can be accomplished implicitly. Given the phrase, &lt;em&gt;Bank of America&lt;/em&gt; and &lt;em&gt;Bank of Amazon&lt;/em&gt;, in which &lt;em&gt;bank&lt;/em&gt; are actually two totally different concepts, &lt;em&gt;bank&lt;/em&gt; can be disambiguated. Thanks to the context information, a voting scheme involving all the words in the phrase manages to disambiguation the pivotal word, &lt;em&gt;bank&lt;/em&gt; in the case.&lt;/li&gt;
  &lt;li&gt;It uses information encoded in Wikipedia, which is vast and rich in knowledge.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;roaming-thinking-word-sense-disambiguation-by-vector-projection&quot;&gt;Roaming thinking-word sense disambiguation by vector projection&lt;/h2&gt;

&lt;p&gt;In this paper, word sense disambiguation is achieved by &lt;em&gt;summing up&lt;/em&gt; the vectors from contextual words, which resembles the scenario of voting. In this scenario, a list of words, each with its associated concepts, votes on the concepts and finally come up with a ranked list of concepts in terms of their association.&lt;/p&gt;

&lt;p&gt;However, since we can think of a word as a vector in the high dimensional space of concepts, can we &lt;strong&gt;project&lt;/strong&gt; a word into the context so that irrelevant concepts can be denoised?&lt;/p&gt;

&lt;p&gt;In addition, the drawbacks of the word sense disambiguation approach in this paper is, it considers also the concepts in other words so that concepts from there might over-emphasize the intended concepts. For example, for the phrase, &lt;em&gt;Bank of China&lt;/em&gt;, if we want to disambiguate &lt;em&gt;bank&lt;/em&gt; using the summation approach, &lt;em&gt;China&lt;/em&gt; might pop up as the first relevant concept.&lt;/p&gt;
</description>
                <link>http://simpleyyt.github.io/paper-reading/2014/08/03/wikipedia-based-semantic-relatedness</link>
                <guid>http://simpleyyt.github.io/paper-reading/2014/08/03/wikipedia-based-semantic-relatedness</guid>
                <pubDate>2014-08-03T19:20:00+02:00</pubDate>
        </item>

        <item>
                <title>Welcome!</title>
                <description>&lt;p&gt;2014-08-01 is the birthday of this blog. Cheers!&lt;/p&gt;

&lt;p&gt;Fellow. Feel free to express any ideas!&lt;/p&gt;

</description>
                <link>http://simpleyyt.github.io/misc/2014/08/01/welcome!</link>
                <guid>http://simpleyyt.github.io/misc/2014/08/01/welcome!</guid>
                <pubDate>2014-08-01T18:01:41+02:00</pubDate>
        </item>

        <item>
                <title>Things learned from setting up Wikipedia miner</title>
                <description>&lt;p&gt;I was recently using &lt;a href=&quot;https://github.com/dnmilne/wikipediaminer/wiki/Downloads&quot;&gt;Wikipedia miner&lt;/a&gt; [dump extraction module][wm-extraction]. This module requires &lt;code&gt;Hadoop 0.20.2&lt;/code&gt; to run.&lt;/p&gt;

&lt;p&gt;I initially set out with setting up &lt;code&gt;Hadoop 2.4.0&lt;/code&gt;, which became a painful process for me. Becauase:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I almost knows nothing about Hadoop.&lt;/li&gt;
  &lt;li&gt;The dump extraction is based on &lt;code&gt;Hadoop 0.20.2&lt;/code&gt;, which is different from &lt;code&gt;Hadoop 2.4.0&lt;/code&gt;. According to Apache Hadoop guy, the newer one &lt;a href=&quot;https://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YARN.html&quot;&gt;has undergone a complete overhaul&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason to use 2.4.0 is simple. It is the newest and using it is fashionable.&lt;/p&gt;

&lt;p&gt;After playing with &lt;code&gt;Hadoop 2.4.0&lt;/code&gt; and trying to make it work for Wikipedia Miner for 3 days, I decided to give up and switch to what Wikipedia miner is built upon, &lt;code&gt;Hadoop 0.20.2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally, the beautiful console information pops up. &lt;code&gt;Hadoop 0.20.2&lt;/code&gt; rules!&lt;/p&gt;

&lt;p&gt;The lesson I have learned is: try to use the appropriate tool, not the coolest!&lt;/p&gt;

</description>
                <link>http://simpleyyt.github.io/experience/2014/07/26/things-learned-from-setting-up-wikipedia-miner</link>
                <guid>http://simpleyyt.github.io/experience/2014/07/26/things-learned-from-setting-up-wikipedia-miner</guid>
                <pubDate>2014-07-26T18:01:41+02:00</pubDate>
        </item>

        <item>
                <title>How Maui works with Wikipedia?</title>
                <description>&lt;p&gt;&lt;a href=&quot;https://code.google.com/p/maui-indexer/&quot; title=&quot;Maui host at google code&quot;&gt;Maui&lt;/a&gt; is a key phrase indexing tool. One of capability is assigning key terms to documents. One option is let the task base on the knowledge of Wikipedia. This article talks about how Maui actually does this by use of Wikipedia.&lt;/p&gt;

&lt;p&gt;Three steps, &lt;em&gt;Candidate generation&lt;/em&gt;, &lt;em&gt;disambiguating&lt;/em&gt; and &lt;em&gt;filter&lt;/em&gt;, are performed.&lt;/p&gt;

&lt;h2 id=&quot;candidate-generation&quot;&gt;Candidate generation&lt;/h2&gt;

&lt;p&gt;Using keyphraseness(#links in all Wikipedia pages/ #occurrences in all Wikipedia pages) and a threshold value to be compared against to identifying important words and phrases&lt;/p&gt;

&lt;h2 id=&quot;disambiguation&quot;&gt;Disambiguation&lt;/h2&gt;

&lt;p&gt;Disambiguating the phrases is necessary due to the ambiguous nature of natural language. So this step maps each phrase to one Wikipedia article, which represents one sense/concept.&lt;/p&gt;

&lt;p&gt;First case-fold and stem all the phrases. If they happen to be the name of an article, use it as the sense. If the phase leads to redirect page. Then use the target article as the sense.&lt;/p&gt;

&lt;p&gt;Interesting happens when it leads to disambiguation pages, the intended sense needs to be determined.&lt;/p&gt;

&lt;p&gt;Commonness and semantic relatedness is used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Commonness&lt;/em&gt;: To what extent the sense T is well known as the phrase S&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Semantic relatedness&lt;/em&gt; in terms of the context: context refers to the set of senses unambiguous and relatedness computation of two senses is based on the number of shared in-going links to the two sense.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;filtering&quot;&gt;Filtering&lt;/h2&gt;

&lt;p&gt;Classifying the candidate article names as being key phrase or not&lt;/p&gt;

&lt;p&gt;Using several features to classify each article name individually.&lt;/p&gt;

&lt;p&gt;Features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TD-IDF based&lt;/li&gt;
  &lt;li&gt;first/last occurences.&lt;/li&gt;
  &lt;li&gt;spreadness: last occur position - first occur (How about some measurement similar to Kurtosis)&lt;/li&gt;
  &lt;li&gt;total keyphrases of article name &lt;code&gt;A&lt;/code&gt; in document &lt;code&gt;d&lt;/code&gt;: sum of keyphrasesness of the phrase &lt;code&gt;p&lt;/code&gt; that maps to &lt;code&gt;A&lt;/code&gt; times the frequency of &lt;code&gt;p&lt;/code&gt; appearing in &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;semantic relatedness:&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/paper-reading/2014/07/25/how-maui-works-with-wikipedia</link>
                <guid>http://simpleyyt.github.io/paper-reading/2014/07/25/how-maui-works-with-wikipedia</guid>
                <pubDate>2014-07-25T18:01:41+02:00</pubDate>
        </item>

        <item>
                <title>How KEA works?</title>
                <description>&lt;p&gt;Paper link is &lt;a href=&quot;http://arxiv.org/ftp/cs/papers/9902/9902007.pdf&quot; title=&quot;Paper here&quot;&gt;here&lt;/a&gt;. Published in 1999, with software written and open-sourced.&lt;/p&gt;

&lt;p&gt;Two major steps are involved: &lt;em&gt;identify candidate phrases&lt;/em&gt; and &lt;em&gt;classifying the candidates&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;identify-candidate-phrases&quot;&gt;Identify candidate phrases&lt;/h2&gt;

&lt;h3 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h3&gt;

&lt;p&gt;Cleaning, replacing punctuation, etc with phrase boundary, resulting a list of sequences of words&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;p&gt;Before cleaning&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;authors are unlikely to use—for example, gauge, smooth, and especially garbage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After cleaning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;authors are unlikely to use&lt;/li&gt;
  &lt;li&gt;for example&lt;/li&gt;
  &lt;li&gt;gauge&lt;/li&gt;
  &lt;li&gt;smooth&lt;/li&gt;
  &lt;li&gt;and especially garbage&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phrase-identification&quot;&gt;Phrase identification&lt;/h3&gt;

&lt;p&gt;Using stop words to separate the words sequences into phrases&lt;/p&gt;

&lt;p&gt;Given stop words to be &lt;code&gt;are&lt;/code&gt;, &lt;code&gt;to&lt;/code&gt;, &lt;code&gt;and&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;will result in:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;authors&lt;/code&gt;, &lt;code&gt;unlikely to use&lt;/code&gt;, &lt;code&gt;for example&lt;/code&gt;, &lt;code&gt;gauge&lt;/code&gt;, &lt;code&gt;smooth&lt;/code&gt;, &lt;code&gt;especially garbag&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stemming-and-case-folding&quot;&gt;Stemming and case-folding&lt;/h3&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;authors -&amp;gt; author
Gauge -&amp;gt; gauge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;classifiy-the-candidate-phrases-as-key-or-not&quot;&gt;Classifiy the candidate phrases as key or not&lt;/h2&gt;

&lt;p&gt;A Naive Bayes classifier is used to classify the candidate  phrases as key phrase or not.&lt;/p&gt;

&lt;p&gt;Two features are used in the model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Tf-idf of the phrase&lt;/em&gt;: the document frequency is calcualted using a global corpus&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;First occurence of the phrase&lt;/em&gt;: number of words that occure before the first occurence of the phrase / number of words in the document&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, a Naive Bayes requires discrete feature values, discretization is used. NOTE: supervised discretization method is used here.&lt;/p&gt;

&lt;h3 id=&quot;traning-of-the-model&quot;&gt;Traning of the model&lt;/h3&gt;

&lt;p&gt;Given a set of documents with key phrases annotated as the training set, the candidate key phrases are extracted and marked as 1, if it is a key phrase and 0, otherwise.&lt;/p&gt;

&lt;p&gt;Features are calcualted and discretized and used for training.&lt;/p&gt;

&lt;h3 id=&quot;prediction&quot;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;The Naive Bayes prediction sutff.&lt;/p&gt;

&lt;h2 id=&quot;my-thought&quot;&gt;My thought&lt;/h2&gt;

&lt;h3 id=&quot;highlight-of-the-paper&quot;&gt;Highlight of the paper:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Ways to identify candidate key phrases can even be applied to Chinese.&lt;/li&gt;
  &lt;li&gt;Really simple to understand&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;some-possible-improvement&quot;&gt;Some possible improvement:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;more features can be added&lt;/li&gt;
  &lt;li&gt;key phrases not existing but convey the key message of the document can not be extracted. So how to solve it?&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://simpleyyt.github.io/paper-reading/2014/07/21/how-kea-works</link>
                <guid>http://simpleyyt.github.io/paper-reading/2014/07/21/how-kea-works</guid>
                <pubDate>2014-07-21T18:01:41+02:00</pubDate>
        </item>


</channel>
</rss>
