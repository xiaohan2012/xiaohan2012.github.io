Principle of Maximum Entropy:
=============

Simply put, being ignorant is better than knowing what something that is wrong.

More specifically, given a set of facts(produced by some random process), we should build a model(of the random process) that is consistent with all the facts and as uniform as possible for the unknown.

And *entropy* defines *uniformness*.

Entropy:
=========

To quantify the degree of uncertainty of a probability distribution. In other words, the expected uncertainty/surprise over a distribution.

Uncertainty/surprise is defined as log(1/p_x).

The expected value of uncertainty is:

- \sum\limits_{x} p_x log(p_x)

## More than maximizing entropy

In real cases, we are contrained by the facts that we observe. Thus, the problem is usually defind as finding the distribution of the model that:

1. obeys the contraints imposed by facts(e.g, empirical observations)
2. maximizes the entropy(so that as uniform/ignorant as possible)


## Property of the problem


1. the function we want to maximize(with contraints) is convex, thus globally unique best value exists

## Solution of the problem(Traning)



## Application


##Sequence model(POS, NER, word segmentation)

Inference:
- Inexact: beam inference
- Exact: Viterbi inference