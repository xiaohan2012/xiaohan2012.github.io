---
layout: post
title:  "ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering"
date:   2015-09-02 18:49:00
categories: paper
tags: dm nlp entity-typing graph optimization
---

## What it solves

Given a knowledge base, document corpus and a set of target types, T, the method:

- Detects candidate entity mentions
- Types the mentions to *one* of T or Not-Of-Interest 

As a side-product, it also:

- Detects relation phrases
- Clusters the phrases

## Basic workflow

It works as follow:

1. Mining possible entity mentions
2. Construct heterogenous graph on entity mentions, mention names and relation phrases
3. Based on confidently mapped entity mentions, perform label propagation and relation phrase clustering simultaneously by modeling it a mix integer optimization problem

Three challenges it tries to address:

- **Domain restriction**: if using chunkers to detect entity mention, it might perform badly in domain-specific corpus. This papers makes little linguistic assumptions and mine the phrases in a data-driven way.
- **Name ambiguity**: some approaches treat mentions with the same surface names as one entity. This is probelmatic. This paper predicts each mention's type individually
- **Context sparsity**: some relation phrase occurs rarely, making it hard to provide useful/reliable information for its argument. This paper clusters the phrases so that more information can be propagated/shared

## Steps In detail

### Entity mention and relation phrase detection

1. Mine *frequent continuous patterns* up to a maximum length
2. Merge the patterns agglomeratively and greedily. At each step, the decision is made by choosing the one with the highest *global significance score*, kind of *rectified phrase frequency*. Meanwhile, the merged phrases should obey certain POS tag patterns. *NN+* for mentions
3. Merging stops when the score is below some threshold.

This step can be replaced by the method in "SegPhrase" paper.

### Heterogenous graph construction

Three types of nodes:

- Entity mention
- Mention surface name(number far fewer than number of entity mentions)
- Relation phrase

Mention surface name is included to:

- reduce time complexity / sparsity
- (indirectly) allow more efficient information propagation

Three types of edges/subgraphs:

- mention-name: type info on name directly gives cues on types of mentions
  For each mention or name, its type is represented by a one-hot vector

- name-phrase: if we know "XX" is "Food" and "serve XX", then if we see "serve YY", then "YY" might be "Food". "serve" serves like a bridge. **Hypothesis 1**  
  Type for phrase is represented by two(left/right arg) one-hot vector
  
- mention-mention: if two mentions are *strongly correlated*, they tend to share similar type. **Hypothesis 2**
  
### *ClusType* algorithm

Two more hypo:
*Hypothesis 3*: if two relation phrases are in the same cluster, they tend have similar types
*Hypothesis 4*: if two relation phrases tend to 1, words, 2, context words, 3, similar types signatures, they tend to be in similar cluster

And these hypothesis are encoded in an optimization problem, 3 terms:

1. Minimize the distance between mention types and corresponding phrase type signature(**Hypothesis 1**)
2. Modeled as clustering as an non-negative matrix factorization problem plus some consensus matrix(because of multiple views). **Hypothesis 3 and 4**
3. Miminize distance between mention type and aggregation(e.g, *sum*) from corresponding name type and relation phrase type. Plus, strongly correlated mentions should have similar types(**Hypothesis 2**). Last, should be consistent with types of initial seed.

**Optimization:**

Mix integer problem. NP-hard. Using relaxation and alternating algorithm(Updating subsets of parameters step by step) as approximation.

Don't quite understand.

## What I learned

- A cool thing: entity typing
- Limitations of NER: domain restrictness(human effort)
- Limitations of entity linking: limited coverage/freshness(entity may not be in KB)
- The three challenges: entity detection, name ambiguity and context sparsity
- The idea of mutually enhancing is also applied in the *SegPhrase* paper, in which segmentation and quality estimation enhances each other
- Another way to do phrase mining: agglomeratively merging patterns under POS pattern constraint. The decision is based on some kind of *rectified frequency*, which also appeared in *SegPhrase* paper
- Multiplication of N co-occurence matrices(of N+1 types of objects) lead to then co-occurence matrix between 1st type of obj and (N+1)th type of obj
- I might need to know more about discrete optimization / integer programming
- `splotlight.dbpedia.org` can be used for entity linking
- Very stunnhing entity coverage (`<7%`) by some entity linking method
- Non-negtive matrix factorization(NMF), `min_{U,V}||F - UV||`
- Clustering as an NMF problem
- Relaxation and alternating minimization as approximation techniqu for mix-integer programming
- Two broad types of efforts for entity typing: weak-supervision(bootstrapping)and distant supervision


## My questions and thinking

1. Phrase have left/right argument: is it reasonable to distinguish the arguments by left/right? How about three or more arguments?
2. More cues for surface name, besides the relation or mention? How about from knowledge base, even though the name is ambigous
3. Vector representation for surface name can be an over-simplification, as name can be ambiguous. Can we apply some (non)-linear transformation function that transforms the type by context(also some vector)?
4. Can we also use entity mention clustering to further reduce the sparsity?
5. Should we normalize the relation phrases(passive/active voice)?
6. Representation for the phrases is sparse for now(BoW). Any stuff like *phrase2vec* like *word2vec*?
7. Can we **link** the mentions to KB also? For example, KB contain the  mentioned entity but doesn't record its surface name.
8. Can we model **multiple** types? For now, only one type is assigned.
9. Is the POS tagger sensitive to domain? If so, the entity mention detection might be problematic? Also, for Twitter, specific taggers for Twitter might be used.
10. 



## Related interesting topics

1. Online learning/prediction(forever learner)?
2. Scalability: can the integer programming algorithm be applied to large scale entity typing


## Related papers and comparison

### Entity typing / distance supervision

- T. Lin, O. Etzioni, et al. No noun phrase left behind: detecting and typing unlinkable entities. In EMNLP, 2012.
- N. Nakashole, T. Tylenda, and G. Weikum. Fine-grained semantic typing of emerging entities. In ACL, 2013.

### Entity linking

- W. Shen, J. Wang, and J. Han. Entity linking with a knowledge base: Issues, techniques, and solutions. TKDE, (99):1-20, 2014.

### Label propagation

- W. Shen, J. Wang, P. Luo, and M. Wang. A graph-based approach for ontology population with named entities. In CIKM, 2012.
- P. P. Talukdar and F. Pereira. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In ACL, 2010.

### Relation extraction

- A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP, 2011.

### NMF&Clustering

- J. Liu, C. Wang, J. Gao, and J. Han. Multi-view clustering via joint nonnegative matrix factorization. In SDM, 2013.

### Dimension reduction

- X. He and P. Niyogi. Locality preserving projections. In NIPS, 2004.

### Discrete optimization 

- P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. JOTA, 109(3):475-494,2001.