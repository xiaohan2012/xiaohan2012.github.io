---
layout: post
title: "Study Note: Introduction to Monte Carlo Method"
date:   2014-09-03 16:41:00
categories: study-note
tags: monte-carlo importance-sampling rejection-sampling metropolis-method
---

This is the study note of this [Introduction to Monte Carlo Methods](http://leonidzhukov.net/hse/2013/stochmod/papers/intro_to_mcmc_mackay.pdf)

## Introduction

Problem we are facing:

1. To generate samples from \\(P(x)\\).
2. To estimate the expectation under the distribution.


If we know 1, the 2 can be solved by sampling.

Assume \\(P(x)\\) can be *evaluated* to be multiplicative constant, that is

 $$ P(x) = P^{*}(x) / Z $$

Where \\(Z\\) is the normalising constant.

**My questions**:

- why called **evaluated** and how is it done?
- why it is a multiplicative constant, not additive constant?

But sampling from \\(P(x)\\) is still difficult because:

1. \\(Z\\) is unknown.
2. Even if \\(Z\\) is known, sampling(especially from high-dimensional space) is challenging for computers(in the numerical sense), because knowing how to **plot** the density function does not necessarily mean it is easy to **sample** from it.

Let's assume we have the explicit form of the distirbution we want to sample from.  We can discretize the x axis and make the plot into a list of bins. Then use uniform distribution to sample based on the height of the bins. 

The essay's idea is that given \\( P^{*}(x) \\), in order to get \\(P(X)\\), we need to calculate \\(Z\\). Given high data dimension, the state space is large. 

**My questions**:

- Can I say discretization is the general approach to sample numerically? 
- \\(Z\\) seems to be an important element, but why do we introduce it? Again, how \\( P^{*}(x) \\) is retrieved.

## Uniform Sampling

We can use sampling to estimate the expectation. But given the existence of typical set especially for high dimensional data. It is often fairly possible not to hit any point in the typical set at all when doing the sampling!

So we need to use different sampling methods.

## Importance Sampling

This is used to solve the expectation estimation problem.

### Basic Idea

Given that we know the form of the density function, \\(P(x)\\), which could be hard to sample from, we use a simpler density function to approximate the \\(P(x)\\). And we call it sampler density and denote it as \\(Q(x)\\), which could be a Gaussian or Cauchy distribution.

As their might be inaccuracy when sampling from \\(Q(x)\\) compared to the original \\(P(x)\\), we introduce a weight, \\( P(x\_i) / Q(x\_i) \\) to modify the effect of the sampling points.

For example, if \\( Q(x\_i) > P(x\_i) \\), \\( x\_i \\) is over-presented, a weight, \\( P(x\_i) / Q(x\_i) \\) below 1, is multiplied to the value \\( \phi\_{x\_i} \\). The idea goes vice versa.

And as the sampling goes on, it is proved to converge.

### Practical Problem

There might be region where \\(Q(x)\\) is small while \\(P(x)\\) are large and \\( \phi(x)P(x) \\) are large. Then x might had little chance falling into that region. And the big contribution from that region is absent

In one example provided by the author, Cauchy sampler is used and converges only after 5000 samplings, while Gaussian sampler does not converge even after ten thousand samplings. 

In contrast, Cauchy sampler has heavier tails than Gaussian sampler.

### Multiple Dimension

Assume a uniform distribution inside a sphere, the proposal density is multi dimensional Guassian.

It can be shown that the weights of several sample points will dominate the others. For \\(N=1000\\), the largest weight will be \\( 10^{19} \\) larger than the median weight.

Problem with importance sampling:

1. \\(Q\\) should be a good approximation of \\(P\\) so that \\(Q\\) lies in the typical set of \\(P\\). It could take time to find. BTW, how to find?
2. Large weights are likely to dominate by exponential factor.


## Rejection Sampling

### Basic Idea
Let's use a one dimentional example. Sampling from \\(P(x)\\) is equavalent to uniformly sampling from the **area** of its plot.

We can define another distribution, \\(Q(x)\\), which hopefully approximates well \\(P(x)\\). Before sampling we must make sure that \\(Q(x)\\) is larger than \\(P(x)\\) up to a multiplicative factor. That is:

   \\( P(x) < c \cdot Q(x) \\) for all \\(x\\)

We first sample from \\(Q(x)\\), then do a uniform sampling from \\( [0, c \cdot Q(x)] \\). If the value is less than \\(P(x)\\), we *accept* the sample. Otherwise, we *reject* it.

In this way, we can make sure that we are sampling uniformly from the area of the plot.

However, this requires that \\(Q(x)\\) approximates \\(P(x)\\) well, otherwise, c could be very large. Consequently, acceptance will be very rare.

### Problem
For high-dimensional data, it is hard to find the value of c as we enumerating all values of \\(P(x)\\) is computationally expensive.

Also, c could be very large(approximately 20,000) even for two normal distributions, \\(P(x)\\) and \\(Q(x)\\) where the std of \\(Q(x)\\) is only 1% larger than \\(P(x)\\).

The acceptance ratio(volume under \\(P(x)\\) to volume under c \\(Q(x)\\) ) will be 1/20,000. And volume under \\(P(x)\\) and \\(Q(x)\\) are 1(normalized).



## Metropolis method

### Basic Idea
Essentially a random walk method.

Acceptance:

If the move is accepted, we are in a new state. If not, we have some chances(equal to the acceptance probability) of moving to the new state.

###  Lower Bound for Iteration Number

Approximately equal to  \\( (L / \sigma)^{2} \\)

- \\(L\\): largest length scale of the state space.

### Self Question and Answering

1. **What is the intuition of sampling in Markov way? **

  Random walk(current walk depends on the previous state/position) on the landscape of the target density function hoping to cover almost every where.

2. **Why use such a formula to determine acceptance?**

  [This Wikipedia page](http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm#Formal_derivation_of_the_Metropolis-Hastings_algorithm) explains it.

3. **What is the form of \\( Q(x^{'}, x\_i) \\)? How could Q change based on the current state \\( x\_i \\)? **

  Remeber the \\(Q\\) function essentially implements the random walk mechanism. For example we can move left or right by 1 unit. This equals to \\( Q(x\_i, x^{'}) = 1/2 \\) if \\( x\_i = x^{'} \pm 1 \\).

4. **How is dependent sampling turn into independent samples after certain amount of iterations? **

  Intuitively, it seems to be the case once you have being randomly walking for a long long time. Then the places you hace been to seems to be independent.

5. **Why the name involves Metropolis? **

  My guess: let's imaging someone wandering in a big city?

6. **How does the Rule of Thumb come about?**

  I can only figure it out for two dimensional space.

Gibbs sampling will be posted next.