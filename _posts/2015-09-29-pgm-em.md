---
layout: post
title: "PGM: The Expectation-Maximization Algorithm "
date: 2015-09-29 23:33:45
categories: course
tags: pgm latent-variable em gmm hmm baum-welch
---
We are discussing EM for only **BN** here.

## Gaussian Mixture Model

![](/assets/images/pgm/gmm.png)

Task is to learn: \\(pi_k, \mu_k, \sigma_k\\)

### Difficulty in parameter estimation

Decomposability(local estimation) in fully observed GMM:

![](/assets/images/pgm/em_mixture_model_for_fully_observed_data.png)

In partially observed GMM, parameters are coupled together(sum within logarithm).

![](/assets/images/pgm/em_mixture_model_for_partially_observed_data.png)


### Expectation Maximization

In EM, label assignment of each data is fractional(probability).

- **Expectation step**: calculate the expected value of hidden variables given the current estimation of parameters and data(\\(p(z \vert D, \theta)\\), posterior inference problem)
- **Maximization step**: compute the parameters by maximizing \\(\mathcal{l}\\) under the expected value of hidden variables(essentially, MLE for fully observed BN)

Related model is K-means. Compared to K-means, label assignment is "hard"(or MAP) in K-means while it's "soft" in EM.


### E step for GLIM

![](/assets/images/pgm/expected_complete_log_likelihood_for_glim.png)

### Theory: why EM works?

Why EM works? Essentially, we want to show the marginal likelihood is increasing as EM runs.

Definition of expected complete log likelihood:

![](/assets/images/pgm/expected_complete_log_likelihood.png)

Essentially the expectation of complete log likelihood over the distribution of hidden variables.

- **Q**: Why do we use the *expected complete log likelihood*?
  1. For notational convenience in deriving the lower bound of \\(\mathcal{l}\\).
  2. Have the picture of **what to infer about** in the learning process

For E step:

Jensen's equality leads to lower bound for \\(\mathcal{l}\\).

$$
\begin{eqnarray*}
\mathcal{l}(\theta; \mathbf{x})
&=& \log p(\mathbf{x} \vert \theta) \\
&=& \log \sum\limits_z p(\mathbf{x}, z \vert \theta) \\
&=& \log \sum\limits_z q(z\vert x) \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vert x)} \\
&\ge& \sum\limits_z  q(z\vert x) \log \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vert x)}
&=& <\mathcal{l}(\theta, x, z)>_q + H(q)
\end{eqnarray*}
$$

Running EM is coordinate-ascent where both E closes the gap between \\(F(\theta, x)\\) and loss function, while M step maximizes the lower bound of \\(\mathcal{l}(\theta, x)\\).

![](/assets/images/pgm/em_in_terms_of_lower_bound.png)

The lower bound is coined *free energy function*, \\(F(q, \theta)\\).

Or in the information theoretic view:

$$ \mathcal{l}(\theta, x) - F(q, \theta) = KL(q \vert \vert p(z \vert x, \theta))$$

By pushing up the lower bound, we wish to increase \\(\mathcal{l}(\theta, x)\\).

For M step, setting \\(q(z \vert x)\\) to \\(p(z \vert x, \theta)\\) leads to the \\(F(q, \theta) = \mathcal{l}(\theta, x)\\), which indicates \\(F(\theta, x)\\) is maximized.

For M step, given the optimal \\(q\\) in previous step(which can be treated as known) and the entropy term does not depend on \\(\theta\\), it reduces to MLE of fully observed model.

![](/assets/images/pgm/m_step_for_lower_bound.png)

Thus, we are pushing up the lower bounds until convergence.

**Q**: pushing up upper bound only means the minimum value is increasing. Does it necessarily mean the actual value is increasing as well?

## HMM(Baum Welch Algorithm)

The labels are not observed(POS tags, NER labels, etc).

Complete log likelihood:

![](/assets/images/pgm/hmm_complete_log_likelihood.PNG)

Expected complete log likelihood:

![](/assets/images/pgm/hmm_expected_complete_log_likelihood.png)

Note how the above formula is constructed: for each type of parameter, we make its likelihood expectation over all the hidden variables that are involved in the parameter. In this way, we know that we want to perform inference on:

- \\(p(y_{t} \vert \mathbf{x})\\)(actually, once we know this, the next term is easy to compute)
- \\(p(y_{t-1}, y_{t} \vert \mathbf{x})\\)

The EM algorithm(also called Baum Welch algorithm) for HMM:

- E step:
  ![](/assets/images/pgm/hmm_e_step.png)
- M step:
  ![](/assets/images/pgm/hmm_m_step.png)

## EM for BN in general

![](/assets/images/pgm/em_for_BN_in_general.png)

For E step, we are essentially doing inference and accumulating expected sufficient statistics

- **Q**: are we collection ESS for each node? For Baum Welch algorithm, it's not the case.
  Actually, once we collected the expected value for all hidden nodes, then we ca use them to infer whatever quantity. In the HMM case, \\(p(y_{t}, y_{t+1} \vrt \mathbf{x})\\) can be calculated directly/easily from the single node case. So the above procedure is general enough.
- **Q**: what are the ESS for the Baum Welch case?
  The expected value for \\(p(y_t \vert \mathbf{x})\\)
- **Q**: how to cope with partially observed MN?
  Two inference sub problem: infer the hidden nodes and partition part.

## Other mixture models

### Conditional mixture model

Application example: give real-value data points, we would like to fit several lines each of which explain non-overlapping parts of the data points.

![](/assets/images/pgm/conditiona_mixture_model_example.png)

where input data's cluster membership is conditionally dependent on its value.

![](/assets/images/pgm/conditiona_mixture_model_graph.png)

The membership is determined by \\(x\\) through *softmax* function and the value \\(y\\) via linear regression.

Model:

![](/assets/images/pgm/conditiona_mixture_model.png)

Expected complete log likelihood:

![](/assets/images/pgm/conditiona_mixture_model_expected_complete_log_likelihood.png)

- E step:
  ![](/assets/images/pgm/conditiona_mixture_model_e_step.png)
- M step:
  - normal equation for standard LE with data re-weighted by \\(\tau\\)
  - or using IRLS
  - **Q**: how?

### Mixture of overlapping experts

In contrast to the previous model, the cluster membership is marginally independent on \\(x\\).

![](/assets/images/pgm/mixture_of_overlapping_experts_graph.png)

Note the difference in e step:

![](/assets/images/pgm/mixture_of_overlapping_experts_e_step.png)


### Incomplete data(partially hidden data)

For example, in HMM, only part of the labels are given. Then what to do? Easy, treat the given one as fixed while the missing one as latent and estimate them.

![](/assets/images/pgm/partially_hidden_data_log_likelihood.png)

The EM we are talking about previously can be seen as a special case of EM for partially hidden data. In the previous EM, all data are missing while here part are missing.

## EM variants

### Sparse EM

For the latent variable of some data points, its value can be quite certain(posterior is closes to 0 or 1). Thus, it won't help much in estimating them at **every** iteration. Instead, we can estimate those "dead" points once a while. By keeping track of the change rate of the posterior value, we can have a active list of data points and only recompute those in the list.

### Generalized(incomplete) EM

Sometimes, closed form of solution in E/M step is not available, we improve the likelihood a bit for example, via gradient step.


## Summary on EM

Good:

- very general(can be applied to all graphical model)
- no learning rate
- guaranteed to improve likelihood

Bad:

- stuck in local minimum
- slower than conjugate gradient
- expensive inference
