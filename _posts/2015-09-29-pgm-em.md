---
layout: post
title: "PGM: The Expectation-Maximization Algorithm "
date: 2015-09-29 23:33:45
categories: course
tags: pgm latent-variable em gmm
---




## Guassian Mixture Model

![](/assets/images/pgm/gmm.png)

### Difficulty in parameter estimation

Decomposability(local estimation) in fully observed GMM:

![](/assets/images/pgm/em_mixture_model_for_fully_observed_data.png)

In partially observed GMM, parameters are coupled together(sum within logarithm).

![](/assets/images/pgm/em_mixture_model_for_partially_observed_data.png)


### Expectation Maximization

In EM, label assignment of each data is fractional(probability).

- **Expectation step**: calculate the expected value of hidden variables given the current estimation of parameters and data(\\(p(z \vert D, \theta)\\), posterior inference problem)
- **Maximization step**: compute the parameters by maximizing \\(\mathcal{l}\\) under the expected value of hidden variables(essentially, MLE for fully observed BN)

Related model is K-means. Compared to K-means, label assignment is "hard"(or MAP) in K-means while it's "soft" in EM.


### Why EM works?

Why EM works? Essentially, we want to show the marginal likelihood is increasing as EM runs.


- **Q*
*: Why do we use the *expected complete log likelihood*?
  For notational convenience in deriving the lower bould of \\(\mathcal{l}\\).

For E step:

Jensen's equality leads to lower bound for \\(\mathcal{l}\\).

$$
\begin{eqnarray*}
\mathcal{l}(\theta; \mathbf{x})
&=& \log p(\mathbf{x} \vert \theta) \\
&=& \log \sum\limits_z p(\mathbf{x}, z \vert \theta) \\
&=& \log \sum\limits_z q(z\vertx) \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vertx)} \\
&\ge& \sum\limits_z  q(z\vertx) \log \frac{p(\mathbf{x}, z \vert \theta)}{q(z\vertx)}
\end{eqnarray*}
$$

Running EM is coordinate-ascent where both E and M step maximizes the lower bound of \\(\mathbf{l}(\theta, x)\\).

![](/assets/images/pgm/em_in_terms_of_lower_bound.png)

The lower bound is coined *free energy function*, \\(F(q, \theta)\\).

By pushing up the lower bound, we wish to increase \\(\mathcal{l}(\theta, x)\\).

For M step, setting \\(q(z \vert x)\\) to \\(p(z \vert x, \theta)\\) leads to the \\(F(q, \theta) = \mathcal{l}(\theta, x)\\), which indicates \\(F(\theta, x)\\) is maxmized.

For M step, given the optimal \\(q\\) in previous step(which can be treated as known) and the entropy term does not depend on \\(\theta\\), it reduces to MLE of fully observed model.

![](/assets/images/pgm/m_step_for_lower_bound.png)

Thus, we are pushing up the lower bounds until convergence.

**Q**: pushing up upper bound only means the minimum value is increasing. Does it necessarily mean the actual value is increasing as well?
