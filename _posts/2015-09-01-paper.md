---
layout: post
title:  "Mining Quality Phrase from Massive Text Corpora"
date:   2015-09-01 23:54:00
categories: paper
tags: nlp dm phrase-mining
---


## Basic ideas
- Goal: mine quality phrases
- Traditional approach: use raw frequency as the indicator
- Problem: raw frequency is inaccurate("support vector" in "support vector machine")
- Solution: **rectify** the frequency by segmenting the documents(ambiguity can be resolved)

## Approach

1. Train a prhase quality classifier based on labeled quality phrases and bad phrases(*concordance* and *informative* feautures)
2. Segment the documents by leveraging the classifier(some dyanmic programming algorithm)
3. Rectify the raw frequency based on the segmentation(Hard-EM/Viterbi training algorithm)
4. Retrain the classifier using the new features based on the rectified frequency
5. Repeat 2 and 3
6. Filter low rectified phrases

I/O:

- Input: positive/negative phrases, documents
- Output: segmented documents and ranked list of phrases


## What I learned


| Knew                         | Learned                                                                                   | Question                                                     | Pointers                                                                                               |
|------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------+--------------------------------------------------------------------------------------------------------|
|                              | Similar problems to phrase segmentation: query segmentation, Chinese word segmentation    |                                                              |                                                                                                        |
| PMI as concordance measure   | point-wise KL: takes frequency into account, gives higher score for more frequent phrases |                                                              |                                                                                                        |
| IDF as weighting             | Interpretation: informativeness                                                           |                                                              |                                                                                                        |
|                              | New measurements on "informativeness": contain-stopwords, in-quotation                    |                                                              |                                                                                                        |
|                              | Ways to reduce labeling effort: PU learning, active learning                              |                                                              | Li, 2003, Learning to classify texts using positive and unlabeled data. Settles, 2012, Active Learning |
| Substring frequency counting | A linear time version that utilises some properties                                       |                                                              |                                                                                                        |
| Dynamic programming          | DP for phrase segmentation                                                                | Possible for query segmentations?                            |                                                                                                        |
| Generative model, HMM        | How this framework is fitted in this problem. Length penalty                              | Can we use discriminative models to use more features   |                                                                                                        |
| Viterbi decoding             | Viterbi training, aka Hard-EM. Bawn-Welch as Soft-EM.                                     | What's the difference?                                       | Book: Bishop, 2006, Pattern recognition and machine learning                                           |
| Binary search                | Used for finding the optimial hyper-parameter, length penalty term                        |                                                              |                                                                                                        |
| Def of odd                   | Applied here to compare proba of two events('phrase' and 'should be splitted')            |                                                              |                                                                                                        |
| Aho-Corasick                 | Match-string matching: locates a finite set of strings                                    | Time/space complexity                                        |                                                                                                        |
| K-means                      | can be used for samping representative labeling instances                                 |                                                              |                                                                                                        |
| Wikipedia                    | Phrases can be collected fro Wikipedia                                                    | How to narrow down the ranges(for CS only)                   |                                                                                                        |
|                              | Downsample: sample a small portion to make labeled data more balanced                     |                                                              |                                                                                                        |
| Bag of Word                  | Bag of Phrase                                                                             | How they compare in different tasks(IR, text classification) |                                                                                                        |
| word2vec                     | word2phrase                                                                               |                                                              |                                                                                                        |
  



## Paper writing/organization discvoery

- When display rec/recall/f1, display the count as well(support, #match, etc)
- Performance comparison: time consumption, data file size can be included, proportion of different modules
- Feature contribution(weight as the indicator for example) 

## Possible improvement

- No need for human labeling, distant training(phrases from Wikipedia/Freebase)
- Linguistic-rich patterns to extract candidates
- For other languages: tranfer learning**?**