---
layout: post
title: "GloVe: Global Vectors forWord Representation"
date: 2015-09-13 19:52:45
categories: paper
tags: word-embedding glove
---

# GloVe: Global Vectors forWord Representation

[paper link](http://www-nlp.stanford.edu/pubs/glove.pdf)

It proposes a model that combines global co-occurence information with local context window methods.

## Problem

How to compute meaningful word representations from corpus in an unsupervised way?

## Approach

Maximize the objective function:

$$ J = \sum_{i,j=1}^V f(X_{ij}) (w_i^T\widetilde{w}_j + b_i + \widetilde{b}_j - \log{X_{ij$$

Where:

- \\(w_i,  \widetide{w}_j\\) are word representations
- \\(X_{ij}\\) is the frequency of jth word occuring within the context windown of the ith word
- \\( b_i, \widetilde{b}_j\\) are bias terms
- \\(f\\) are weighting function to prevent the overwhelming effect of large \\(X_{ij}\\)


## Connection with *Skip Gram* in *word2vec* 

In skip gram,

$$ J = \sum_{i \in corpus, j \in context(i)} log Q(i, j) $$

where:

- Q(i, j) = \frac{exp(w_i^T \widetilde{w}_j)}{\sum_{k=1}^Vexp(w_i^T \widetilde{w}_k)}. The probability that word j appears in the context of word i.

The above is the same as(**key transformation**):

$$
\begin{eqnarray}
 J & = \sum_{i, j=1}^V X_{i,j} log Q(i, j) \\
   & = \sum_{i} X_i \sum_j  P_{i,j} log Q(i, j) \\
   & = \sum_{i} X_i H(P_i, Q_i)
\end{eqnarray}
$$

**Q**: does the training algorithm differ for the above two objective functions?

where \\(H\\) is cross entropy.

And the following modifications are made with reasons given.

The \\( \log \\) term in \\( H\\) approaches to \\(-\infty\\) if \\(Q_i \rightarrow - \\), which is bad and often happen.

$$ J = \sum_{i,j} X_i (\hat{P}_{ij} - \hat{Q}_{ij})^2$$

where \\(\hat{P}, \hat{Q}\\) are the unnormalized quantity, as normalization for \\(Q\\) is expensive.

However, the \\(\exp\\) in \\(\hat(Q)\\) makes it bad for training(large values), we can apply \\(\log\\) to it. 

$$
\begin{eqnarray}
J & = \sum_{i,j} X_{ij} (\log\hat{P}_{ij} - \log\hat{Q}_{ij})^2 \\
  & = \sum_{i,j} X_{ij} (w_i^T \widetilde{w}_j - \log X_{ij})^2 
\end{eqnarray}
$$

After applying reweighting on \\(X_{i,j}\\), it's almost equivalent to the *GloVe* objective function.

So the \\(J\\) of *Skip Gram* and *GloVe* differs in:

- *GloVe* **direcly** uses global occurence information \\(X_{ij}\\) in two parts while *Skip Gram* **implicitly** uses it only in one part
- *GloVe* reweights \\(X_{ij}\\) using \\(f\\)


## Connection to Matrix Factorization

\\( \sum_{i,j} X_{ij} (w_i^T \widetilde{w}_j - \log X_{ij})^2 \\) is a modified form of  \\( (w_i^T \widetilde{w}_j - X_{ij})^2 \\).

## Training

Stochastic Gradient Descent by sampling **non-negative** elements from \\(X\\).


## Misc

- The final word embedding is \\( w_i + \widetilde{w}_i\\)
- When calculating \\(X_{ij}\\), words that are \\(d\\) words apart is discounted by \\(1/d\\) to capture the fact that distant words provide less information


## What I learned

- One way to infer meaning for word: use the global word-word co-occurence to infer. The intuition: word's meaning can be defined by it's co-occuring words.
- Connection prediction-based model such as *Skip Gram* with co-occurence matrix factorization model
- Evaluation for word embedding: word analogy test, word similarity, downstream NLP tasks(using the embedding as features for tasks such as NER)
- Why *Skip Gram* and *GloVe* perform matrix factorization *implicitly* and *explicitly* due to their different ways to formulating the objective function
- Connection between language model and word embeddings: \\(P(w_1 \cdots \w_n) = \prod_i P(w_i | context)\\) where \\(P(w_i | context)\\) is calculated using word embeddings.
- Application of word embedding: as feature for downstream NLP tasks(NER, parsing, etc), representing document[Aggregating ContinuousWord Embeddings for Information Retrieval](http://www.aclweb.org/anthology/W13-3212)
- Recap on *word2vec* and Bengio's 2003 neural language model paper: the goal is to maximize the log-likelihood of the corpus.

## Questions

- Why word analogy test is used? Does it indicate a better performance on other NLP tasks such as parsing and question-answering
- Similarly, why do we bother learn word embedding that captures *linear* (\\( w_i - w_j\\)) relationships with other word embeddings?
- When is *word2vec* useful? Especially, when is the linear property useful?
